{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from importlib import reload\n",
    "import tensorflow as tf\n",
    "from utils_v2 import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import StrawberryDQNAgent\n",
    "reload(StrawberryDQNAgent)\n",
    "from StrawberryDQNAgent import StrawberryDQNAgent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = StrawberryDQNAgent(cash_supply=5000, epsilon_decay=0.9995) # super slow decay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# features\n",
    "agent.external_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def smooth(x,window_len=11,window='hanning'):\n",
    "    \"\"\"smooth the data using a window with requested size.\n",
    "    \n",
    "    This method is based on the convolution of a scaled window with the signal.\n",
    "    The signal is prepared by introducing reflected copies of the signal \n",
    "    (with the window size) in both ends so that transient parts are minimized\n",
    "    in the begining and end part of the output signal.\n",
    "    \n",
    "    input:\n",
    "        x: the input signal \n",
    "        window_len: the dimension of the smoothing window; should be an odd integer\n",
    "        window: the type of window from 'flat', 'hanning', 'hamming', 'bartlett', 'blackman'\n",
    "            flat window will produce a moving average smoothing.\n",
    "\n",
    "    output:\n",
    "        the smoothed signal\n",
    "        \n",
    "    example:\n",
    "\n",
    "    t=linspace(-2,2,0.1)\n",
    "    x=sin(t)+randn(len(t))*0.1\n",
    "    y=smooth(x)\n",
    "    \n",
    "    see also: \n",
    "    \n",
    "    numpy.hanning, numpy.hamming, numpy.bartlett, numpy.blackman, numpy.convolve\n",
    "    scipy.signal.lfilter\n",
    " \n",
    "    TODO: the window parameter could be the window itself if an array instead of a string\n",
    "    NOTE: length(output) != length(input), to correct this: return y[(window_len/2-1):-(window_len/2)] instead of just y.\n",
    "    \"\"\"\n",
    "\n",
    "    if x.ndim != 1:\n",
    "        raise ValueError(\"smooth only accepts 1 dimension arrays.\")\n",
    "\n",
    "    if x.size < window_len:\n",
    "        raise ValueError (\"Input vector needs to be bigger than window size.\")\n",
    "\n",
    "\n",
    "    if window_len<3:\n",
    "        return x\n",
    "\n",
    "\n",
    "    if not window in ['flat', 'hanning', 'hamming', 'bartlett', 'blackman']:\n",
    "        raise ValueError (\"Window is on of 'flat', 'hanning', 'hamming', 'bartlett', 'blackman'\")\n",
    "\n",
    "\n",
    "    s=np.r_[x[window_len-1:0:-1],x,x[-2:-window_len-1:-1]]\n",
    "    #print(len(s))\n",
    "    if window == 'flat': #moving average\n",
    "        w=numpy.ones(window_len,'d')\n",
    "    else:\n",
    "        w=eval('np.'+window+'(window_len)')\n",
    "\n",
    "    y=np.convolve(w/w.sum(),s,mode='valid')\n",
    "    return y\n",
    "\n",
    "\n",
    "def plot_test(start_time, end_time=None):\n",
    "\n",
    "    if end_time is None: # default: one day\n",
    "        end_time = agent.env.end_index\n",
    "        \n",
    "    df = agent.env.df\n",
    "    df = df.loc[df.index >= start_time]\n",
    "    df = df.loc[df.index <= end_time]\n",
    "    prices = df['USDT_BTC_open']\n",
    "#     print(prices.shape)\n",
    "#     print(smooth(prices))\n",
    "    \n",
    "    actions = agent.test_actions\n",
    "    actions = actions[actions.index >= start_time]\n",
    "    actions = actions[actions.index < end_time]\n",
    "\n",
    "    fig, ax1 = plt.subplots(figsize = (15, 8))\n",
    "\n",
    "    ax1.plot(prices.index, prices, 'b-')\n",
    "    ax1.set_ylabel('Price', color='b', fontsize=15)\n",
    "    ax1.tick_params('y', colors='b', labelsize=15)\n",
    "\n",
    "    hold = actions[actions == 1]\n",
    "    buy = actions[actions == 2]\n",
    "    sell = actions[actions == 0]\n",
    "\n",
    "\n",
    "    \n",
    "    sm =  smooth(prices,24)[12:len(prices.index)+12]\n",
    "    import numpy as np\n",
    "    from scipy.signal import argrelextrema\n",
    "\n",
    "    local_minima = argrelextrema(sm,np.less) \n",
    "    local_maxima = argrelextrema(sm,np.greater)\n",
    "    turning = np.concatenate((local_minima[0],local_maxima[0]),axis=0)\n",
    "    turning = np.append(turning,0)\n",
    "    turning = np.append(turning,len(prices.index) - 1)\n",
    "    turning.sort()\n",
    "    sell_first = True\n",
    "    if (prices[0] < prices[1]):\n",
    "        sell_first = False\n",
    "    l_turning = list(turning)\n",
    "    edge = []\n",
    "    edge.append(0)\n",
    "    for i in range(len(l_turning) - 1):\n",
    "        edge.append((l_turning[i] + l_turning[i + 1])//2)\n",
    "    edge.append(len(prices.index) - 1)\n",
    "    \n",
    "    good_action = []\n",
    "    cur_action = 0 # sell\n",
    "    last_edge = 0\n",
    "    if (not sell_first):\n",
    "        cur_action = 2 # buy\n",
    "    for i in range(1,len(edge)):\n",
    "        target_edge = edge[i]\n",
    "        for j in np.arange(last_edge,target_edge,1):\n",
    "            good_action.append(cur_action)\n",
    "        cur_action = 2 - cur_action\n",
    "        last_edge = target_edge\n",
    "        \n",
    "    good_action_df = pd.DataFrame()\n",
    "    good_action_df[\"good_action\"] = good_action\n",
    "    good_action_df.index = actions.index\n",
    "\n",
    "\n",
    "    \n",
    "    good_buy = actions[(actions == 2) & (actions == good_action)]\n",
    "    bad_buy = actions[(actions== 2) & (actions != good_action)]\n",
    "    good_sell = actions[(actions == 0) & (actions == good_action)]\n",
    "    bad_sell = actions[(actions== 0) & (actions != good_action)] \n",
    "        \n",
    "    ax2 = ax1.twinx()\n",
    "    ax2.scatter(hold.index, hold, c='blue', label='HOLD')\n",
    "    ax2.scatter(good_buy.index, good_buy, c='green', marker = 'o',label='GOOD_BUY')\n",
    "    ax2.scatter(bad_buy.index, bad_buy, c='red',marker = 'x', label='BAD_BUY')\n",
    "    ax2.scatter(good_sell.index, good_sell, c='green', marker = 'o', label='GOOD_SELL')\n",
    "    ax2.scatter(bad_sell.index, bad_sell, c='red', marker = 'x', label='BAD_SELL')\n",
    "    ax2.set_yticks([])\n",
    "    ax2.legend(loc=1, fontsize=15)\n",
    "        \n",
    "    \n",
    "    ax3 = ax1.twinx()\n",
    "    ax3.plot(prices.index,sm, 'r-')\n",
    "    ax3.tick_params('y_smooth', colors='r', labelsize=15)\n",
    "    ax3.set_yticks([])\n",
    "    \n",
    "    ax4 = ax1.twinx()\n",
    "#     ax4.set_ylim(0,6000)\n",
    "    ax4.plot(prices.index, agent.test_portfolio_values_cash, 'y', label='Cash Value')\n",
    "    ax4.plot(prices.index, agent.test_portfolio_values_coin, 'orange', label='Coin Value in USD')\n",
    "    ax4.legend(loc=4, fontsize=15)\n",
    "\n",
    "    plt.xlim(actions.index[0], actions.index[-1])       \n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set Experiment Name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_name = 'Strawberry'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train (2017-01-01 ~ 2018-01-01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.epsilon = 1\n",
    "agent.epsilon_min = 0.005"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Auto loading is off. Creating model with fresh parameters.\n",
      "Training, randomly selecting episodes from  2017-01-01 00:00:00  to 2018-01-01 00:00:00 :  ~ 365 days\n",
      "\n",
      "regression loss:  35699612.0\n",
      "regression loss:  35228944.0\n",
      "episode: 1/1000000, returns: 0.44, epsilon: 1.0\n",
      "regression loss:  41440828.0\n",
      "regression loss:  35799308.0\n",
      "episode: 2/1000000, returns: -8.8, epsilon: 1.0\n",
      "regression loss:  88514424.0\n",
      "regression loss:  23220090.0\n",
      "episode: 3/1000000, returns: 1.3, epsilon: 1.0\n",
      "regression loss:  19937844.0\n",
      "regression loss:  18946884.0\n",
      "episode: 4/1000000, returns: -8.1, epsilon: 1.0\n",
      "regression loss:  137983620.0\n",
      "regression loss:  121090650.0\n",
      "episode: 5/1000000, returns: 2.8, epsilon: 1.0\n",
      "regression loss:  113795720.0\n",
      "regression loss:  320696580.0\n",
      "episode: 6/1000000, returns: 3.3, epsilon: 1.0\n",
      "regression loss:  169997440.0\n",
      "regression loss:  190855710.0\n",
      "episode: 7/1000000, returns: 5.0, epsilon: 1.0\n",
      "regression loss:  33680310.0\n",
      "regression loss:  79348170.0\n",
      "episode: 8/1000000, returns: -7.8, epsilon: 1.0\n",
      "regression loss:  59349812.0\n",
      "regression loss:  65239860.0\n",
      "episode: 9/1000000, returns: 1.5, epsilon: 1.0\n",
      "regression loss:  54709390.0\n",
      "regression loss:  125644216.0\n",
      "episode: 10/1000000, returns: -1.1, epsilon: 1.0\n",
      "regression loss:  84113330.0\n",
      "regression loss:  80089300.0\n",
      "episode: 11/1000000, returns: -5.7, epsilon: 0.99\n",
      "regression loss:  19601616.0\n",
      "regression loss:  25119656.0\n",
      "episode: 12/1000000, returns: -0.13, epsilon: 0.99\n",
      "regression loss:  31200540.0\n",
      "regression loss:  23325086.0\n",
      "episode: 13/1000000, returns: -1.7, epsilon: 0.99\n",
      "regression loss:  17027274.0\n",
      "regression loss:  27403620.0\n",
      "episode: 14/1000000, returns: 2.3, epsilon: 0.99\n",
      "regression loss:  15337854.0\n",
      "regression loss:  17642726.0\n",
      "episode: 15/1000000, returns: -4.6, epsilon: 0.99\n",
      "regression loss:  14295662.0\n",
      "regression loss:  20278636.0\n",
      "episode: 16/1000000, returns: 2.0, epsilon: 0.99\n",
      "regression loss:  16849508.0\n",
      "regression loss:  11258391.0\n",
      "episode: 17/1000000, returns: -2.0, epsilon: 0.99\n",
      "regression loss:  5890627.0\n",
      "regression loss:  8692030.0\n",
      "episode: 18/1000000, returns: 3.1, epsilon: 0.99\n",
      "regression loss:  5000611.0\n",
      "regression loss:  23375950.0\n",
      "episode: 19/1000000, returns: -1.1, epsilon: 0.99\n",
      "regression loss:  11744345.0\n",
      "regression loss:  16392600.0\n",
      "episode: 20/1000000, returns: -1.4, epsilon: 0.99\n",
      "regression loss:  12586250.0\n",
      "regression loss:  16805192.0\n",
      "episode: 21/1000000, returns: 4.0, epsilon: 0.99\n",
      "regression loss:  13258679.0\n",
      "regression loss:  13127744.0\n",
      "episode: 22/1000000, returns: 3.7, epsilon: 0.99\n",
      "regression loss:  15302716.0\n",
      "regression loss:  6063177.5\n",
      "episode: 23/1000000, returns: -0.41, epsilon: 0.99\n",
      "regression loss:  6160051.5\n",
      "regression loss:  5632724.0\n",
      "episode: 24/1000000, returns: 3.3, epsilon: 0.99\n",
      "regression loss:  8957304.0\n",
      "regression loss:  7467298.0\n",
      "episode: 25/1000000, returns: -3.3, epsilon: 0.99\n",
      "regression loss:  7718029.5\n",
      "regression loss:  5991029.5\n",
      "episode: 26/1000000, returns: -5.0, epsilon: 0.99\n",
      "regression loss:  4337846.0\n",
      "regression loss:  11290980.0\n",
      "episode: 27/1000000, returns: 0.062, epsilon: 0.99\n",
      "regression loss:  3733887.0\n",
      "regression loss:  5884440.5\n",
      "episode: 28/1000000, returns: 3.0, epsilon: 0.99\n",
      "regression loss:  3612529.2\n",
      "regression loss:  10037882.0\n",
      "episode: 29/1000000, returns: 8.9, epsilon: 0.99\n",
      "regression loss:  3012011.8\n",
      "regression loss:  7769214.0\n",
      "episode: 30/1000000, returns: -9.9, epsilon: 0.99\n",
      "regression loss:  7817423.5\n",
      "regression loss:  11365494.0\n",
      "episode: 31/1000000, returns: 7.0, epsilon: 0.98\n",
      "regression loss:  3702140.0\n",
      "regression loss:  9983746.0\n",
      "episode: 32/1000000, returns: 8.4, epsilon: 0.98\n",
      "regression loss:  2250626.5\n",
      "regression loss:  4522216.0\n",
      "episode: 33/1000000, returns: 7.7, epsilon: 0.98\n",
      "regression loss:  3159018.2\n",
      "regression loss:  7668906.0\n",
      "episode: 34/1000000, returns: 0.92, epsilon: 0.98\n",
      "regression loss:  2106473.2\n",
      "regression loss:  5590688.5\n",
      "episode: 35/1000000, returns: -0.27, epsilon: 0.98\n",
      "regression loss:  3796347.5\n",
      "regression loss:  4237767.0\n",
      "episode: 36/1000000, returns: -4.1, epsilon: 0.98\n",
      "regression loss:  2884132.8\n",
      "regression loss:  5040661.0\n",
      "episode: 37/1000000, returns: 3.6, epsilon: 0.98\n",
      "regression loss:  3568740.8\n",
      "regression loss:  4549623.5\n",
      "episode: 38/1000000, returns: 5.2, epsilon: 0.98\n",
      "regression loss:  1953628.9\n",
      "regression loss:  4535716.5\n",
      "episode: 39/1000000, returns: 1.5, epsilon: 0.98\n",
      "regression loss:  1106788.5\n",
      "regression loss:  2529602.5\n",
      "episode: 40/1000000, returns: -9.4, epsilon: 0.98\n",
      "regression loss:  913600.2\n",
      "regression loss:  4079789.0\n",
      "episode: 41/1000000, returns: 2.0, epsilon: 0.98\n",
      "regression loss:  880013.5\n",
      "regression loss:  7545192.0\n",
      "episode: 42/1000000, returns: 2.3, epsilon: 0.98\n",
      "regression loss:  511618.78\n",
      "regression loss:  3100001.8\n",
      "episode: 43/1000000, returns: 0.94, epsilon: 0.98\n",
      "regression loss:  2191039.5\n",
      "regression loss:  12532599.0\n",
      "episode: 44/1000000, returns: 3.3, epsilon: 0.98\n",
      "regression loss:  1286437.5\n",
      "regression loss:  7379162.5\n",
      "episode: 45/1000000, returns: -2.5, epsilon: 0.98\n",
      "regression loss:  1600848.9\n",
      "regression loss:  4557817.0\n",
      "episode: 46/1000000, returns: 7.1, epsilon: 0.98\n",
      "regression loss:  1306823.0\n",
      "regression loss:  2494220.2\n",
      "episode: 47/1000000, returns: -4.7, epsilon: 0.98\n",
      "regression loss:  3292790.2\n",
      "regression loss:  4572347.5\n",
      "episode: 48/1000000, returns: 3.9, epsilon: 0.98\n",
      "regression loss:  1617821.1\n",
      "regression loss:  3822344.2\n",
      "episode: 49/1000000, returns: 1.1, epsilon: 0.98\n",
      "regression loss:  1957754.0\n",
      "regression loss:  3025111.5\n",
      "episode: 50/1000000, returns: -1.2, epsilon: 0.98\n",
      "regression loss:  1626154.2\n",
      "regression loss:  3526662.0\n",
      "episode: 51/1000000, returns: 7.3, epsilon: 0.97\n",
      "regression loss:  914550.1\n",
      "regression loss:  2017691.0\n",
      "episode: 52/1000000, returns: -0.69, epsilon: 0.97\n",
      "regression loss:  1241834.5\n",
      "regression loss:  2770769.0\n",
      "episode: 53/1000000, returns: -3.3, epsilon: 0.97\n",
      "regression loss:  3231690.8\n",
      "regression loss:  4141772.0\n",
      "episode: 54/1000000, returns: -0.72, epsilon: 0.97\n",
      "regression loss:  593438.94\n",
      "regression loss:  3763501.5\n",
      "episode: 55/1000000, returns: 5.9, epsilon: 0.97\n",
      "regression loss:  2191600.2\n",
      "regression loss:  2586584.5\n",
      "episode: 56/1000000, returns: -1.7, epsilon: 0.97\n",
      "regression loss:  1140502.0\n",
      "regression loss:  3418183.8\n",
      "episode: 57/1000000, returns: 5.8, epsilon: 0.97\n",
      "regression loss:  934415.9\n",
      "regression loss:  3537184.8\n",
      "episode: 58/1000000, returns: 7.7, epsilon: 0.97\n",
      "regression loss:  536148.1\n",
      "regression loss:  3225241.8\n",
      "episode: 59/1000000, returns: 3.3, epsilon: 0.97\n",
      "regression loss:  1254691.5\n",
      "regression loss:  4604091.0\n",
      "episode: 60/1000000, returns: 2.1, epsilon: 0.97\n",
      "regression loss:  549793.44\n",
      "regression loss:  2096289.0\n",
      "episode: 61/1000000, returns: -1.1, epsilon: 0.97\n",
      "regression loss:  469057.72\n",
      "regression loss:  2102659.0\n",
      "episode: 62/1000000, returns: -1.8, epsilon: 0.97\n",
      "regression loss:  320058.97\n",
      "regression loss:  3003717.8\n",
      "episode: 63/1000000, returns: 3.1, epsilon: 0.97\n",
      "regression loss:  341691.38\n",
      "regression loss:  1332247.8\n",
      "episode: 64/1000000, returns: -4.8, epsilon: 0.97\n",
      "regression loss:  654904.6\n",
      "regression loss:  3032840.0\n",
      "episode: 65/1000000, returns: 3.0, epsilon: 0.97\n",
      "regression loss:  873846.1\n",
      "regression loss:  1477996.0\n",
      "episode: 66/1000000, returns: -8.6, epsilon: 0.97\n",
      "regression loss:  403555.56\n",
      "regression loss:  1950407.9\n",
      "episode: 67/1000000, returns: -8.4, epsilon: 0.97\n",
      "regression loss:  309744.06\n",
      "regression loss:  2235455.8\n",
      "episode: 68/1000000, returns: -0.18, epsilon: 0.97\n",
      "regression loss:  408231.28\n",
      "regression loss:  1853039.5\n",
      "episode: 69/1000000, returns: 1.3, epsilon: 0.97\n",
      "regression loss:  356280.66\n",
      "regression loss:  2218997.0\n",
      "episode: 70/1000000, returns: 8.8, epsilon: 0.97\n",
      "regression loss:  553192.3\n",
      "regression loss:  2114651.0\n",
      "episode: 71/1000000, returns: -2.2, epsilon: 0.97\n",
      "regression loss:  330690.53\n",
      "regression loss:  1605931.6\n",
      "episode: 72/1000000, returns: 2.4, epsilon: 0.96\n",
      "regression loss:  212604.89\n",
      "regression loss:  2750795.2\n",
      "episode: 73/1000000, returns: 3.6, epsilon: 0.96\n",
      "regression loss:  122237.875\n",
      "regression loss:  1900006.6\n",
      "episode: 74/1000000, returns: -1.7, epsilon: 0.96\n",
      "regression loss:  198318.08\n",
      "regression loss:  1813273.0\n",
      "episode: 75/1000000, returns: 4.6, epsilon: 0.96\n",
      "regression loss:  310432.75\n",
      "regression loss:  1462006.0\n",
      "episode: 76/1000000, returns: 0.44, epsilon: 0.96\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "regression loss:  299028.16\n",
      "regression loss:  1348107.8\n",
      "episode: 77/1000000, returns: 2.0, epsilon: 0.96\n",
      "regression loss:  214976.05\n",
      "regression loss:  1494462.1\n",
      "episode: 78/1000000, returns: 4.1, epsilon: 0.96\n",
      "regression loss:  383472.9\n",
      "regression loss:  1063560.9\n",
      "episode: 79/1000000, returns: 1.5, epsilon: 0.96\n",
      "regression loss:  271807.97\n",
      "regression loss:  1180022.2\n",
      "episode: 80/1000000, returns: -2.2, epsilon: 0.96\n",
      "regression loss:  148117.66\n",
      "regression loss:  1181995.9\n",
      "episode: 81/1000000, returns: 8.1, epsilon: 0.96\n",
      "regression loss:  115506.31\n",
      "regression loss:  896179.6\n",
      "episode: 82/1000000, returns: 3.9, epsilon: 0.96\n",
      "regression loss:  90342.01\n",
      "regression loss:  961774.9\n",
      "episode: 83/1000000, returns: 8.6, epsilon: 0.96\n",
      "regression loss:  106387.23\n",
      "regression loss:  977709.0\n",
      "episode: 84/1000000, returns: 3.6, epsilon: 0.96\n",
      "regression loss:  70957.33\n",
      "regression loss:  915191.44\n",
      "episode: 85/1000000, returns: 6.4, epsilon: 0.96\n",
      "regression loss:  73671.19\n",
      "regression loss:  827957.44\n",
      "episode: 86/1000000, returns: -5.4, epsilon: 0.96\n",
      "regression loss:  40295.406\n",
      "regression loss:  710641.5\n",
      "episode: 87/1000000, returns: 4.2, epsilon: 0.96\n",
      "regression loss:  51716.67\n",
      "regression loss:  452751.0\n",
      "episode: 88/1000000, returns: 2.4, epsilon: 0.96\n",
      "regression loss:  52954.523\n",
      "regression loss:  456967.72\n",
      "episode: 89/1000000, returns: 0.42, epsilon: 0.96\n",
      "regression loss:  50222.598\n",
      "regression loss:  491129.84\n",
      "episode: 90/1000000, returns: 2.9, epsilon: 0.96\n",
      "regression loss:  25969.71\n",
      "regression loss:  342440.1\n",
      "episode: 91/1000000, returns: -0.089, epsilon: 0.96\n",
      "regression loss:  36405.918\n",
      "regression loss:  394291.4\n",
      "episode: 92/1000000, returns: 2.1, epsilon: 0.96\n",
      "regression loss:  19021.273\n",
      "regression loss:  362143.03\n",
      "episode: 93/1000000, returns: 3.0, epsilon: 0.95\n",
      "regression loss:  17812.893\n",
      "regression loss:  234185.16\n",
      "episode: 94/1000000, returns: -5.0, epsilon: 0.95\n",
      "regression loss:  10548.326\n",
      "regression loss:  196213.34\n",
      "episode: 95/1000000, returns: -1.4, epsilon: 0.95\n",
      "regression loss:  26128.773\n",
      "regression loss:  210487.12\n",
      "episode: 96/1000000, returns: -0.67, epsilon: 0.95\n",
      "regression loss:  26289.117\n",
      "regression loss:  269869.8\n",
      "episode: 97/1000000, returns: 1.2, epsilon: 0.95\n",
      "regression loss:  12391.61\n",
      "regression loss:  155931.03\n",
      "episode: 98/1000000, returns: -0.88, epsilon: 0.95\n",
      "regression loss:  7446.265\n",
      "regression loss:  135387.42\n",
      "episode: 99/1000000, returns: -2.8, epsilon: 0.95\n",
      "regression loss:  22415.725\n",
      "regression loss:  148902.83\n",
      "episode: 100/1000000, returns: 2.1, epsilon: 0.95\n",
      "regression loss:  6527.5337\n",
      "regression loss:  118256.77\n",
      "episode: 101/1000000, returns: 3.7, epsilon: 0.95\n",
      "regression loss:  16032.969\n",
      "regression loss:  153710.42\n",
      "episode: 102/1000000, returns: -0.47, epsilon: 0.95\n",
      "regression loss:  52623.395\n",
      "regression loss:  132384.64\n",
      "episode: 103/1000000, returns: 7.3, epsilon: 0.95\n",
      "regression loss:  19080.512\n",
      "regression loss:  82764.25\n",
      "episode: 104/1000000, returns: -6.3, epsilon: 0.95\n",
      "regression loss:  11284.972\n",
      "regression loss:  81677.555\n",
      "episode: 105/1000000, returns: 5.4, epsilon: 0.95\n",
      "regression loss:  35890.5\n",
      "regression loss:  77576.46\n",
      "episode: 106/1000000, returns: 1.7, epsilon: 0.95\n",
      "regression loss:  19639.027\n",
      "regression loss:  78068.81\n",
      "episode: 107/1000000, returns: 2.6, epsilon: 0.95\n",
      "regression loss:  10826.096\n",
      "regression loss:  101959.43\n",
      "episode: 108/1000000, returns: 3.2, epsilon: 0.95\n",
      "regression loss:  48552.8\n",
      "regression loss:  138364.88\n",
      "episode: 109/1000000, returns: 8.0, epsilon: 0.95\n",
      "regression loss:  21728.438\n",
      "regression loss:  68488.44\n",
      "episode: 110/1000000, returns: 0.22, epsilon: 0.95\n",
      "regression loss:  7203.9585\n",
      "regression loss:  83351.12\n",
      "episode: 111/1000000, returns: 4.7, epsilon: 0.95\n",
      "regression loss:  13909.264\n",
      "regression loss:  101182.89\n",
      "episode: 112/1000000, returns: -0.37, epsilon: 0.95\n",
      "regression loss:  10683.101\n",
      "regression loss:  50802.383\n",
      "episode: 113/1000000, returns: 1.7, epsilon: 0.95\n",
      "regression loss:  18319.518\n",
      "regression loss:  78945.64\n",
      "episode: 114/1000000, returns: -0.4, epsilon: 0.94\n",
      "regression loss:  38280.074\n",
      "regression loss:  85065.38\n",
      "episode: 115/1000000, returns: 3.5, epsilon: 0.94\n",
      "regression loss:  12036.558\n",
      "regression loss:  46166.477\n",
      "episode: 116/1000000, returns: 0.7, epsilon: 0.94\n",
      "regression loss:  12538.999\n",
      "regression loss:  98086.625\n",
      "episode: 117/1000000, returns: -0.49, epsilon: 0.94\n",
      "regression loss:  19825.568\n",
      "regression loss:  85392.54\n",
      "episode: 118/1000000, returns: -2.4, epsilon: 0.94\n",
      "regression loss:  5426.561\n",
      "regression loss:  62131.566\n",
      "episode: 119/1000000, returns: 2.5, epsilon: 0.94\n",
      "regression loss:  15865.284\n",
      "regression loss:  54021.61\n",
      "episode: 120/1000000, returns: 0.28, epsilon: 0.94\n",
      "regression loss:  6061.946\n",
      "regression loss:  42903.734\n",
      "episode: 121/1000000, returns: 1.2e+01, epsilon: 0.94\n",
      "regression loss:  8916.593\n",
      "regression loss:  56027.68\n",
      "episode: 122/1000000, returns: 5.2, epsilon: 0.94\n",
      "regression loss:  6706.0044\n",
      "regression loss:  40329.9\n",
      "episode: 123/1000000, returns: 2.1, epsilon: 0.94\n",
      "regression loss:  7769.253\n",
      "regression loss:  48619.8\n",
      "episode: 124/1000000, returns: 3.0, epsilon: 0.94\n",
      "regression loss:  4411.114\n",
      "regression loss:  35092.67\n",
      "episode: 125/1000000, returns: 2.5, epsilon: 0.94\n",
      "regression loss:  7939.3574\n",
      "regression loss:  51181.58\n",
      "episode: 126/1000000, returns: 1.3, epsilon: 0.94\n",
      "regression loss:  5273.8057\n",
      "regression loss:  29977.453\n",
      "episode: 127/1000000, returns: -3.6, epsilon: 0.94\n",
      "regression loss:  5355.126\n",
      "regression loss:  27906.96\n",
      "episode: 128/1000000, returns: -2.1, epsilon: 0.94\n",
      "regression loss:  7467.644\n",
      "regression loss:  30680.117\n",
      "episode: 129/1000000, returns: 1.4, epsilon: 0.94\n",
      "regression loss:  5046.691\n",
      "regression loss:  26224.324\n",
      "episode: 130/1000000, returns: -2.5, epsilon: 0.94\n",
      "regression loss:  9988.335\n",
      "regression loss:  39167.164\n",
      "episode: 131/1000000, returns: 6.5, epsilon: 0.94\n",
      "regression loss:  10318.67\n",
      "regression loss:  30989.152\n",
      "episode: 132/1000000, returns: 1.1e+01, epsilon: 0.94\n",
      "regression loss:  2936.6428\n",
      "regression loss:  35855.99\n",
      "episode: 133/1000000, returns: 2.5, epsilon: 0.94\n",
      "regression loss:  7691.38\n",
      "regression loss:  27804.363\n",
      "episode: 134/1000000, returns: -3.6, epsilon: 0.94\n",
      "regression loss:  1300.5947\n",
      "regression loss:  25310.877\n",
      "episode: 135/1000000, returns: 0.66, epsilon: 0.93\n",
      "regression loss:  11302.106\n",
      "regression loss:  36393.242\n",
      "episode: 136/1000000, returns: 0.54, epsilon: 0.93\n",
      "regression loss:  6743.036\n",
      "regression loss:  21972.643\n",
      "episode: 137/1000000, returns: 1.6, epsilon: 0.93\n",
      "regression loss:  10912.936\n",
      "regression loss:  28737.145\n",
      "episode: 138/1000000, returns: 4.8, epsilon: 0.93\n",
      "regression loss:  21917.057\n",
      "regression loss:  29332.473\n",
      "episode: 139/1000000, returns: -3.0, epsilon: 0.93\n",
      "regression loss:  1219.2646\n",
      "regression loss:  25694.186\n",
      "episode: 140/1000000, returns: -0.66, epsilon: 0.93\n",
      "regression loss:  23086.367\n",
      "regression loss:  56699.938\n",
      "episode: 141/1000000, returns: 1e+01, epsilon: 0.93\n",
      "regression loss:  16122.446\n",
      "regression loss:  21107.742\n",
      "episode: 142/1000000, returns: -5.8, epsilon: 0.93\n",
      "regression loss:  1806.8732\n",
      "regression loss:  24765.867\n",
      "episode: 143/1000000, returns: -5.4, epsilon: 0.93\n",
      "regression loss:  5509.0938\n",
      "regression loss:  31049.033\n",
      "episode: 144/1000000, returns: 5.4, epsilon: 0.93\n",
      "regression loss:  3565.954\n",
      "regression loss:  30318.768\n",
      "episode: 145/1000000, returns: 2.6, epsilon: 0.93\n",
      "regression loss:  7390.511\n",
      "regression loss:  22121.736\n",
      "episode: 146/1000000, returns: 0.13, epsilon: 0.93\n",
      "regression loss:  2773.4001\n",
      "regression loss:  26018.943\n",
      "episode: 147/1000000, returns: 0.7, epsilon: 0.93\n",
      "regression loss:  2410.1287\n",
      "regression loss:  15158.755\n",
      "episode: 148/1000000, returns: -3.9, epsilon: 0.93\n",
      "regression loss:  2964.3528\n",
      "regression loss:  16444.322\n",
      "episode: 149/1000000, returns: -0.26, epsilon: 0.93\n",
      "regression loss:  2423.9036\n",
      "regression loss:  15102.971\n",
      "episode: 150/1000000, returns: 5.5, epsilon: 0.93\n",
      "regression loss:  955.1012\n",
      "regression loss:  10092.715\n",
      "episode: 151/1000000, returns: -0.073, epsilon: 0.93\n",
      "regression loss:  4587.669\n",
      "regression loss:  8058.129\n",
      "episode: 152/1000000, returns: -5.6, epsilon: 0.93\n",
      "regression loss:  2784.6265\n",
      "regression loss:  9133.414\n",
      "episode: 153/1000000, returns: -3.5, epsilon: 0.93\n",
      "regression loss:  2592.0532\n",
      "regression loss:  8102.84\n",
      "episode: 154/1000000, returns: 2.1, epsilon: 0.93\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "regression loss:  3768.425\n",
      "regression loss:  6878.1436\n",
      "episode: 155/1000000, returns: -1.3, epsilon: 0.93\n",
      "regression loss:  3082.7708\n",
      "regression loss:  12488.851\n",
      "episode: 156/1000000, returns: 1.2, epsilon: 0.92\n",
      "regression loss:  7341.675\n",
      "regression loss:  13121.358\n",
      "episode: 157/1000000, returns: 5.3, epsilon: 0.92\n",
      "regression loss:  3815.5354\n",
      "regression loss:  12020.909\n",
      "episode: 158/1000000, returns: 0.47, epsilon: 0.92\n",
      "regression loss:  810.0279\n",
      "regression loss:  11814.415\n",
      "episode: 159/1000000, returns: -3.4, epsilon: 0.92\n",
      "regression loss:  8491.355\n",
      "regression loss:  12223.468\n",
      "episode: 160/1000000, returns: 0.61, epsilon: 0.92\n",
      "regression loss:  1354.1929\n",
      "regression loss:  12334.484\n",
      "episode: 161/1000000, returns: -0.78, epsilon: 0.92\n",
      "regression loss:  13211.846\n",
      "regression loss:  28685.205\n",
      "episode: 162/1000000, returns: 2.8, epsilon: 0.92\n",
      "regression loss:  12450.787\n",
      "regression loss:  12550.444\n",
      "episode: 163/1000000, returns: 9.8, epsilon: 0.92\n",
      "regression loss:  2722.089\n",
      "regression loss:  16255.182\n",
      "episode: 164/1000000, returns: 2.5, epsilon: 0.92\n",
      "regression loss:  13715.135\n",
      "regression loss:  22361.727\n",
      "episode: 165/1000000, returns: -0.64, epsilon: 0.92\n",
      "regression loss:  749.5804\n",
      "regression loss:  14358.131\n",
      "episode: 166/1000000, returns: 2.1, epsilon: 0.92\n",
      "regression loss:  9772.556\n",
      "regression loss:  16778.574\n",
      "episode: 167/1000000, returns: 1.3, epsilon: 0.92\n",
      "regression loss:  1707.4473\n",
      "regression loss:  14960.701\n",
      "episode: 168/1000000, returns: 0.51, epsilon: 0.92\n",
      "regression loss:  12934.46\n",
      "regression loss:  25915.68\n",
      "episode: 169/1000000, returns: -0.029, epsilon: 0.92\n",
      "regression loss:  5772.108\n",
      "regression loss:  14757.279\n",
      "episode: 170/1000000, returns: -6.4, epsilon: 0.92\n",
      "regression loss:  11128.345\n",
      "regression loss:  34149.414\n",
      "episode: 171/1000000, returns: 3.3, epsilon: 0.92\n",
      "regression loss:  12836.588\n",
      "regression loss:  16432.357\n",
      "episode: 172/1000000, returns: 2.5, epsilon: 0.92\n",
      "regression loss:  1731.0718\n",
      "regression loss:  13658.354\n",
      "episode: 173/1000000, returns: 0.63, epsilon: 0.92\n",
      "regression loss:  4455.784\n",
      "regression loss:  11668.081\n",
      "episode: 174/1000000, returns: 0.71, epsilon: 0.92\n",
      "regression loss:  2898.9688\n",
      "regression loss:  17716.016\n",
      "episode: 175/1000000, returns: 2.1, epsilon: 0.92\n",
      "regression loss:  3461.1406\n",
      "regression loss:  11161.831\n",
      "episode: 176/1000000, returns: -0.74, epsilon: 0.92\n",
      "regression loss:  2578.1255\n",
      "regression loss:  13799.658\n",
      "episode: 177/1000000, returns: 1.4, epsilon: 0.92\n",
      "regression loss:  3040.1533\n",
      "regression loss:  12248.088\n",
      "episode: 178/1000000, returns: -4.3, epsilon: 0.91\n",
      "regression loss:  4655.367\n",
      "regression loss:  19102.414\n",
      "episode: 179/1000000, returns: 3.0, epsilon: 0.91\n",
      "regression loss:  3773.371\n",
      "regression loss:  14621.939\n",
      "episode: 180/1000000, returns: 1.2, epsilon: 0.91\n",
      "regression loss:  3722.1116\n",
      "regression loss:  11017.262\n",
      "episode: 181/1000000, returns: 1.2, epsilon: 0.91\n",
      "regression loss:  1854.5463\n",
      "regression loss:  13345.74\n",
      "episode: 182/1000000, returns: -0.043, epsilon: 0.91\n",
      "regression loss:  9457.286\n",
      "regression loss:  18567.836\n",
      "episode: 183/1000000, returns: 3.1, epsilon: 0.91\n",
      "regression loss:  11629.344\n",
      "regression loss:  13076.887\n",
      "episode: 184/1000000, returns: -4.2, epsilon: 0.91\n",
      "regression loss:  5377.1504\n",
      "regression loss:  20223.75\n",
      "episode: 185/1000000, returns: -1.2e+01, epsilon: 0.91\n",
      "regression loss:  15147.185\n",
      "regression loss:  15671.694\n",
      "episode: 186/1000000, returns: -8.0, epsilon: 0.91\n",
      "regression loss:  2004.4619\n",
      "regression loss:  29311.303\n",
      "episode: 187/1000000, returns: 1.5, epsilon: 0.91\n",
      "regression loss:  16306.795\n",
      "regression loss:  26884.895\n",
      "episode: 188/1000000, returns: 6.0, epsilon: 0.91\n",
      "regression loss:  4671.8003\n",
      "regression loss:  18445.266\n",
      "episode: 189/1000000, returns: 2.6, epsilon: 0.91\n",
      "regression loss:  2094.7031\n",
      "regression loss:  16855.143\n",
      "episode: 190/1000000, returns: 4.0, epsilon: 0.91\n",
      "regression loss:  794.7154\n",
      "regression loss:  12544.76\n",
      "episode: 191/1000000, returns: -5.7, epsilon: 0.91\n",
      "regression loss:  5080.057\n",
      "regression loss:  15627.374\n",
      "episode: 192/1000000, returns: 8.3, epsilon: 0.91\n",
      "regression loss:  448.4132\n",
      "regression loss:  10195.738\n",
      "episode: 193/1000000, returns: 1.2e+01, epsilon: 0.91\n",
      "regression loss:  7944.3423\n",
      "regression loss:  13276.879\n",
      "episode: 194/1000000, returns: 7.8, epsilon: 0.91\n",
      "regression loss:  1159.484\n",
      "regression loss:  6773.426\n",
      "episode: 195/1000000, returns: -0.33, epsilon: 0.91\n",
      "regression loss:  2048.2734\n",
      "regression loss:  7407.993\n",
      "episode: 196/1000000, returns: 1.3, epsilon: 0.91\n",
      "regression loss:  454.11932\n",
      "regression loss:  8477.386\n",
      "episode: 197/1000000, returns: 2.7, epsilon: 0.91\n",
      "regression loss:  1719.3857\n",
      "regression loss:  4310.788\n",
      "episode: 198/1000000, returns: 2.3, epsilon: 0.91\n",
      "regression loss:  290.19003\n",
      "regression loss:  4786.6553\n",
      "episode: 199/1000000, returns: 1.3, epsilon: 0.91\n",
      "regression loss:  1688.701\n",
      "regression loss:  4409.9697\n",
      "episode: 200/1000000, returns: 1.4, epsilon: 0.9\n",
      "regression loss:  499.63446\n",
      "regression loss:  3060.7292\n",
      "episode: 201/1000000, returns: -0.57, epsilon: 0.9\n",
      "regression loss:  1364.9789\n",
      "regression loss:  3350.7046\n",
      "episode: 202/1000000, returns: -2.0, epsilon: 0.9\n",
      "regression loss:  176.76808\n",
      "regression loss:  3485.985\n",
      "episode: 203/1000000, returns: 4.2, epsilon: 0.9\n",
      "regression loss:  460.53836\n",
      "regression loss:  1829.0331\n",
      "episode: 204/1000000, returns: -9.0, epsilon: 0.9\n",
      "regression loss:  2064.936\n",
      "regression loss:  3477.0854\n",
      "episode: 205/1000000, returns: -5.2, epsilon: 0.9\n",
      "regression loss:  155.73361\n",
      "regression loss:  1669.2795\n",
      "episode: 206/1000000, returns: 4.2, epsilon: 0.9\n",
      "regression loss:  120.42903\n",
      "regression loss:  2236.128\n",
      "episode: 207/1000000, returns: 1.2, epsilon: 0.9\n",
      "regression loss:  476.96994\n",
      "regression loss:  1623.9583\n",
      "episode: 208/1000000, returns: 5.1, epsilon: 0.9\n",
      "regression loss:  2922.5454\n",
      "regression loss:  2627.8257\n",
      "episode: 209/1000000, returns: -5.9, epsilon: 0.9\n",
      "regression loss:  2104.803\n",
      "regression loss:  4057.3828\n",
      "episode: 210/1000000, returns: -8.9, epsilon: 0.9\n",
      "regression loss:  2594.436\n",
      "regression loss:  8946.656\n",
      "episode: 211/1000000, returns: 1.4, epsilon: 0.9\n",
      "regression loss:  5835.0166\n",
      "regression loss:  4717.5234\n",
      "episode: 212/1000000, returns: -1.4, epsilon: 0.9\n",
      "regression loss:  688.69666\n",
      "regression loss:  7776.9043\n",
      "episode: 213/1000000, returns: 5.1, epsilon: 0.9\n",
      "regression loss:  23593.887\n",
      "regression loss:  20661.002\n",
      "episode: 214/1000000, returns: 1.8, epsilon: 0.9\n",
      "regression loss:  20736.271\n",
      "regression loss:  10888.346\n",
      "episode: 215/1000000, returns: 7.6, epsilon: 0.9\n",
      "regression loss:  1054.8411\n",
      "regression loss:  13983.969\n",
      "episode: 216/1000000, returns: 1.8, epsilon: 0.9\n",
      "regression loss:  3449.1772\n",
      "regression loss:  7860.6685\n",
      "episode: 217/1000000, returns: 1.7, epsilon: 0.9\n",
      "regression loss:  994.3401\n",
      "regression loss:  10448.201\n",
      "episode: 218/1000000, returns: 3.6, epsilon: 0.9\n",
      "regression loss:  452.0254\n",
      "regression loss:  7649.3945\n",
      "episode: 219/1000000, returns: 0.44, epsilon: 0.9\n",
      "regression loss:  5765.817\n",
      "regression loss:  11856.273\n",
      "episode: 220/1000000, returns: 2.1, epsilon: 0.9\n",
      "regression loss:  609.0191\n",
      "regression loss:  8369.557\n",
      "episode: 221/1000000, returns: 6.2, epsilon: 0.9\n",
      "regression loss:  578.94775\n",
      "regression loss:  4857.541\n",
      "episode: 222/1000000, returns: 0.3, epsilon: 0.89\n",
      "regression loss:  4531.359\n",
      "regression loss:  6803.0464\n",
      "episode: 223/1000000, returns: 3.0, epsilon: 0.89\n",
      "regression loss:  428.38562\n",
      "regression loss:  3788.2644\n",
      "episode: 224/1000000, returns: -3.2, epsilon: 0.89\n",
      "regression loss:  308.26645\n",
      "regression loss:  2789.8606\n",
      "episode: 225/1000000, returns: -4.1, epsilon: 0.89\n",
      "regression loss:  243.98332\n",
      "regression loss:  2237.6577\n",
      "episode: 226/1000000, returns: 2.7, epsilon: 0.89\n",
      "regression loss:  1915.2675\n",
      "regression loss:  1827.8906\n",
      "episode: 227/1000000, returns: -2.4, epsilon: 0.89\n",
      "regression loss:  482.45645\n",
      "regression loss:  2236.2554\n",
      "episode: 228/1000000, returns: 0.51, epsilon: 0.89\n",
      "regression loss:  217.90652\n",
      "regression loss:  3364.6655\n",
      "episode: 229/1000000, returns: 4.9, epsilon: 0.89\n",
      "regression loss:  341.09882\n",
      "regression loss:  3035.892\n",
      "episode: 230/1000000, returns: 1.4, epsilon: 0.89\n",
      "regression loss:  3187.7285\n",
      "regression loss:  2362.8787\n",
      "episode: 231/1000000, returns: 3.0, epsilon: 0.89\n",
      "regression loss:  1487.3708\n",
      "regression loss:  3455.495\n",
      "episode: 232/1000000, returns: 4.3, epsilon: 0.89\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "regression loss:  1047.2958\n",
      "regression loss:  4578.1416\n",
      "episode: 233/1000000, returns: -5.0, epsilon: 0.89\n",
      "regression loss:  3142.888\n",
      "regression loss:  4888.7773\n",
      "episode: 234/1000000, returns: -5.9, epsilon: 0.89\n",
      "regression loss:  562.5091\n",
      "regression loss:  5780.2246\n",
      "episode: 235/1000000, returns: 0.12, epsilon: 0.89\n",
      "regression loss:  1725.2408\n",
      "regression loss:  5061.853\n",
      "episode: 236/1000000, returns: 6.0, epsilon: 0.89\n",
      "regression loss:  2972.8398\n",
      "regression loss:  4036.477\n",
      "episode: 237/1000000, returns: -0.89, epsilon: 0.89\n",
      "regression loss:  2564.5178\n",
      "regression loss:  3693.8123\n",
      "episode: 238/1000000, returns: 0.098, epsilon: 0.89\n",
      "regression loss:  5307.1655\n",
      "regression loss:  12001.965\n",
      "episode: 239/1000000, returns: -0.8, epsilon: 0.89\n",
      "regression loss:  5597.025\n",
      "regression loss:  10773.239\n",
      "episode: 240/1000000, returns: 0.49, epsilon: 0.89\n",
      "regression loss:  5434.0063\n",
      "regression loss:  9566.207\n",
      "episode: 241/1000000, returns: -0.64, epsilon: 0.89\n",
      "regression loss:  2294.129\n",
      "regression loss:  14369.119\n",
      "episode: 242/1000000, returns: 3.9, epsilon: 0.89\n",
      "regression loss:  11069.191\n",
      "regression loss:  20771.809\n",
      "episode: 243/1000000, returns: 4.8, epsilon: 0.89\n",
      "regression loss:  13251.572\n",
      "regression loss:  8695.133\n",
      "episode: 244/1000000, returns: -7.4, epsilon: 0.89\n",
      "regression loss:  1666.9049\n",
      "regression loss:  17372.74\n",
      "episode: 245/1000000, returns: 1.6, epsilon: 0.88\n",
      "regression loss:  2950.936\n",
      "regression loss:  10122.533\n",
      "episode: 246/1000000, returns: -0.82, epsilon: 0.88\n",
      "regression loss:  3432.6401\n",
      "regression loss:  21552.672\n",
      "episode: 247/1000000, returns: -1.3, epsilon: 0.88\n",
      "regression loss:  2161.6633\n",
      "regression loss:  12567.951\n",
      "episode: 248/1000000, returns: -2.8, epsilon: 0.88\n",
      "regression loss:  5362.8896\n",
      "regression loss:  11635.881\n",
      "episode: 249/1000000, returns: -3.6, epsilon: 0.88\n",
      "regression loss:  2544.508\n",
      "regression loss:  11445.853\n",
      "episode: 250/1000000, returns: 3.6, epsilon: 0.88\n",
      "regression loss:  7417.0083\n",
      "regression loss:  15675.467\n",
      "episode: 251/1000000, returns: 2.5, epsilon: 0.88\n",
      "regression loss:  4114.406\n",
      "regression loss:  8571.059\n",
      "episode: 252/1000000, returns: -1.8, epsilon: 0.88\n",
      "regression loss:  6423.7188\n",
      "regression loss:  8062.93\n",
      "episode: 253/1000000, returns: 0.089, epsilon: 0.88\n",
      "regression loss:  8593.727\n",
      "regression loss:  10508.943\n",
      "episode: 254/1000000, returns: 2.2, epsilon: 0.88\n",
      "regression loss:  2071.9353\n",
      "regression loss:  19267.283\n",
      "episode: 255/1000000, returns: 8.5, epsilon: 0.88\n",
      "regression loss:  3336.6973\n",
      "regression loss:  12577.423\n",
      "episode: 256/1000000, returns: -4.1, epsilon: 0.88\n",
      "regression loss:  3252.4087\n",
      "regression loss:  10113.085\n",
      "episode: 257/1000000, returns: 0.6, epsilon: 0.88\n",
      "regression loss:  2036.1774\n",
      "regression loss:  7456.735\n",
      "episode: 258/1000000, returns: 4.1, epsilon: 0.88\n",
      "regression loss:  7341.4956\n",
      "regression loss:  12666.199\n",
      "episode: 259/1000000, returns: 1.8, epsilon: 0.88\n",
      "regression loss:  2217.4983\n",
      "regression loss:  16916.246\n",
      "episode: 260/1000000, returns: -0.64, epsilon: 0.88\n",
      "regression loss:  6062.561\n",
      "regression loss:  9692.434\n",
      "episode: 261/1000000, returns: -1.9, epsilon: 0.88\n",
      "regression loss:  827.594\n",
      "regression loss:  11713.397\n",
      "episode: 262/1000000, returns: 8.3, epsilon: 0.88\n",
      "regression loss:  5001.247\n",
      "regression loss:  8165.178\n",
      "episode: 263/1000000, returns: 0.69, epsilon: 0.88\n",
      "regression loss:  2137.7034\n",
      "regression loss:  8312.841\n",
      "episode: 264/1000000, returns: 1.1, epsilon: 0.88\n",
      "regression loss:  1830.5262\n",
      "regression loss:  4199.218\n",
      "episode: 265/1000000, returns: 6.5, epsilon: 0.88\n",
      "regression loss:  6000.8853\n",
      "regression loss:  9313.973\n",
      "episode: 266/1000000, returns: -1.1, epsilon: 0.88\n",
      "regression loss:  638.01447\n",
      "regression loss:  8259.33\n",
      "episode: 267/1000000, returns: -0.062, epsilon: 0.87\n",
      "regression loss:  7350.615\n",
      "regression loss:  11628.887\n",
      "episode: 268/1000000, returns: 2.0, epsilon: 0.87\n",
      "regression loss:  719.5846\n",
      "regression loss:  11138.607\n",
      "episode: 269/1000000, returns: -6.2, epsilon: 0.87\n",
      "regression loss:  6402.498\n",
      "regression loss:  13602.326\n",
      "episode: 270/1000000, returns: 1.8, epsilon: 0.87\n",
      "regression loss:  800.6653\n",
      "regression loss:  8214.795\n",
      "episode: 271/1000000, returns: -9.4, epsilon: 0.87\n",
      "regression loss:  27885.332\n",
      "regression loss:  32067.844\n",
      "episode: 272/1000000, returns: 2.5, epsilon: 0.87\n",
      "regression loss:  50680.043\n",
      "regression loss:  28623.98\n",
      "episode: 273/1000000, returns: 3.2, epsilon: 0.87\n",
      "regression loss:  5130.2017\n",
      "regression loss:  14468.811\n",
      "episode: 274/1000000, returns: -2.7, epsilon: 0.87\n",
      "regression loss:  9365.414\n",
      "regression loss:  22101.775\n",
      "episode: 275/1000000, returns: -2.3, epsilon: 0.87\n",
      "regression loss:  9854.993\n",
      "regression loss:  12011.707\n",
      "episode: 276/1000000, returns: -0.38, epsilon: 0.87\n",
      "regression loss:  5207.896\n",
      "regression loss:  20812.477\n",
      "episode: 277/1000000, returns: 1.5, epsilon: 0.87\n",
      "regression loss:  16843.1\n",
      "regression loss:  20989.174\n",
      "episode: 278/1000000, returns: 4.3, epsilon: 0.87\n",
      "regression loss:  733.04376\n",
      "regression loss:  28823.342\n",
      "episode: 279/1000000, returns: 1.3, epsilon: 0.87\n",
      "regression loss:  7065.1855\n",
      "regression loss:  23511.555\n",
      "episode: 280/1000000, returns: 1.2, epsilon: 0.87\n",
      "regression loss:  4730.294\n",
      "regression loss:  17997.508\n",
      "episode: 281/1000000, returns: 8.2, epsilon: 0.87\n",
      "regression loss:  3512.2341\n",
      "regression loss:  19851.932\n",
      "episode: 282/1000000, returns: -0.12, epsilon: 0.87\n",
      "regression loss:  957.4874\n",
      "regression loss:  16298.451\n",
      "episode: 283/1000000, returns: 1.1e+01, epsilon: 0.87\n",
      "regression loss:  4124.167\n",
      "regression loss:  11993.109\n",
      "episode: 284/1000000, returns: 9.6, epsilon: 0.87\n",
      "regression loss:  456.6672\n",
      "regression loss:  8730.596\n",
      "episode: 285/1000000, returns: 3.1, epsilon: 0.87\n",
      "regression loss:  8508.791\n",
      "regression loss:  10277.73\n",
      "episode: 286/1000000, returns: -0.15, epsilon: 0.87\n",
      "regression loss:  384.53662\n",
      "regression loss:  6076.1436\n",
      "episode: 287/1000000, returns: -8.6, epsilon: 0.87\n",
      "regression loss:  2503.673\n",
      "regression loss:  5505.8213\n",
      "episode: 288/1000000, returns: -3.3, epsilon: 0.87\n",
      "regression loss:  3193.9202\n",
      "regression loss:  9643.233\n",
      "episode: 289/1000000, returns: -1.1, epsilon: 0.87\n",
      "regression loss:  2820.4429\n",
      "regression loss:  12048.321\n",
      "episode: 290/1000000, returns: 0.52, epsilon: 0.86\n",
      "regression loss:  5178.008\n",
      "regression loss:  6574.8774\n",
      "episode: 291/1000000, returns: 1.2, epsilon: 0.86\n",
      "regression loss:  7362.757\n",
      "regression loss:  7724.6855\n",
      "episode: 292/1000000, returns: 0.7, epsilon: 0.86\n",
      "regression loss:  12194.755\n",
      "regression loss:  18601.66\n",
      "episode: 293/1000000, returns: -7.4, epsilon: 0.86\n",
      "regression loss:  18332.236\n",
      "regression loss:  22695.828\n",
      "episode: 294/1000000, returns: 1.5, epsilon: 0.86\n",
      "regression loss:  2206.3887\n",
      "regression loss:  23124.29\n",
      "episode: 295/1000000, returns: 3.5, epsilon: 0.86\n",
      "regression loss:  9092.925\n",
      "regression loss:  16688.62\n",
      "episode: 296/1000000, returns: -0.022, epsilon: 0.86\n",
      "regression loss:  1794.8374\n",
      "regression loss:  13213.765\n",
      "episode: 297/1000000, returns: -0.3, epsilon: 0.86\n",
      "regression loss:  16944.293\n",
      "regression loss:  24841.195\n",
      "episode: 298/1000000, returns: 3.1, epsilon: 0.86\n",
      "regression loss:  7029.7075\n",
      "regression loss:  15341.84\n",
      "episode: 299/1000000, returns: 1.6, epsilon: 0.86\n",
      "regression loss:  5179.0156\n",
      "regression loss:  40774.414\n",
      "episode: 300/1000000, returns: 6.9, epsilon: 0.86\n",
      "regression loss:  8823.892\n",
      "regression loss:  26607.74\n",
      "episode: 301/1000000, returns: 6.1, epsilon: 0.86\n",
      "regression loss:  4266.9116\n",
      "regression loss:  20108.457\n",
      "episode: 302/1000000, returns: -9.6, epsilon: 0.86\n",
      "regression loss:  8709.03\n",
      "regression loss:  19427.01\n",
      "episode: 303/1000000, returns: -0.15, epsilon: 0.86\n",
      "regression loss:  3535.4707\n",
      "regression loss:  28492.318\n",
      "episode: 304/1000000, returns: 2.7, epsilon: 0.86\n",
      "regression loss:  4942.6514\n",
      "regression loss:  29603.545\n",
      "episode: 305/1000000, returns: 4.3, epsilon: 0.86\n",
      "regression loss:  1286.9305\n",
      "regression loss:  24575.955\n",
      "episode: 306/1000000, returns: -2.5, epsilon: 0.86\n",
      "regression loss:  4275.6177\n",
      "regression loss:  28587.81\n",
      "episode: 307/1000000, returns: -1.1, epsilon: 0.86\n",
      "regression loss:  2719.863\n",
      "regression loss:  31548.242\n",
      "episode: 308/1000000, returns: 3.4, epsilon: 0.86\n",
      "regression loss:  4979.657\n",
      "regression loss:  16492.748\n",
      "episode: 309/1000000, returns: -3.2, epsilon: 0.86\n",
      "regression loss:  1360.5342\n",
      "regression loss:  15847.49\n",
      "episode: 310/1000000, returns: 0.81, epsilon: 0.86\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "regression loss:  1162.0879\n",
      "regression loss:  12566.844\n",
      "episode: 311/1000000, returns: -1.5, epsilon: 0.86\n",
      "regression loss:  5501.062\n",
      "regression loss:  16051.07\n",
      "episode: 312/1000000, returns: 7.0, epsilon: 0.86\n",
      "regression loss:  2792.6062\n",
      "regression loss:  11732.633\n",
      "episode: 313/1000000, returns: -2.7, epsilon: 0.86\n",
      "regression loss:  6027.829\n",
      "regression loss:  18179.883\n",
      "episode: 314/1000000, returns: -0.41, epsilon: 0.85\n",
      "regression loss:  2526.6401\n",
      "regression loss:  19250.3\n",
      "episode: 315/1000000, returns: -3.5, epsilon: 0.85\n",
      "regression loss:  5979.611\n",
      "regression loss:  11822.373\n",
      "episode: 316/1000000, returns: -5.4, epsilon: 0.85\n",
      "regression loss:  3502.2122\n",
      "regression loss:  15432.463\n",
      "episode: 317/1000000, returns: 6.5, epsilon: 0.85\n",
      "regression loss:  1975.4668\n",
      "regression loss:  16053.343\n",
      "episode: 318/1000000, returns: 1.5, epsilon: 0.85\n",
      "regression loss:  1565.4067\n",
      "regression loss:  12156.78\n",
      "episode: 319/1000000, returns: 0.33, epsilon: 0.85\n",
      "regression loss:  3287.1272\n",
      "regression loss:  11104.214\n",
      "episode: 320/1000000, returns: 1.9, epsilon: 0.85\n",
      "regression loss:  967.25745\n",
      "regression loss:  13646.844\n",
      "episode: 321/1000000, returns: 5.6, epsilon: 0.85\n",
      "regression loss:  2422.137\n",
      "regression loss:  10243.096\n",
      "episode: 322/1000000, returns: -4.0, epsilon: 0.85\n",
      "regression loss:  864.9084\n",
      "regression loss:  9471.182\n",
      "episode: 323/1000000, returns: 9.6, epsilon: 0.85\n",
      "regression loss:  2286.1287\n",
      "regression loss:  9551.031\n",
      "episode: 324/1000000, returns: 0.18, epsilon: 0.85\n",
      "regression loss:  800.41266\n",
      "regression loss:  6582.468\n",
      "episode: 325/1000000, returns: 0.73, epsilon: 0.85\n",
      "regression loss:  5043.4546\n",
      "regression loss:  2103.2742\n",
      "episode: 326/1000000, returns: -1e+01, epsilon: 0.85\n",
      "regression loss:  1133.5297\n",
      "regression loss:  4741.5146\n",
      "episode: 327/1000000, returns: 0.99, epsilon: 0.85\n",
      "regression loss:  557.9399\n",
      "regression loss:  3306.83\n",
      "episode: 328/1000000, returns: 4.9, epsilon: 0.85\n",
      "regression loss:  205.51245\n",
      "regression loss:  2178.2017\n",
      "episode: 329/1000000, returns: 0.39, epsilon: 0.85\n",
      "regression loss:  205.22469\n",
      "regression loss:  2866.1416\n",
      "episode: 330/1000000, returns: 2.3, epsilon: 0.85\n",
      "regression loss:  144.28503\n",
      "regression loss:  1165.7388\n",
      "episode: 331/1000000, returns: -7.4, epsilon: 0.85\n",
      "regression loss:  218.54942\n",
      "regression loss:  1519.0623\n",
      "episode: 332/1000000, returns: -0.0053, epsilon: 0.85\n",
      "regression loss:  1757.2616\n",
      "regression loss:  928.28595\n",
      "episode: 333/1000000, returns: -2.9, epsilon: 0.85\n",
      "regression loss:  3205.501\n",
      "regression loss:  4353.3276\n",
      "episode: 334/1000000, returns: 2.8, epsilon: 0.85\n",
      "regression loss:  1243.6648\n",
      "regression loss:  4404.0034\n",
      "episode: 335/1000000, returns: -2.9, epsilon: 0.85\n",
      "regression loss:  2579.2131\n",
      "regression loss:  5945.8525\n",
      "episode: 336/1000000, returns: 3.4, epsilon: 0.85\n",
      "regression loss:  622.53925\n",
      "regression loss:  7875.588\n",
      "episode: 337/1000000, returns: 4.1, epsilon: 0.84\n",
      "regression loss:  19135.967\n",
      "regression loss:  22296.959\n",
      "episode: 338/1000000, returns: 4.3, epsilon: 0.84\n",
      "regression loss:  6314.344\n",
      "regression loss:  4483.453\n",
      "episode: 339/1000000, returns: -3.6, epsilon: 0.84\n",
      "regression loss:  9262.551\n",
      "regression loss:  28878.793\n",
      "episode: 340/1000000, returns: 7.9, epsilon: 0.84\n",
      "regression loss:  24314.498\n",
      "regression loss:  12845.345\n",
      "episode: 341/1000000, returns: -6.3, epsilon: 0.84\n",
      "regression loss:  2072.4316\n",
      "regression loss:  17109.947\n",
      "episode: 342/1000000, returns: 3.7, epsilon: 0.84\n",
      "regression loss:  23514.297\n",
      "regression loss:  44321.297\n",
      "episode: 343/1000000, returns: 8.4, epsilon: 0.84\n",
      "regression loss:  31102.297\n",
      "regression loss:  25662.469\n",
      "episode: 344/1000000, returns: 7.7, epsilon: 0.84\n",
      "regression loss:  4174.902\n",
      "regression loss:  25749.836\n",
      "episode: 345/1000000, returns: 1.0, epsilon: 0.84\n",
      "regression loss:  11499.378\n",
      "regression loss:  21753.344\n",
      "episode: 346/1000000, returns: 4.3, epsilon: 0.84\n",
      "regression loss:  16434.678\n",
      "regression loss:  19015.432\n",
      "episode: 347/1000000, returns: 2.3, epsilon: 0.84\n",
      "regression loss:  1079.6626\n",
      "regression loss:  31402.656\n",
      "episode: 348/1000000, returns: 1.3, epsilon: 0.84\n",
      "regression loss:  6348.068\n",
      "regression loss:  27954.383\n",
      "episode: 349/1000000, returns: 2.8, epsilon: 0.84\n",
      "regression loss:  2292.857\n",
      "regression loss:  19080.295\n",
      "episode: 350/1000000, returns: -1.1, epsilon: 0.84\n",
      "regression loss:  14831.777\n",
      "regression loss:  26760.371\n",
      "episode: 351/1000000, returns: 2.4, epsilon: 0.84\n",
      "regression loss:  5554.7544\n",
      "regression loss:  12023.014\n",
      "episode: 352/1000000, returns: 4.5, epsilon: 0.84\n",
      "regression loss:  1364.1691\n",
      "regression loss:  14669.473\n",
      "episode: 353/1000000, returns: 4.8, epsilon: 0.84\n",
      "regression loss:  1578.1775\n",
      "regression loss:  8776.367\n",
      "episode: 354/1000000, returns: -7.4, epsilon: 0.84\n",
      "regression loss:  8593.362\n",
      "regression loss:  10692.372\n",
      "episode: 355/1000000, returns: 6.0, epsilon: 0.84\n",
      "regression loss:  6699.5894\n",
      "regression loss:  7090.2866\n",
      "episode: 356/1000000, returns: -3.9, epsilon: 0.84\n",
      "regression loss:  1286.0164\n",
      "regression loss:  10574.906\n",
      "episode: 357/1000000, returns: 1.6, epsilon: 0.84\n",
      "regression loss:  5451.2373\n",
      "regression loss:  11282.879\n",
      "episode: 358/1000000, returns: 1.5, epsilon: 0.84\n",
      "regression loss:  1784.9474\n",
      "regression loss:  4356.0977\n",
      "episode: 359/1000000, returns: -4.7, epsilon: 0.84\n",
      "regression loss:  3007.715\n",
      "regression loss:  3938.7002\n",
      "episode: 360/1000000, returns: -0.61, epsilon: 0.84\n",
      "regression loss:  1364.8829\n",
      "regression loss:  4548.327\n",
      "episode: 361/1000000, returns: 5.1, epsilon: 0.83\n",
      "regression loss:  1415.13\n",
      "regression loss:  3608.0244\n",
      "episode: 362/1000000, returns: -6.3, epsilon: 0.83\n",
      "regression loss:  1004.3551\n",
      "regression loss:  5291.907\n",
      "episode: 363/1000000, returns: 1.6, epsilon: 0.83\n",
      "regression loss:  2118.3416\n",
      "regression loss:  8391.978\n",
      "episode: 364/1000000, returns: -0.69, epsilon: 0.83\n",
      "regression loss:  839.0943\n",
      "regression loss:  5131.0522\n",
      "episode: 365/1000000, returns: -3.3, epsilon: 0.83\n",
      "regression loss:  8904.493\n",
      "regression loss:  4121.3936\n",
      "episode: 366/1000000, returns: -2.8, epsilon: 0.83\n",
      "regression loss:  1555.2644\n",
      "regression loss:  5022.6924\n",
      "episode: 367/1000000, returns: 2.9, epsilon: 0.83\n",
      "regression loss:  13104.276\n",
      "regression loss:  14194.374\n",
      "episode: 368/1000000, returns: -0.64, epsilon: 0.83\n",
      "regression loss:  2877.6243\n",
      "regression loss:  22159.824\n",
      "episode: 369/1000000, returns: 1.8, epsilon: 0.83\n",
      "regression loss:  12189.507\n",
      "regression loss:  15573.348\n",
      "episode: 370/1000000, returns: 2.7, epsilon: 0.83\n",
      "regression loss:  7933.673\n",
      "regression loss:  7032.0093\n",
      "episode: 371/1000000, returns: 0.37, epsilon: 0.83\n",
      "regression loss:  2111.5806\n",
      "regression loss:  19955.697\n",
      "episode: 372/1000000, returns: 2.4, epsilon: 0.83\n",
      "regression loss:  5325.285\n",
      "regression loss:  11117.954\n",
      "episode: 373/1000000, returns: 1.9, epsilon: 0.83\n",
      "regression loss:  1815.8236\n",
      "regression loss:  14142.4375\n",
      "episode: 374/1000000, returns: -0.75, epsilon: 0.83\n",
      "regression loss:  4943.5806\n",
      "regression loss:  17640.04\n",
      "episode: 375/1000000, returns: -1.5, epsilon: 0.83\n",
      "regression loss:  1276.214\n",
      "regression loss:  17607.693\n",
      "episode: 376/1000000, returns: -0.76, epsilon: 0.83\n",
      "regression loss:  4343.212\n",
      "regression loss:  8803.928\n",
      "episode: 377/1000000, returns: 0.72, epsilon: 0.83\n",
      "regression loss:  757.3343\n",
      "regression loss:  9999.848\n",
      "episode: 378/1000000, returns: -1e+01, epsilon: 0.83\n",
      "regression loss:  9836.781\n",
      "regression loss:  15293.865\n",
      "episode: 379/1000000, returns: 5.8, epsilon: 0.83\n",
      "regression loss:  765.1945\n",
      "regression loss:  10339.69\n",
      "episode: 380/1000000, returns: -1.6, epsilon: 0.83\n",
      "regression loss:  7088.175\n",
      "regression loss:  15677.433\n",
      "episode: 381/1000000, returns: 6.6, epsilon: 0.83\n",
      "regression loss:  14060.424\n",
      "regression loss:  10909.887\n",
      "episode: 382/1000000, returns: -7.2, epsilon: 0.83\n",
      "regression loss:  3553.2576\n",
      "regression loss:  12618.176\n",
      "episode: 383/1000000, returns: -3.7, epsilon: 0.83\n",
      "regression loss:  4215.8857\n",
      "regression loss:  10227.86\n",
      "episode: 384/1000000, returns: 0.69, epsilon: 0.83\n",
      "regression loss:  11394.3955\n",
      "regression loss:  13058.581\n",
      "episode: 385/1000000, returns: 1.4, epsilon: 0.82\n",
      "regression loss:  8978.675\n",
      "regression loss:  14407.346\n",
      "episode: 386/1000000, returns: -1.1e+01, epsilon: 0.82\n",
      "regression loss:  1559.4863\n",
      "regression loss:  19968.785\n",
      "episode: 387/1000000, returns: 5.2, epsilon: 0.82\n",
      "regression loss:  11499.224\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "regression loss:  22028.371\n",
      "episode: 388/1000000, returns: 6.0, epsilon: 0.82\n",
      "regression loss:  4050.3145\n",
      "regression loss:  13237.632\n",
      "episode: 389/1000000, returns: -6.2, epsilon: 0.82\n",
      "regression loss:  17600.145\n",
      "regression loss:  25681.32\n",
      "episode: 390/1000000, returns: 1e+01, epsilon: 0.82\n",
      "regression loss:  7793.374\n",
      "regression loss:  16185.248\n",
      "episode: 391/1000000, returns: 1.6, epsilon: 0.82\n",
      "regression loss:  868.659\n",
      "regression loss:  13915.646\n",
      "episode: 392/1000000, returns: -1.7, epsilon: 0.82\n",
      "regression loss:  2275.8728\n",
      "regression loss:  9461.57\n",
      "episode: 393/1000000, returns: -0.7, epsilon: 0.82\n",
      "regression loss:  929.13696\n",
      "regression loss:  11357.543\n",
      "episode: 394/1000000, returns: 0.56, epsilon: 0.82\n",
      "regression loss:  967.80334\n",
      "regression loss:  6855.1206\n",
      "episode: 395/1000000, returns: -0.087, epsilon: 0.82\n",
      "regression loss:  1713.0393\n",
      "regression loss:  8174.6978\n",
      "episode: 396/1000000, returns: -4.1, epsilon: 0.82\n",
      "regression loss:  819.45435\n",
      "regression loss:  5652.1304\n",
      "episode: 397/1000000, returns: 1.1e+01, epsilon: 0.82\n",
      "regression loss:  1004.3835\n",
      "regression loss:  5684.335\n",
      "episode: 398/1000000, returns: 3.4, epsilon: 0.82\n",
      "regression loss:  503.31244\n",
      "regression loss:  3147.4263\n",
      "episode: 399/1000000, returns: 0.63, epsilon: 0.82\n",
      "regression loss:  1503.9246\n",
      "regression loss:  3577.9482\n",
      "episode: 400/1000000, returns: 1.9, epsilon: 0.82\n",
      "regression loss:  2244.204\n",
      "regression loss:  4982.48\n",
      "episode: 401/1000000, returns: 1.6, epsilon: 0.82\n",
      "regression loss:  220.6096\n",
      "regression loss:  5205.7646\n",
      "episode: 402/1000000, returns: -1.1, epsilon: 0.82\n",
      "regression loss:  1618.0841\n",
      "regression loss:  3220.3516\n",
      "episode: 403/1000000, returns: 2.1, epsilon: 0.82\n",
      "regression loss:  1579.7411\n",
      "regression loss:  5865.61\n",
      "episode: 404/1000000, returns: 7.6, epsilon: 0.82\n",
      "regression loss:  212.1321\n",
      "regression loss:  4564.2812\n",
      "episode: 405/1000000, returns: 4.1, epsilon: 0.82\n",
      "regression loss:  3653.1904\n",
      "regression loss:  3021.982\n",
      "episode: 406/1000000, returns: -0.69, epsilon: 0.82\n",
      "regression loss:  3487.9907\n",
      "regression loss:  5967.5986\n",
      "episode: 407/1000000, returns: -4.5, epsilon: 0.82\n",
      "regression loss:  6736.6763\n",
      "regression loss:  4871.94\n",
      "episode: 408/1000000, returns: 3.7, epsilon: 0.82\n",
      "regression loss:  5541.1094\n",
      "regression loss:  18996.666\n",
      "episode: 409/1000000, returns: 2.6, epsilon: 0.82\n",
      "regression loss:  13156.2705\n",
      "regression loss:  14922.408\n",
      "episode: 410/1000000, returns: 7.8, epsilon: 0.81\n",
      "regression loss:  1344.4584\n",
      "regression loss:  7619.5273\n",
      "episode: 411/1000000, returns: 1.4e+01, epsilon: 0.81\n",
      "regression loss:  11871.756\n",
      "regression loss:  23319.25\n",
      "episode: 412/1000000, returns: 0.073, epsilon: 0.81\n",
      "regression loss:  5991.431\n",
      "regression loss:  11281.89\n",
      "episode: 413/1000000, returns: 3.4, epsilon: 0.81\n",
      "regression loss:  3636.2075\n",
      "regression loss:  18991.912\n",
      "episode: 414/1000000, returns: -0.67, epsilon: 0.81\n",
      "regression loss:  14898.108\n",
      "regression loss:  18488.273\n",
      "episode: 415/1000000, returns: 6.1, epsilon: 0.81\n",
      "regression loss:  3647.4468\n",
      "regression loss:  9225.602\n",
      "episode: 416/1000000, returns: -2.4, epsilon: 0.81\n",
      "regression loss:  6065.737\n",
      "regression loss:  20161.664\n",
      "episode: 417/1000000, returns: 1.6, epsilon: 0.81\n",
      "regression loss:  14449.904\n",
      "regression loss:  20496.607\n",
      "episode: 418/1000000, returns: 0.76, epsilon: 0.81\n",
      "regression loss:  2546.7405\n",
      "regression loss:  11996.369\n",
      "episode: 419/1000000, returns: -4.8, epsilon: 0.81\n",
      "regression loss:  7647.2925\n",
      "regression loss:  8057.49\n",
      "episode: 420/1000000, returns: -0.93, epsilon: 0.81\n",
      "regression loss:  2806.78\n",
      "regression loss:  14319.359\n",
      "episode: 421/1000000, returns: 8.2, epsilon: 0.81\n",
      "regression loss:  2738.749\n",
      "regression loss:  9394.117\n",
      "episode: 422/1000000, returns: 0.61, epsilon: 0.81\n",
      "regression loss:  1454.4895\n",
      "regression loss:  12314.248\n",
      "episode: 423/1000000, returns: 7.1, epsilon: 0.81\n",
      "regression loss:  4129.7915\n",
      "regression loss:  10655.707\n",
      "episode: 424/1000000, returns: -3.9, epsilon: 0.81\n",
      "regression loss:  1556.9382\n",
      "regression loss:  6264.01\n",
      "episode: 425/1000000, returns: -0.35, epsilon: 0.81\n",
      "regression loss:  1829.6833\n",
      "regression loss:  8600.617\n",
      "episode: 426/1000000, returns: 5.2, epsilon: 0.81\n",
      "regression loss:  10568.248\n",
      "regression loss:  10849.912\n",
      "episode: 427/1000000, returns: 5.1, epsilon: 0.81\n",
      "regression loss:  4281.858\n",
      "regression loss:  6101.962\n",
      "episode: 428/1000000, returns: 7.7, epsilon: 0.81\n",
      "regression loss:  1983.5092\n",
      "regression loss:  9143.497\n",
      "episode: 429/1000000, returns: -0.65, epsilon: 0.81\n",
      "regression loss:  1654.8417\n",
      "regression loss:  4511.657\n",
      "episode: 430/1000000, returns: 1.4, epsilon: 0.81\n",
      "regression loss:  6836.997\n",
      "regression loss:  9932.098\n",
      "episode: 431/1000000, returns: 2.2, epsilon: 0.81\n",
      "regression loss:  3590.5754\n",
      "regression loss:  9875.736\n",
      "episode: 432/1000000, returns: 2.3, epsilon: 0.81\n",
      "regression loss:  3137.8079\n",
      "regression loss:  10787.124\n",
      "episode: 433/1000000, returns: 3.0, epsilon: 0.81\n",
      "regression loss:  1668.7219\n",
      "regression loss:  9211.072\n",
      "episode: 434/1000000, returns: -0.35, epsilon: 0.8\n",
      "regression loss:  3699.0662\n",
      "regression loss:  7797.8667\n",
      "episode: 435/1000000, returns: 3.1, epsilon: 0.8\n",
      "regression loss:  1277.5151\n",
      "regression loss:  8561.843\n",
      "episode: 436/1000000, returns: 3.7, epsilon: 0.8\n",
      "regression loss:  4287.689\n",
      "regression loss:  11675.0\n",
      "episode: 437/1000000, returns: 3.0, epsilon: 0.8\n",
      "regression loss:  433.61078\n",
      "regression loss:  6992.213\n",
      "episode: 438/1000000, returns: 0.36, epsilon: 0.8\n",
      "regression loss:  4006.5037\n",
      "regression loss:  3681.0298\n",
      "episode: 439/1000000, returns: 2.9, epsilon: 0.8\n",
      "regression loss:  1582.0128\n",
      "regression loss:  4075.2546\n",
      "episode: 440/1000000, returns: 2.7, epsilon: 0.8\n",
      "regression loss:  4534.6445\n",
      "regression loss:  8494.245\n",
      "episode: 441/1000000, returns: -0.22, epsilon: 0.8\n",
      "regression loss:  1176.3964\n",
      "regression loss:  8082.737\n",
      "episode: 442/1000000, returns: 3.7, epsilon: 0.8\n",
      "regression loss:  12201.793\n",
      "regression loss:  12046.487\n",
      "episode: 443/1000000, returns: 3.3, epsilon: 0.8\n",
      "regression loss:  5846.7134\n",
      "regression loss:  8564.797\n",
      "episode: 444/1000000, returns: 9.2, epsilon: 0.8\n",
      "regression loss:  10168.167\n",
      "regression loss:  9980.389\n",
      "episode: 445/1000000, returns: 6.4, epsilon: 0.8\n",
      "regression loss:  5268.847\n",
      "regression loss:  8530.549\n",
      "episode: 446/1000000, returns: -0.35, epsilon: 0.8\n",
      "regression loss:  2224.1802\n",
      "regression loss:  10716.484\n",
      "episode: 447/1000000, returns: 6.2, epsilon: 0.8\n",
      "regression loss:  8069.277\n",
      "regression loss:  8413.736\n",
      "episode: 448/1000000, returns: 1.8, epsilon: 0.8\n",
      "regression loss:  10125.764\n",
      "regression loss:  15313.182\n",
      "episode: 449/1000000, returns: 2.3, epsilon: 0.8\n",
      "regression loss:  18374.29\n",
      "regression loss:  16885.707\n",
      "episode: 450/1000000, returns: 8.4, epsilon: 0.8\n",
      "regression loss:  4974.8345\n",
      "regression loss:  8367.416\n",
      "episode: 451/1000000, returns: -3.0, epsilon: 0.8\n",
      "regression loss:  4319.3594\n",
      "regression loss:  27374.674\n",
      "episode: 452/1000000, returns: 1.7e+01, epsilon: 0.8\n",
      "regression loss:  12820.282\n",
      "regression loss:  16900.348\n",
      "episode: 453/1000000, returns: 3.1, epsilon: 0.8\n",
      "regression loss:  7703.442\n",
      "regression loss:  26830.012\n",
      "episode: 454/1000000, returns: -0.36, epsilon: 0.8\n",
      "regression loss:  17642.834\n",
      "regression loss:  27354.572\n",
      "episode: 455/1000000, returns: -4.5, epsilon: 0.8\n",
      "regression loss:  4686.7524\n",
      "regression loss:  39644.89\n",
      "episode: 456/1000000, returns: -0.27, epsilon: 0.8\n",
      "regression loss:  17362.047\n",
      "regression loss:  28000.336\n",
      "episode: 457/1000000, returns: -4.2, epsilon: 0.8\n",
      "regression loss:  12718.104\n",
      "regression loss:  29343.023\n",
      "episode: 458/1000000, returns: 1.7, epsilon: 0.8\n",
      "regression loss:  1822.4017\n",
      "regression loss:  25597.543\n",
      "episode: 459/1000000, returns: 2.5, epsilon: 0.79\n",
      "regression loss:  3686.4968\n",
      "regression loss:  18338.957\n",
      "episode: 460/1000000, returns: 1.8, epsilon: 0.79\n",
      "regression loss:  1230.6804\n",
      "regression loss:  12737.967\n",
      "episode: 461/1000000, returns: 1.1, epsilon: 0.79\n",
      "regression loss:  1939.2078\n",
      "regression loss:  10922.493\n",
      "episode: 462/1000000, returns: 0.19, epsilon: 0.79\n",
      "regression loss:  2739.3853\n",
      "regression loss:  10393.717\n",
      "episode: 463/1000000, returns: -3.3, epsilon: 0.79\n",
      "regression loss:  1013.72205\n",
      "regression loss:  6600.2866\n",
      "episode: 464/1000000, returns: 2.7, epsilon: 0.79\n",
      "regression loss:  4191.18\n",
      "regression loss:  8586.248\n",
      "episode: 465/1000000, returns: 4.5, epsilon: 0.79\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "regression loss:  1221.749\n",
      "regression loss:  11828.99\n",
      "episode: 466/1000000, returns: 1.9, epsilon: 0.79\n",
      "regression loss:  14241.05\n",
      "regression loss:  20071.734\n",
      "episode: 467/1000000, returns: 4.1, epsilon: 0.79\n",
      "regression loss:  11399.478\n",
      "regression loss:  6373.743\n",
      "episode: 468/1000000, returns: 1.2e+01, epsilon: 0.79\n",
      "regression loss:  2878.1301\n",
      "regression loss:  15476.04\n",
      "episode: 469/1000000, returns: 2.6, epsilon: 0.79\n",
      "regression loss:  5258.4683\n",
      "regression loss:  19255.535\n",
      "episode: 470/1000000, returns: 3.3, epsilon: 0.79\n",
      "regression loss:  1363.1313\n",
      "regression loss:  16843.516\n",
      "episode: 471/1000000, returns: -6.4, epsilon: 0.79\n",
      "regression loss:  8438.393\n",
      "regression loss:  10389.615\n",
      "episode: 472/1000000, returns: -2.5, epsilon: 0.79\n",
      "regression loss:  7663.722\n",
      "regression loss:  12239.053\n",
      "episode: 473/1000000, returns: 1.7, epsilon: 0.79\n",
      "regression loss:  1625.4565\n",
      "regression loss:  14781.643\n",
      "episode: 474/1000000, returns: -1.7, epsilon: 0.79\n",
      "regression loss:  1717.5476\n",
      "regression loss:  10766.881\n",
      "episode: 475/1000000, returns: 5.6, epsilon: 0.79\n",
      "regression loss:  6613.471\n",
      "regression loss:  16466.035\n",
      "episode: 476/1000000, returns: 1.4, epsilon: 0.79\n",
      "regression loss:  8691.75\n",
      "regression loss:  14920.25\n",
      "episode: 477/1000000, returns: -0.14, epsilon: 0.79\n",
      "regression loss:  6765.624\n",
      "regression loss:  10592.475\n",
      "episode: 478/1000000, returns: -4.4, epsilon: 0.79\n",
      "regression loss:  3991.081\n",
      "regression loss:  10868.584\n",
      "episode: 479/1000000, returns: 0.99, epsilon: 0.79\n",
      "regression loss:  6824.6353\n",
      "regression loss:  28322.168\n",
      "episode: 480/1000000, returns: -0.19, epsilon: 0.79\n",
      "regression loss:  10269.3\n",
      "regression loss:  6358.6772\n",
      "episode: 481/1000000, returns: 0.38, epsilon: 0.79\n",
      "regression loss:  1479.7112\n",
      "regression loss:  10017.826\n",
      "episode: 482/1000000, returns: 0.87, epsilon: 0.79\n",
      "regression loss:  9722.193\n",
      "regression loss:  11332.189\n",
      "episode: 483/1000000, returns: 1.5, epsilon: 0.79\n",
      "regression loss:  833.30786\n",
      "regression loss:  16413.953\n",
      "episode: 484/1000000, returns: 0.5, epsilon: 0.79\n",
      "regression loss:  11730.427\n",
      "regression loss:  16300.254\n",
      "episode: 485/1000000, returns: 7.3, epsilon: 0.78\n",
      "regression loss:  5822.1465\n",
      "regression loss:  5584.3916\n",
      "episode: 486/1000000, returns: -0.79, epsilon: 0.78\n",
      "regression loss:  12439.88\n",
      "regression loss:  21835.432\n",
      "episode: 487/1000000, returns: -4.8, epsilon: 0.78\n",
      "regression loss:  10807.101\n",
      "regression loss:  15454.953\n",
      "episode: 488/1000000, returns: 9.9, epsilon: 0.78\n",
      "regression loss:  1172.5764\n",
      "regression loss:  13364.971\n",
      "episode: 489/1000000, returns: -2.9, epsilon: 0.78\n",
      "regression loss:  9863.145\n",
      "regression loss:  17937.783\n",
      "episode: 490/1000000, returns: 2.6, epsilon: 0.78\n",
      "regression loss:  4235.882\n",
      "regression loss:  8665.9\n",
      "episode: 491/1000000, returns: 3.4, epsilon: 0.78\n",
      "regression loss:  4099.098\n",
      "regression loss:  21039.424\n",
      "episode: 492/1000000, returns: 1.2, epsilon: 0.78\n",
      "regression loss:  10809.716\n",
      "regression loss:  19592.266\n",
      "episode: 493/1000000, returns: 1.4, epsilon: 0.78\n",
      "regression loss:  4514.3926\n",
      "regression loss:  12695.688\n",
      "episode: 494/1000000, returns: 1.9, epsilon: 0.78\n",
      "regression loss:  4744.9893\n",
      "regression loss:  16368.094\n",
      "episode: 495/1000000, returns: -2.0, epsilon: 0.78\n",
      "regression loss:  57914.168\n",
      "regression loss:  38372.043\n",
      "episode: 496/1000000, returns: 1.8, epsilon: 0.78\n",
      "regression loss:  39229.227\n",
      "regression loss:  22147.562\n",
      "episode: 497/1000000, returns: 6.8, epsilon: 0.78\n",
      "regression loss:  7514.3057\n",
      "regression loss:  31342.623\n",
      "episode: 498/1000000, returns: 1.1, epsilon: 0.78\n",
      "regression loss:  14207.232\n",
      "regression loss:  24854.49\n",
      "episode: 499/1000000, returns: 0.45, epsilon: 0.78\n",
      "regression loss:  10187.321\n",
      "regression loss:  27104.535\n",
      "episode: 500/1000000, returns: 1.2, epsilon: 0.78\n",
      "regression loss:  13676.386\n",
      "regression loss:  20878.55\n",
      "episode: 501/1000000, returns: 6.8, epsilon: 0.78\n",
      "regression loss:  30355.865\n",
      "regression loss:  30687.742\n",
      "episode: 502/1000000, returns: 0.37, epsilon: 0.78\n",
      "regression loss:  4101.7886\n",
      "regression loss:  33682.023\n",
      "episode: 503/1000000, returns: 3.1, epsilon: 0.78\n",
      "regression loss:  19023.78\n",
      "regression loss:  86645.484\n",
      "episode: 504/1000000, returns: -5.8, epsilon: 0.78\n",
      "regression loss:  34574.61\n",
      "regression loss:  59665.066\n",
      "episode: 505/1000000, returns: 0.31, epsilon: 0.78\n",
      "regression loss:  7760.2207\n",
      "regression loss:  53792.68\n",
      "episode: 506/1000000, returns: 2.4, epsilon: 0.78\n",
      "regression loss:  20774.83\n",
      "regression loss:  46123.86\n",
      "episode: 507/1000000, returns: -0.95, epsilon: 0.78\n",
      "regression loss:  47868.082\n",
      "regression loss:  45904.82\n",
      "episode: 508/1000000, returns: -0.054, epsilon: 0.78\n",
      "regression loss:  9186.902\n",
      "regression loss:  54956.074\n",
      "episode: 509/1000000, returns: -0.42, epsilon: 0.78\n",
      "regression loss:  8662.35\n",
      "regression loss:  52687.004\n",
      "episode: 510/1000000, returns: 4.4, epsilon: 0.77\n",
      "regression loss:  25550.693\n",
      "regression loss:  66501.47\n",
      "episode: 511/1000000, returns: 0.3, epsilon: 0.77\n",
      "regression loss:  9230.25\n",
      "regression loss:  50620.312\n",
      "episode: 512/1000000, returns: 3.6, epsilon: 0.77\n",
      "regression loss:  22413.883\n",
      "regression loss:  45554.477\n",
      "episode: 513/1000000, returns: -4.7, epsilon: 0.77\n",
      "regression loss:  48842.902\n",
      "regression loss:  43036.54\n",
      "episode: 514/1000000, returns: -0.39, epsilon: 0.77\n",
      "regression loss:  6660.998\n",
      "regression loss:  41089.836\n",
      "episode: 515/1000000, returns: 4.5, epsilon: 0.77\n",
      "regression loss:  8734.956\n",
      "regression loss:  36892.715\n",
      "episode: 516/1000000, returns: -0.55, epsilon: 0.77\n",
      "regression loss:  17414.68\n",
      "regression loss:  38003.875\n",
      "episode: 517/1000000, returns: -2.0, epsilon: 0.77\n",
      "regression loss:  4228.1787\n",
      "regression loss:  25887.518\n",
      "episode: 518/1000000, returns: -0.014, epsilon: 0.77\n",
      "regression loss:  6793.8193\n",
      "regression loss:  29747.867\n",
      "episode: 519/1000000, returns: -1.0, epsilon: 0.77\n",
      "regression loss:  3976.5737\n",
      "regression loss:  26744.428\n",
      "episode: 520/1000000, returns: 6.6, epsilon: 0.77\n",
      "regression loss:  3721.2046\n",
      "regression loss:  20327.398\n",
      "episode: 521/1000000, returns: 1.6, epsilon: 0.77\n",
      "regression loss:  4171.3613\n",
      "regression loss:  17728.604\n",
      "episode: 522/1000000, returns: -0.47, epsilon: 0.77\n",
      "regression loss:  2185.2231\n",
      "regression loss:  11020.41\n",
      "episode: 523/1000000, returns: 3.9, epsilon: 0.77\n",
      "regression loss:  2218.0\n",
      "regression loss:  19710.988\n",
      "episode: 524/1000000, returns: -4.4, epsilon: 0.77\n",
      "regression loss:  1344.4596\n",
      "regression loss:  10421.74\n",
      "episode: 525/1000000, returns: -1.8, epsilon: 0.77\n",
      "regression loss:  1646.3386\n",
      "regression loss:  8165.746\n",
      "episode: 526/1000000, returns: 7.5, epsilon: 0.77\n",
      "regression loss:  11646.54\n",
      "regression loss:  5777.105\n",
      "episode: 527/1000000, returns: 1.1, epsilon: 0.77\n",
      "regression loss:  1616.34\n",
      "regression loss:  11884.895\n",
      "episode: 528/1000000, returns: 3.9, epsilon: 0.77\n",
      "regression loss:  5852.442\n",
      "regression loss:  9396.488\n",
      "episode: 529/1000000, returns: 3.1, epsilon: 0.77\n",
      "regression loss:  1715.0381\n",
      "regression loss:  6207.176\n",
      "episode: 530/1000000, returns: 0.00029, epsilon: 0.77\n",
      "regression loss:  1271.2668\n",
      "regression loss:  7266.8413\n",
      "episode: 531/1000000, returns: -0.89, epsilon: 0.77\n",
      "regression loss:  4480.918\n",
      "regression loss:  8893.6875\n",
      "episode: 532/1000000, returns: 6.8, epsilon: 0.77\n",
      "regression loss:  1187.943\n",
      "regression loss:  5866.635\n",
      "episode: 533/1000000, returns: 0.51, epsilon: 0.77\n",
      "regression loss:  2527.005\n",
      "regression loss:  4541.485\n",
      "episode: 534/1000000, returns: 4.9, epsilon: 0.77\n",
      "regression loss:  625.93427\n",
      "regression loss:  5422.2793\n",
      "episode: 535/1000000, returns: 1.6, epsilon: 0.77\n",
      "regression loss:  1375.935\n",
      "regression loss:  5573.0693\n",
      "episode: 536/1000000, returns: -2.4, epsilon: 0.76\n",
      "regression loss:  1427.1846\n",
      "regression loss:  4154.187\n",
      "episode: 537/1000000, returns: 8.0, epsilon: 0.76\n",
      "regression loss:  2073.7637\n",
      "regression loss:  3501.5134\n",
      "episode: 538/1000000, returns: -1e+01, epsilon: 0.76\n",
      "regression loss:  2326.0686\n",
      "regression loss:  3783.4878\n",
      "episode: 539/1000000, returns: 1e+01, epsilon: 0.76\n",
      "regression loss:  2583.8682\n",
      "regression loss:  4858.7705\n",
      "episode: 540/1000000, returns: 0.31, epsilon: 0.76\n",
      "regression loss:  275.5788\n",
      "regression loss:  7026.422\n",
      "episode: 541/1000000, returns: 0.44, epsilon: 0.76\n",
      "regression loss:  2804.3364\n",
      "regression loss:  5746.8105\n",
      "episode: 542/1000000, returns: 1.6, epsilon: 0.76\n",
      "regression loss:  702.6358\n",
      "regression loss:  6683.932\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 543/1000000, returns: 6.3, epsilon: 0.76\n",
      "regression loss:  9107.427\n",
      "regression loss:  7790.7773\n",
      "episode: 544/1000000, returns: -2.1, epsilon: 0.76\n",
      "regression loss:  378.35745\n",
      "regression loss:  8597.726\n",
      "episode: 545/1000000, returns: 0.48, epsilon: 0.76\n",
      "regression loss:  15843.712\n",
      "regression loss:  10013.0625\n",
      "episode: 546/1000000, returns: 3.2, epsilon: 0.76\n",
      "regression loss:  2480.8342\n",
      "regression loss:  5406.6157\n",
      "episode: 547/1000000, returns: -4.1, epsilon: 0.76\n",
      "regression loss:  1808.9104\n",
      "regression loss:  6352.8496\n",
      "episode: 548/1000000, returns: -2.2, epsilon: 0.76\n",
      "regression loss:  919.0243\n",
      "regression loss:  4977.8286\n",
      "episode: 549/1000000, returns: 1.6, epsilon: 0.76\n",
      "regression loss:  499.50372\n",
      "regression loss:  3989.7087\n",
      "episode: 550/1000000, returns: 1.1e+01, epsilon: 0.76\n",
      "regression loss:  370.92456\n",
      "regression loss:  4481.795\n",
      "episode: 551/1000000, returns: 4.8, epsilon: 0.76\n",
      "regression loss:  233.35341\n",
      "regression loss:  2578.4153\n",
      "episode: 552/1000000, returns: 1.2e+01, epsilon: 0.76\n",
      "regression loss:  651.49066\n",
      "regression loss:  1655.0278\n",
      "episode: 553/1000000, returns: 4.3, epsilon: 0.76\n",
      "regression loss:  333.60394\n",
      "regression loss:  1094.322\n",
      "episode: 554/1000000, returns: -9.7, epsilon: 0.76\n",
      "regression loss:  504.95322\n",
      "regression loss:  664.73737\n",
      "episode: 555/1000000, returns: -5.2, epsilon: 0.76\n",
      "regression loss:  224.67822\n",
      "regression loss:  1112.657\n",
      "episode: 556/1000000, returns: -3.3, epsilon: 0.76\n",
      "regression loss:  415.9366\n",
      "regression loss:  3463.8013\n",
      "episode: 557/1000000, returns: 1.2, epsilon: 0.76\n",
      "regression loss:  5924.0703\n",
      "regression loss:  1873.3114\n",
      "episode: 558/1000000, returns: -2.4, epsilon: 0.76\n",
      "regression loss:  358.17514\n",
      "regression loss:  2798.1418\n",
      "episode: 559/1000000, returns: 1.4, epsilon: 0.76\n",
      "regression loss:  2129.1536\n",
      "regression loss:  1927.481\n",
      "episode: 560/1000000, returns: 2.9, epsilon: 0.76\n",
      "regression loss:  1449.6621\n",
      "regression loss:  5769.382\n",
      "episode: 561/1000000, returns: 0.94, epsilon: 0.76\n",
      "regression loss:  4911.8726\n",
      "regression loss:  4550.308\n",
      "episode: 562/1000000, returns: 1.5, epsilon: 0.75\n",
      "regression loss:  2668.4573\n",
      "regression loss:  9377.522\n",
      "episode: 563/1000000, returns: 2.0, epsilon: 0.75\n",
      "regression loss:  2502.8037\n",
      "regression loss:  5178.4307\n",
      "episode: 564/1000000, returns: 2.0, epsilon: 0.75\n",
      "regression loss:  1035.4708\n",
      "regression loss:  5568.549\n",
      "episode: 565/1000000, returns: -4.0, epsilon: 0.75\n",
      "regression loss:  2100.576\n",
      "regression loss:  5155.8604\n",
      "episode: 566/1000000, returns: -8.2, epsilon: 0.75\n",
      "regression loss:  648.15076\n",
      "regression loss:  6416.308\n",
      "episode: 567/1000000, returns: 4.9, epsilon: 0.75\n",
      "regression loss:  3457.2092\n",
      "regression loss:  2567.141\n",
      "episode: 568/1000000, returns: -0.26, epsilon: 0.75\n",
      "regression loss:  471.5515\n",
      "regression loss:  5531.3877\n",
      "episode: 569/1000000, returns: 0.5, epsilon: 0.75\n",
      "regression loss:  3507.1575\n",
      "regression loss:  5570.105\n",
      "episode: 570/1000000, returns: 0.015, epsilon: 0.75\n",
      "regression loss:  497.1092\n",
      "regression loss:  4754.6113\n",
      "episode: 571/1000000, returns: 0.36, epsilon: 0.75\n",
      "regression loss:  1479.3782\n",
      "regression loss:  4067.8162\n",
      "episode: 572/1000000, returns: -3.2, epsilon: 0.75\n",
      "regression loss:  197.03123\n",
      "regression loss:  3647.9338\n",
      "episode: 573/1000000, returns: -3.3, epsilon: 0.75\n",
      "regression loss:  921.02014\n",
      "regression loss:  3499.3232\n",
      "episode: 574/1000000, returns: 2.4, epsilon: 0.75\n",
      "regression loss:  335.89798\n",
      "regression loss:  2624.157\n",
      "episode: 575/1000000, returns: -5.8, epsilon: 0.75\n",
      "regression loss:  1491.8943\n",
      "regression loss:  3652.4521\n",
      "episode: 576/1000000, returns: 7.7, epsilon: 0.75\n",
      "regression loss:  836.0022\n",
      "regression loss:  3393.605\n",
      "episode: 577/1000000, returns: 1.7, epsilon: 0.75\n",
      "regression loss:  1479.8069\n",
      "regression loss:  1814.6775\n",
      "episode: 578/1000000, returns: -0.82, epsilon: 0.75\n",
      "regression loss:  230.59029\n",
      "regression loss:  1998.4424\n",
      "episode: 579/1000000, returns: -6.5, epsilon: 0.75\n",
      "regression loss:  115.85483\n",
      "regression loss:  2234.571\n",
      "episode: 580/1000000, returns: 0.059, epsilon: 0.75\n",
      "regression loss:  1358.6317\n",
      "regression loss:  824.06116\n",
      "episode: 581/1000000, returns: -7.7, epsilon: 0.75\n",
      "regression loss:  3838.2922\n",
      "regression loss:  3236.0532\n",
      "episode: 582/1000000, returns: 0.71, epsilon: 0.75\n",
      "regression loss:  1390.7435\n",
      "regression loss:  2860.2207\n",
      "episode: 583/1000000, returns: -6.5, epsilon: 0.75\n",
      "regression loss:  2045.606\n",
      "regression loss:  3824.0525\n",
      "episode: 584/1000000, returns: 1.7, epsilon: 0.75\n",
      "regression loss:  762.7954\n",
      "regression loss:  2951.2302\n",
      "episode: 585/1000000, returns: 0.75, epsilon: 0.75\n",
      "regression loss:  1613.9917\n",
      "regression loss:  4370.8\n",
      "episode: 586/1000000, returns: -1.3, epsilon: 0.75\n",
      "regression loss:  428.569\n",
      "regression loss:  2766.4238\n",
      "episode: 587/1000000, returns: -0.61, epsilon: 0.75\n",
      "regression loss:  3365.3296\n",
      "regression loss:  2568.5566\n",
      "episode: 588/1000000, returns: -6.2, epsilon: 0.75\n",
      "regression loss:  4629.766\n",
      "regression loss:  8935.21\n",
      "episode: 589/1000000, returns: 1.2, epsilon: 0.74\n",
      "regression loss:  4888.301\n",
      "regression loss:  5508.2637\n",
      "episode: 590/1000000, returns: -1.0, epsilon: 0.74\n",
      "regression loss:  6361.4736\n",
      "regression loss:  18604.082\n",
      "episode: 591/1000000, returns: -0.93, epsilon: 0.74\n",
      "regression loss:  5802.093\n",
      "regression loss:  9375.685\n",
      "episode: 592/1000000, returns: 0.46, epsilon: 0.74\n",
      "regression loss:  1631.7158\n",
      "regression loss:  12723.928\n",
      "episode: 593/1000000, returns: -1.6, epsilon: 0.74\n",
      "regression loss:  10219.089\n",
      "regression loss:  10899.232\n",
      "episode: 594/1000000, returns: 1.4, epsilon: 0.74\n",
      "regression loss:  986.8148\n",
      "regression loss:  13994.402\n",
      "episode: 595/1000000, returns: -2.0, epsilon: 0.74\n",
      "regression loss:  3660.19\n",
      "regression loss:  11875.164\n",
      "episode: 596/1000000, returns: 5.1, epsilon: 0.74\n",
      "regression loss:  2657.7063\n",
      "regression loss:  12211.655\n",
      "episode: 597/1000000, returns: 0.34, epsilon: 0.74\n",
      "regression loss:  1185.9698\n",
      "regression loss:  10635.733\n",
      "episode: 598/1000000, returns: 0.57, epsilon: 0.74\n",
      "regression loss:  6108.598\n",
      "regression loss:  12706.729\n",
      "episode: 599/1000000, returns: 0.02, epsilon: 0.74\n",
      "regression loss:  12229.823\n",
      "regression loss:  7529.304\n",
      "episode: 600/1000000, returns: 2.4, epsilon: 0.74\n",
      "regression loss:  1493.7146\n",
      "regression loss:  9653.848\n",
      "episode: 601/1000000, returns: 2.5, epsilon: 0.74\n",
      "regression loss:  1127.363\n",
      "regression loss:  6903.92\n",
      "episode: 602/1000000, returns: 0.18, epsilon: 0.74\n",
      "regression loss:  2984.7944\n",
      "regression loss:  7036.2026\n",
      "episode: 603/1000000, returns: 0.18, epsilon: 0.74\n",
      "regression loss:  5120.337\n",
      "regression loss:  6742.2603\n",
      "episode: 604/1000000, returns: 2.5, epsilon: 0.74\n",
      "regression loss:  1996.47\n",
      "regression loss:  5999.4067\n",
      "episode: 605/1000000, returns: -5.1, epsilon: 0.74\n",
      "regression loss:  2568.5476\n",
      "regression loss:  8413.13\n",
      "episode: 606/1000000, returns: 1.2, epsilon: 0.74\n",
      "regression loss:  1326.446\n",
      "regression loss:  6374.5723\n",
      "episode: 607/1000000, returns: 1.3e+01, epsilon: 0.74\n",
      "regression loss:  5469.6353\n",
      "regression loss:  5207.651\n",
      "episode: 608/1000000, returns: -0.82, epsilon: 0.74\n",
      "regression loss:  4322.9116\n",
      "regression loss:  12115.375\n",
      "episode: 609/1000000, returns: -1.3, epsilon: 0.74\n",
      "regression loss:  2986.6833\n",
      "regression loss:  6861.199\n",
      "episode: 610/1000000, returns: 0.15, epsilon: 0.74\n",
      "regression loss:  3649.9258\n",
      "regression loss:  6211.3633\n",
      "episode: 611/1000000, returns: 9.1, epsilon: 0.74\n",
      "regression loss:  2245.1553\n",
      "regression loss:  7810.723\n",
      "episode: 612/1000000, returns: 3.5, epsilon: 0.74\n",
      "regression loss:  1732.6537\n",
      "regression loss:  7459.7637\n",
      "episode: 613/1000000, returns: -2.2, epsilon: 0.74\n",
      "regression loss:  565.1736\n",
      "regression loss:  3673.141\n",
      "episode: 614/1000000, returns: -1.1, epsilon: 0.74\n",
      "regression loss:  6468.9106\n",
      "regression loss:  6021.0454\n",
      "episode: 615/1000000, returns: 5.6, epsilon: 0.74\n",
      "regression loss:  2059.0854\n",
      "regression loss:  6255.99\n",
      "episode: 616/1000000, returns: 1.3, epsilon: 0.73\n",
      "regression loss:  6400.028\n",
      "regression loss:  10064.014\n",
      "episode: 617/1000000, returns: 2.5, epsilon: 0.73\n",
      "regression loss:  1606.9447\n",
      "regression loss:  3454.0537\n",
      "episode: 618/1000000, returns: -6.5, epsilon: 0.73\n",
      "regression loss:  10302.971\n",
      "regression loss:  5281.04\n",
      "episode: 619/1000000, returns: 0.12, epsilon: 0.73\n",
      "regression loss:  3746.6455\n",
      "regression loss:  8038.7676\n",
      "episode: 620/1000000, returns: 0.28, epsilon: 0.73\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "regression loss:  5078.916\n",
      "regression loss:  11273.604\n",
      "episode: 621/1000000, returns: 8.4, epsilon: 0.73\n",
      "regression loss:  1738.7366\n",
      "regression loss:  4701.5703\n",
      "episode: 622/1000000, returns: -1.2e+01, epsilon: 0.73\n",
      "regression loss:  6481.0522\n",
      "regression loss:  5971.8364\n",
      "episode: 623/1000000, returns: 1.1, epsilon: 0.73\n",
      "regression loss:  1078.3536\n",
      "regression loss:  12426.836\n",
      "episode: 624/1000000, returns: -2.5, epsilon: 0.73\n",
      "regression loss:  17561.832\n",
      "regression loss:  20696.86\n",
      "episode: 625/1000000, returns: -6.7, epsilon: 0.73\n",
      "regression loss:  3808.9375\n",
      "regression loss:  25858.316\n",
      "episode: 626/1000000, returns: -2.5, epsilon: 0.73\n",
      "regression loss:  10706.352\n",
      "regression loss:  27131.496\n",
      "episode: 627/1000000, returns: 2.1, epsilon: 0.73\n",
      "regression loss:  13677.812\n",
      "regression loss:  13139.523\n",
      "episode: 628/1000000, returns: -3.3, epsilon: 0.73\n",
      "regression loss:  646.5756\n",
      "regression loss:  8867.967\n",
      "episode: 629/1000000, returns: -8.6, epsilon: 0.73\n",
      "regression loss:  5653.8394\n",
      "regression loss:  11075.551\n",
      "episode: 630/1000000, returns: 0.82, epsilon: 0.73\n",
      "regression loss:  1064.8593\n",
      "regression loss:  8740.278\n",
      "episode: 631/1000000, returns: 0.18, epsilon: 0.73\n",
      "regression loss:  3686.1704\n",
      "regression loss:  10910.377\n",
      "episode: 632/1000000, returns: -2.0, epsilon: 0.73\n",
      "regression loss:  1253.3618\n",
      "regression loss:  11610.898\n",
      "episode: 633/1000000, returns: 1.3, epsilon: 0.73\n",
      "regression loss:  5650.0747\n",
      "regression loss:  7385.241\n",
      "episode: 634/1000000, returns: -2.2, epsilon: 0.73\n",
      "regression loss:  550.4131\n",
      "regression loss:  6260.653\n",
      "episode: 635/1000000, returns: -8.0, epsilon: 0.73\n",
      "regression loss:  2161.4824\n",
      "regression loss:  6387.591\n",
      "episode: 636/1000000, returns: 4.4, epsilon: 0.73\n",
      "regression loss:  951.6157\n",
      "regression loss:  6352.403\n",
      "episode: 637/1000000, returns: 0.83, epsilon: 0.73\n",
      "regression loss:  1658.8036\n",
      "regression loss:  3796.3586\n",
      "episode: 638/1000000, returns: 2.1e+01, epsilon: 0.73\n",
      "regression loss:  3996.7944\n",
      "regression loss:  4056.7542\n",
      "episode: 639/1000000, returns: 3.8, epsilon: 0.73\n",
      "regression loss:  648.1183\n",
      "regression loss:  5764.118\n",
      "episode: 640/1000000, returns: 4.1, epsilon: 0.73\n",
      "regression loss:  2151.1284\n",
      "regression loss:  1925.0984\n",
      "episode: 641/1000000, returns: 1.6, epsilon: 0.73\n",
      "regression loss:  355.84216\n",
      "regression loss:  3177.1558\n",
      "episode: 642/1000000, returns: 8.2, epsilon: 0.73\n",
      "regression loss:  184.66168\n",
      "regression loss:  4169.376\n",
      "episode: 643/1000000, returns: 0.42, epsilon: 0.73\n",
      "regression loss:  1097.3448\n",
      "regression loss:  2890.566\n",
      "episode: 644/1000000, returns: 2.2, epsilon: 0.72\n",
      "regression loss:  5601.092\n",
      "regression loss:  10262.526\n",
      "episode: 645/1000000, returns: -5.2, epsilon: 0.72\n",
      "regression loss:  5015.31\n",
      "regression loss:  4326.557\n",
      "episode: 646/1000000, returns: -1.0, epsilon: 0.72\n",
      "regression loss:  2112.8606\n",
      "regression loss:  3036.5083\n",
      "episode: 647/1000000, returns: 4.0, epsilon: 0.72\n",
      "regression loss:  1044.0004\n",
      "regression loss:  3554.9448\n",
      "episode: 648/1000000, returns: -5.1, epsilon: 0.72\n",
      "regression loss:  4086.97\n",
      "regression loss:  5713.468\n",
      "episode: 649/1000000, returns: -0.76, epsilon: 0.72\n",
      "regression loss:  867.85095\n",
      "regression loss:  10236.071\n",
      "episode: 650/1000000, returns: -4.3, epsilon: 0.72\n",
      "regression loss:  12341.933\n",
      "regression loss:  12210.743\n",
      "episode: 651/1000000, returns: 0.45, epsilon: 0.72\n",
      "regression loss:  5697.2256\n",
      "regression loss:  15159.128\n",
      "episode: 652/1000000, returns: 1.3e+01, epsilon: 0.72\n",
      "regression loss:  3136.251\n",
      "regression loss:  7208.7583\n",
      "episode: 653/1000000, returns: -3.0, epsilon: 0.72\n",
      "regression loss:  6617.338\n",
      "regression loss:  4783.34\n",
      "episode: 654/1000000, returns: 8.2, epsilon: 0.72\n",
      "regression loss:  2564.442\n",
      "regression loss:  9814.783\n",
      "episode: 655/1000000, returns: 1.4, epsilon: 0.72\n",
      "regression loss:  4097.58\n",
      "regression loss:  5074.089\n",
      "episode: 656/1000000, returns: 2.2, epsilon: 0.72\n",
      "regression loss:  3052.7773\n",
      "regression loss:  12856.902\n",
      "episode: 657/1000000, returns: 0.25, epsilon: 0.72\n",
      "regression loss:  6804.7446\n",
      "regression loss:  7757.5264\n",
      "episode: 658/1000000, returns: 8.9, epsilon: 0.72\n",
      "regression loss:  896.1826\n",
      "regression loss:  7575.032\n",
      "episode: 659/1000000, returns: 9.8, epsilon: 0.72\n",
      "regression loss:  4043.9607\n",
      "regression loss:  9806.078\n",
      "episode: 660/1000000, returns: -9.1, epsilon: 0.72\n",
      "regression loss:  1193.5854\n",
      "regression loss:  8297.848\n",
      "episode: 661/1000000, returns: 0.63, epsilon: 0.72\n",
      "regression loss:  1538.069\n",
      "regression loss:  3858.1934\n",
      "episode: 662/1000000, returns: 2.1e+01, epsilon: 0.72\n",
      "regression loss:  4636.1885\n",
      "regression loss:  3019.1328\n",
      "episode: 663/1000000, returns: 0.062, epsilon: 0.72\n",
      "regression loss:  1403.982\n",
      "regression loss:  7238.67\n",
      "episode: 664/1000000, returns: 0.6, epsilon: 0.72\n",
      "regression loss:  913.9749\n",
      "regression loss:  4483.131\n",
      "episode: 665/1000000, returns: 1.7, epsilon: 0.72\n",
      "regression loss:  16955.494\n",
      "regression loss:  12087.84\n",
      "episode: 666/1000000, returns: 2.0, epsilon: 0.72\n",
      "regression loss:  8877.3545\n",
      "regression loss:  7065.0557\n",
      "episode: 667/1000000, returns: -1.4, epsilon: 0.72\n",
      "regression loss:  2219.5186\n",
      "regression loss:  8094.832\n",
      "episode: 668/1000000, returns: -2.0, epsilon: 0.72\n",
      "regression loss:  14752.823\n",
      "regression loss:  11298.359\n",
      "episode: 669/1000000, returns: 1.8, epsilon: 0.72\n",
      "regression loss:  7171.2905\n",
      "regression loss:  4018.9043\n",
      "episode: 670/1000000, returns: -5.8, epsilon: 0.72\n",
      "regression loss:  42818.555\n",
      "regression loss:  30887.918\n",
      "episode: 671/1000000, returns: -0.34, epsilon: 0.71\n",
      "regression loss:  45078.332\n",
      "regression loss:  39278.598\n",
      "episode: 672/1000000, returns: -1.0, epsilon: 0.71\n",
      "regression loss:  21827.367\n",
      "regression loss:  14755.516\n",
      "episode: 673/1000000, returns: -0.92, epsilon: 0.71\n",
      "regression loss:  2885.1016\n",
      "regression loss:  27030.113\n",
      "episode: 674/1000000, returns: 2.5, epsilon: 0.71\n",
      "regression loss:  13691.477\n",
      "regression loss:  16947.479\n",
      "episode: 675/1000000, returns: 3.2, epsilon: 0.71\n",
      "regression loss:  6375.6426\n",
      "regression loss:  13551.385\n",
      "episode: 676/1000000, returns: 1.1e+01, epsilon: 0.71\n",
      "regression loss:  13226.38\n",
      "regression loss:  25858.305\n",
      "episode: 677/1000000, returns: 0.39, epsilon: 0.71\n",
      "regression loss:  9760.707\n",
      "regression loss:  32584.498\n",
      "episode: 678/1000000, returns: 2.7, epsilon: 0.71\n",
      "regression loss:  27673.207\n",
      "regression loss:  31268.23\n",
      "episode: 679/1000000, returns: 2.6, epsilon: 0.71\n",
      "regression loss:  10225.763\n",
      "regression loss:  24456.465\n",
      "episode: 680/1000000, returns: 1.7, epsilon: 0.71\n",
      "regression loss:  25820.412\n",
      "regression loss:  41988.984\n",
      "episode: 681/1000000, returns: 0.00016, epsilon: 0.71\n",
      "regression loss:  21494.154\n",
      "regression loss:  17568.773\n",
      "episode: 682/1000000, returns: 1.2, epsilon: 0.71\n",
      "regression loss:  12941.934\n",
      "regression loss:  55757.938\n",
      "episode: 683/1000000, returns: -0.79, epsilon: 0.71\n",
      "regression loss:  12470.139\n",
      "regression loss:  37272.383\n",
      "episode: 684/1000000, returns: -4.1, epsilon: 0.71\n",
      "regression loss:  4462.1943\n",
      "regression loss:  31451.146\n",
      "episode: 685/1000000, returns: -6.8, epsilon: 0.71\n",
      "regression loss:  16493.424\n",
      "regression loss:  27527.25\n",
      "episode: 686/1000000, returns: 3.0, epsilon: 0.71\n",
      "regression loss:  17444.592\n",
      "regression loss:  15658.444\n",
      "episode: 687/1000000, returns: 1.8, epsilon: 0.71\n",
      "regression loss:  3208.8076\n",
      "regression loss:  33261.746\n",
      "episode: 688/1000000, returns: -9.9, epsilon: 0.71\n",
      "regression loss:  5347.244\n",
      "regression loss:  14229.109\n",
      "episode: 689/1000000, returns: -3.0, epsilon: 0.71\n",
      "regression loss:  3926.7163\n",
      "regression loss:  11657.201\n",
      "episode: 690/1000000, returns: -2.3, epsilon: 0.71\n",
      "regression loss:  1782.8838\n",
      "regression loss:  10265.49\n",
      "episode: 691/1000000, returns: -2.5, epsilon: 0.71\n",
      "regression loss:  2781.209\n",
      "regression loss:  11733.812\n",
      "episode: 692/1000000, returns: 1.0, epsilon: 0.71\n",
      "regression loss:  1663.4087\n",
      "regression loss:  6878.4585\n",
      "episode: 693/1000000, returns: 2.0, epsilon: 0.71\n",
      "regression loss:  3432.953\n",
      "regression loss:  8889.966\n",
      "episode: 694/1000000, returns: -0.14, epsilon: 0.71\n",
      "regression loss:  4266.0615\n",
      "regression loss:  4775.1323\n",
      "episode: 695/1000000, returns: 1.0, epsilon: 0.71\n",
      "regression loss:  2939.035\n",
      "regression loss:  5983.902\n",
      "episode: 696/1000000, returns: -7.8, epsilon: 0.71\n",
      "regression loss:  2345.4453\n",
      "regression loss:  8281.705\n",
      "episode: 697/1000000, returns: -0.91, epsilon: 0.71\n",
      "regression loss:  5841.915\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "regression loss:  12003.35\n",
      "episode: 698/1000000, returns: -0.26, epsilon: 0.71\n",
      "regression loss:  8457.349\n",
      "regression loss:  11221.044\n",
      "episode: 699/1000000, returns: -0.21, epsilon: 0.7\n",
      "regression loss:  3366.8984\n",
      "regression loss:  10745.399\n",
      "episode: 700/1000000, returns: -1.1, epsilon: 0.7\n",
      "regression loss:  7061.1113\n",
      "regression loss:  5730.63\n",
      "episode: 701/1000000, returns: 0.36, epsilon: 0.7\n",
      "regression loss:  1526.1957\n",
      "regression loss:  6522.205\n",
      "episode: 702/1000000, returns: -7.2, epsilon: 0.7\n",
      "regression loss:  3593.0378\n",
      "regression loss:  4900.957\n",
      "episode: 703/1000000, returns: -0.42, epsilon: 0.7\n",
      "regression loss:  2529.448\n",
      "regression loss:  6930.7666\n",
      "episode: 704/1000000, returns: -0.77, epsilon: 0.7\n",
      "regression loss:  5096.6997\n",
      "regression loss:  4791.5273\n",
      "episode: 705/1000000, returns: 7.7, epsilon: 0.7\n",
      "regression loss:  13481.658\n",
      "regression loss:  10581.102\n",
      "episode: 706/1000000, returns: -4.8, epsilon: 0.7\n",
      "regression loss:  13125.833\n",
      "regression loss:  6839.9155\n",
      "episode: 707/1000000, returns: 0.48, epsilon: 0.7\n",
      "regression loss:  7508.23\n",
      "regression loss:  25163.95\n",
      "episode: 708/1000000, returns: 1.6, epsilon: 0.7\n",
      "regression loss:  26025.404\n",
      "regression loss:  24923.613\n",
      "episode: 709/1000000, returns: -0.2, epsilon: 0.7\n",
      "regression loss:  4967.516\n",
      "regression loss:  16932.889\n",
      "episode: 710/1000000, returns: 4.3, epsilon: 0.7\n",
      "regression loss:  6129.0957\n",
      "regression loss:  22018.668\n",
      "episode: 711/1000000, returns: -1.1, epsilon: 0.7\n",
      "regression loss:  7017.8823\n",
      "regression loss:  17544.723\n",
      "episode: 712/1000000, returns: -0.43, epsilon: 0.7\n",
      "regression loss:  3376.6226\n",
      "regression loss:  14899.149\n",
      "episode: 713/1000000, returns: -1.9, epsilon: 0.7\n",
      "regression loss:  4848.4126\n",
      "regression loss:  19259.61\n",
      "episode: 714/1000000, returns: -0.016, epsilon: 0.7\n",
      "regression loss:  3702.002\n",
      "regression loss:  11796.201\n",
      "episode: 715/1000000, returns: -2.2, epsilon: 0.7\n",
      "regression loss:  11421.329\n",
      "regression loss:  21957.492\n",
      "episode: 716/1000000, returns: 2.0, epsilon: 0.7\n",
      "regression loss:  4708.954\n",
      "regression loss:  19181.936\n",
      "episode: 717/1000000, returns: -1.4, epsilon: 0.7\n",
      "regression loss:  7767.833\n",
      "regression loss:  20772.293\n",
      "episode: 718/1000000, returns: 2.8, epsilon: 0.7\n",
      "regression loss:  7878.5977\n",
      "regression loss:  13411.334\n",
      "episode: 719/1000000, returns: 6.6, epsilon: 0.7\n",
      "regression loss:  17929.938\n",
      "regression loss:  29363.895\n",
      "episode: 720/1000000, returns: -0.79, epsilon: 0.7\n",
      "regression loss:  26362.793\n",
      "regression loss:  26746.832\n",
      "episode: 721/1000000, returns: 3.1, epsilon: 0.7\n",
      "regression loss:  3011.2146\n",
      "regression loss:  18394.057\n",
      "episode: 722/1000000, returns: 0.77, epsilon: 0.7\n",
      "regression loss:  8378.581\n",
      "regression loss:  27635.842\n",
      "episode: 723/1000000, returns: -0.11, epsilon: 0.7\n",
      "regression loss:  11540.16\n",
      "regression loss:  13972.252\n",
      "episode: 724/1000000, returns: -2.0, epsilon: 0.7\n",
      "regression loss:  6628.513\n",
      "regression loss:  32059.867\n",
      "episode: 725/1000000, returns: -4.5, epsilon: 0.7\n",
      "regression loss:  11940.053\n",
      "regression loss:  25993.559\n",
      "episode: 726/1000000, returns: 5.2, epsilon: 0.7\n",
      "regression loss:  1558.9781\n",
      "regression loss:  20569.72\n",
      "episode: 727/1000000, returns: -1.3, epsilon: 0.7\n",
      "regression loss:  12002.228\n",
      "regression loss:  16309.613\n",
      "episode: 728/1000000, returns: -5.4, epsilon: 0.69\n",
      "regression loss:  6462.0967\n",
      "regression loss:  15406.821\n",
      "episode: 729/1000000, returns: 1.3, epsilon: 0.69\n",
      "regression loss:  2834.2815\n",
      "regression loss:  17783.61\n",
      "episode: 730/1000000, returns: 1.7, epsilon: 0.69\n",
      "regression loss:  2949.224\n",
      "regression loss:  15739.929\n",
      "episode: 731/1000000, returns: -2.8, epsilon: 0.69\n",
      "regression loss:  1859.4803\n",
      "regression loss:  13115.805\n",
      "episode: 732/1000000, returns: -4.5, epsilon: 0.69\n",
      "regression loss:  2321.3804\n",
      "regression loss:  10325.758\n",
      "episode: 733/1000000, returns: -2.3, epsilon: 0.69\n",
      "regression loss:  1813.7152\n",
      "regression loss:  10685.303\n",
      "episode: 734/1000000, returns: 6.5, epsilon: 0.69\n",
      "regression loss:  1611.2191\n",
      "regression loss:  8815.704\n",
      "episode: 735/1000000, returns: 3.9, epsilon: 0.69\n",
      "regression loss:  3703.4204\n",
      "regression loss:  8707.33\n",
      "episode: 736/1000000, returns: -3.2, epsilon: 0.69\n",
      "regression loss:  761.7992\n",
      "regression loss:  6138.9766\n",
      "episode: 737/1000000, returns: -4.3, epsilon: 0.69\n",
      "regression loss:  711.6332\n",
      "regression loss:  4343.4546\n",
      "episode: 738/1000000, returns: 4.5, epsilon: 0.69\n",
      "regression loss:  609.571\n",
      "regression loss:  4163.407\n",
      "episode: 739/1000000, returns: 1.7e+01, epsilon: 0.69\n",
      "regression loss:  565.9902\n",
      "regression loss:  2044.825\n",
      "episode: 740/1000000, returns: 0.33, epsilon: 0.69\n",
      "regression loss:  231.72638\n",
      "regression loss:  2736.645\n",
      "episode: 741/1000000, returns: 2.5, epsilon: 0.69\n",
      "regression loss:  242.35626\n",
      "regression loss:  1738.2632\n",
      "episode: 742/1000000, returns: 3.7, epsilon: 0.69\n",
      "regression loss:  2169.9736\n",
      "regression loss:  2546.9739\n",
      "episode: 743/1000000, returns: -1.3e+01, epsilon: 0.69\n",
      "regression loss:  4806.795\n",
      "regression loss:  7821.4824\n",
      "episode: 744/1000000, returns: -6.7, epsilon: 0.69\n",
      "regression loss:  2641.5906\n",
      "regression loss:  4914.0537\n",
      "episode: 745/1000000, returns: -3.0, epsilon: 0.69\n",
      "regression loss:  2135.1401\n",
      "regression loss:  4239.4316\n",
      "episode: 746/1000000, returns: 9.3, epsilon: 0.69\n",
      "regression loss:  2967.1143\n",
      "regression loss:  3957.894\n",
      "episode: 747/1000000, returns: -9.0, epsilon: 0.69\n",
      "regression loss:  8478.31\n",
      "regression loss:  12106.327\n",
      "episode: 748/1000000, returns: 1.2, epsilon: 0.69\n",
      "regression loss:  5606.252\n",
      "regression loss:  6807.171\n",
      "episode: 749/1000000, returns: 3.4, epsilon: 0.69\n",
      "regression loss:  2463.2163\n",
      "regression loss:  7113.6226\n",
      "episode: 750/1000000, returns: -0.095, epsilon: 0.69\n",
      "regression loss:  2651.7007\n",
      "regression loss:  4833.921\n",
      "episode: 751/1000000, returns: -4.2, epsilon: 0.69\n",
      "regression loss:  2615.024\n",
      "regression loss:  12053.537\n",
      "episode: 752/1000000, returns: 3.0, epsilon: 0.69\n",
      "regression loss:  10345.579\n",
      "regression loss:  18322.29\n",
      "episode: 753/1000000, returns: 5.3, epsilon: 0.69\n",
      "regression loss:  13394.349\n",
      "regression loss:  7564.9106\n",
      "episode: 754/1000000, returns: 4.7, epsilon: 0.69\n",
      "regression loss:  3782.1475\n",
      "regression loss:  10958.639\n",
      "episode: 755/1000000, returns: 1.0, epsilon: 0.69\n",
      "regression loss:  1696.558\n",
      "regression loss:  12243.027\n",
      "episode: 756/1000000, returns: 1.5, epsilon: 0.69\n",
      "regression loss:  7309.641\n",
      "regression loss:  16705.348\n",
      "episode: 757/1000000, returns: 8.1, epsilon: 0.68\n",
      "regression loss:  3546.7625\n",
      "regression loss:  8789.627\n",
      "episode: 758/1000000, returns: 3.3, epsilon: 0.68\n",
      "regression loss:  9997.82\n",
      "regression loss:  13250.875\n",
      "episode: 759/1000000, returns: 0.53, epsilon: 0.68\n",
      "regression loss:  8810.207\n",
      "regression loss:  9065.52\n",
      "episode: 760/1000000, returns: 0.66, epsilon: 0.68\n",
      "regression loss:  1816.9382\n",
      "regression loss:  10205.76\n",
      "episode: 761/1000000, returns: 0.46, epsilon: 0.68\n",
      "regression loss:  4094.3687\n",
      "regression loss:  12072.056\n",
      "episode: 762/1000000, returns: 5.9, epsilon: 0.68\n",
      "regression loss:  1907.1788\n",
      "regression loss:  7634.8257\n",
      "episode: 763/1000000, returns: 1.2, epsilon: 0.68\n",
      "regression loss:  1271.5625\n",
      "regression loss:  7585.1\n",
      "episode: 764/1000000, returns: 1.2, epsilon: 0.68\n",
      "regression loss:  1145.4554\n",
      "regression loss:  5848.1973\n",
      "episode: 765/1000000, returns: -2.6, epsilon: 0.68\n",
      "regression loss:  849.4391\n",
      "regression loss:  3637.8508\n",
      "episode: 766/1000000, returns: 4.1, epsilon: 0.68\n",
      "regression loss:  2032.544\n",
      "regression loss:  4690.6396\n",
      "episode: 767/1000000, returns: 0.19, epsilon: 0.68\n",
      "regression loss:  327.29736\n",
      "regression loss:  3846.373\n",
      "episode: 768/1000000, returns: -5.5, epsilon: 0.68\n",
      "regression loss:  1924.5739\n",
      "regression loss:  1458.939\n",
      "episode: 769/1000000, returns: 4.7, epsilon: 0.68\n",
      "regression loss:  1441.9259\n",
      "regression loss:  2536.1943\n",
      "episode: 770/1000000, returns: -0.27, epsilon: 0.68\n",
      "regression loss:  474.0014\n",
      "regression loss:  3386.335\n",
      "episode: 771/1000000, returns: -5.2, epsilon: 0.68\n",
      "regression loss:  4402.439\n",
      "regression loss:  4104.242\n",
      "episode: 772/1000000, returns: 1.8, epsilon: 0.68\n",
      "regression loss:  109.74979\n",
      "regression loss:  3706.729\n",
      "episode: 773/1000000, returns: -7.3, epsilon: 0.68\n",
      "regression loss:  3762.9895\n",
      "regression loss:  1782.4104\n",
      "episode: 774/1000000, returns: -0.73, epsilon: 0.68\n",
      "regression loss:  1249.7457\n",
      "regression loss:  6900.868\n",
      "episode: 775/1000000, returns: -1.3, epsilon: 0.68\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "regression loss:  3671.778\n",
      "regression loss:  5571.463\n",
      "episode: 776/1000000, returns: 8.8, epsilon: 0.68\n",
      "regression loss:  788.5695\n",
      "regression loss:  3622.5493\n",
      "episode: 777/1000000, returns: -4.7, epsilon: 0.68\n",
      "regression loss:  1649.2354\n",
      "regression loss:  7090.5664\n",
      "episode: 778/1000000, returns: 7.3, epsilon: 0.68\n",
      "regression loss:  4896.627\n",
      "regression loss:  7319.445\n",
      "episode: 779/1000000, returns: -1.6, epsilon: 0.68\n",
      "regression loss:  556.16016\n",
      "regression loss:  5680.7812\n",
      "episode: 780/1000000, returns: -0.19, epsilon: 0.68\n",
      "regression loss:  4639.6177\n",
      "regression loss:  8561.984\n",
      "episode: 781/1000000, returns: 0.45, epsilon: 0.68\n",
      "regression loss:  1122.835\n",
      "regression loss:  6450.8906\n",
      "episode: 782/1000000, returns: -6.8, epsilon: 0.68\n",
      "regression loss:  2033.0215\n",
      "regression loss:  5441.833\n",
      "episode: 783/1000000, returns: -0.67, epsilon: 0.68\n",
      "regression loss:  5054.5693\n",
      "regression loss:  11706.646\n",
      "episode: 784/1000000, returns: 0.17, epsilon: 0.68\n",
      "regression loss:  2734.5159\n",
      "regression loss:  4388.473\n",
      "episode: 785/1000000, returns: -2.1, epsilon: 0.68\n",
      "regression loss:  4217.006\n",
      "regression loss:  6778.761\n",
      "episode: 786/1000000, returns: -1.2, epsilon: 0.67\n",
      "regression loss:  4801.118\n",
      "regression loss:  4577.394\n",
      "episode: 787/1000000, returns: -5.9, epsilon: 0.67\n",
      "regression loss:  3502.7043\n",
      "regression loss:  7663.3286\n",
      "episode: 788/1000000, returns: 2.4, epsilon: 0.67\n",
      "regression loss:  2468.816\n",
      "regression loss:  11021.033\n",
      "episode: 789/1000000, returns: -1.4, epsilon: 0.67\n",
      "regression loss:  9670.857\n",
      "regression loss:  13830.916\n",
      "episode: 790/1000000, returns: 8.4, epsilon: 0.67\n",
      "regression loss:  10357.275\n",
      "regression loss:  4525.951\n",
      "episode: 791/1000000, returns: -1.9, epsilon: 0.67\n",
      "regression loss:  973.6248\n",
      "regression loss:  17040.89\n",
      "episode: 792/1000000, returns: 2.6, epsilon: 0.67\n",
      "regression loss:  12809.105\n",
      "regression loss:  12795.877\n",
      "episode: 793/1000000, returns: -4.3, epsilon: 0.67\n",
      "regression loss:  2146.4026\n",
      "regression loss:  21679.525\n",
      "episode: 794/1000000, returns: 4.0, epsilon: 0.67\n",
      "regression loss:  11310.999\n",
      "regression loss:  25235.809\n",
      "episode: 795/1000000, returns: -0.58, epsilon: 0.67\n",
      "regression loss:  23409.002\n",
      "regression loss:  30392.191\n",
      "episode: 796/1000000, returns: 5.4, epsilon: 0.67\n",
      "regression loss:  2240.7249\n",
      "regression loss:  26820.52\n",
      "episode: 797/1000000, returns: -0.056, epsilon: 0.67\n",
      "regression loss:  19353.85\n",
      "regression loss:  32476.113\n",
      "episode: 798/1000000, returns: 7.0, epsilon: 0.67\n",
      "regression loss:  27659.61\n",
      "regression loss:  24885.523\n",
      "episode: 799/1000000, returns: 1.5, epsilon: 0.67\n",
      "regression loss:  3005.7517\n",
      "regression loss:  21039.34\n",
      "episode: 800/1000000, returns: -0.8, epsilon: 0.67\n",
      "regression loss:  37085.42\n",
      "regression loss:  22734.082\n",
      "episode: 801/1000000, returns: -7.8, epsilon: 0.67\n",
      "regression loss:  48433.59\n",
      "regression loss:  28852.717\n",
      "episode: 802/1000000, returns: -9.3, epsilon: 0.67\n",
      "regression loss:  6525.078\n",
      "regression loss:  29153.059\n",
      "episode: 803/1000000, returns: -1.2, epsilon: 0.67\n",
      "regression loss:  13992.787\n",
      "regression loss:  40379.805\n",
      "episode: 804/1000000, returns: -0.9, epsilon: 0.67\n",
      "regression loss:  24272.854\n",
      "regression loss:  31511.465\n",
      "episode: 805/1000000, returns: -1.3, epsilon: 0.67\n",
      "regression loss:  4527.8564\n",
      "regression loss:  16486.342\n",
      "episode: 806/1000000, returns: 6.5, epsilon: 0.67\n",
      "regression loss:  39494.4\n",
      "regression loss:  71930.84\n",
      "episode: 807/1000000, returns: -3.1, epsilon: 0.67\n",
      "regression loss:  61734.234\n",
      "regression loss:  64601.29\n",
      "episode: 808/1000000, returns: 2.1, epsilon: 0.67\n",
      "regression loss:  22141.88\n",
      "regression loss:  31939.006\n",
      "episode: 809/1000000, returns: 0.28, epsilon: 0.67\n",
      "regression loss:  6613.4053\n",
      "regression loss:  63084.836\n",
      "episode: 810/1000000, returns: -1.4, epsilon: 0.67\n",
      "regression loss:  19190.566\n",
      "regression loss:  33503.695\n",
      "episode: 811/1000000, returns: -3.5, epsilon: 0.67\n",
      "regression loss:  13611.49\n",
      "regression loss:  15610.422\n",
      "episode: 812/1000000, returns: -8.6, epsilon: 0.67\n",
      "regression loss:  6733.88\n",
      "regression loss:  19490.781\n",
      "episode: 813/1000000, returns: -6.0, epsilon: 0.67\n",
      "regression loss:  14953.193\n",
      "regression loss:  44259.973\n",
      "episode: 814/1000000, returns: 7.7, epsilon: 0.67\n",
      "regression loss:  3633.4473\n",
      "regression loss:  34330.758\n",
      "episode: 815/1000000, returns: 2.4, epsilon: 0.67\n",
      "regression loss:  7293.3184\n",
      "regression loss:  32290.385\n",
      "episode: 816/1000000, returns: 0.28, epsilon: 0.66\n",
      "regression loss:  9110.701\n",
      "regression loss:  30025.621\n",
      "episode: 817/1000000, returns: 9.9, epsilon: 0.66\n",
      "regression loss:  6824.234\n",
      "regression loss:  20937.854\n",
      "episode: 818/1000000, returns: -8.0, epsilon: 0.66\n",
      "regression loss:  16219.798\n",
      "regression loss:  56150.61\n",
      "episode: 819/1000000, returns: -2.5, epsilon: 0.66\n",
      "regression loss:  21667.232\n",
      "regression loss:  38608.21\n",
      "episode: 820/1000000, returns: 2.1, epsilon: 0.66\n",
      "regression loss:  3820.3418\n",
      "regression loss:  29376.32\n",
      "episode: 821/1000000, returns: 3.8, epsilon: 0.66\n",
      "regression loss:  12796.777\n",
      "regression loss:  22564.113\n",
      "episode: 822/1000000, returns: 1.2, epsilon: 0.66\n",
      "regression loss:  11312.018\n",
      "regression loss:  23612.39\n",
      "episode: 823/1000000, returns: 1e+01, epsilon: 0.66\n",
      "regression loss:  4738.6523\n",
      "regression loss:  19692.406\n",
      "episode: 824/1000000, returns: 5.0, epsilon: 0.66\n",
      "regression loss:  7348.6816\n",
      "regression loss:  16875.488\n",
      "episode: 825/1000000, returns: 2.1, epsilon: 0.66\n",
      "regression loss:  2353.176\n",
      "regression loss:  19427.588\n",
      "episode: 826/1000000, returns: -5.3, epsilon: 0.66\n",
      "regression loss:  2301.9468\n",
      "regression loss:  15532.568\n",
      "episode: 827/1000000, returns: 0.28, epsilon: 0.66\n",
      "regression loss:  3831.8438\n",
      "regression loss:  20926.658\n",
      "episode: 828/1000000, returns: 6.9, epsilon: 0.66\n",
      "regression loss:  1818.1549\n",
      "regression loss:  7608.4385\n",
      "episode: 829/1000000, returns: 0.25, epsilon: 0.66\n",
      "regression loss:  2537.8467\n",
      "regression loss:  11491.458\n",
      "episode: 830/1000000, returns: 2.5, epsilon: 0.66\n",
      "regression loss:  1546.8309\n",
      "regression loss:  8082.311\n",
      "episode: 831/1000000, returns: -3.3, epsilon: 0.66\n",
      "regression loss:  2693.5195\n",
      "regression loss:  6340.279\n",
      "episode: 832/1000000, returns: 3.0, epsilon: 0.66\n",
      "regression loss:  7746.6245\n",
      "regression loss:  3144.4321\n",
      "episode: 833/1000000, returns: -2.1, epsilon: 0.66\n",
      "regression loss:  1433.8446\n",
      "regression loss:  8577.849\n",
      "episode: 834/1000000, returns: 4.3, epsilon: 0.66\n",
      "regression loss:  4948.5376\n",
      "regression loss:  9302.407\n",
      "episode: 835/1000000, returns: 7.0, epsilon: 0.66\n",
      "regression loss:  750.81964\n",
      "regression loss:  6923.914\n",
      "episode: 836/1000000, returns: -4.8, epsilon: 0.66\n",
      "regression loss:  3569.349\n",
      "regression loss:  3880.9404\n",
      "episode: 837/1000000, returns: -0.28, epsilon: 0.66\n",
      "regression loss:  758.24335\n",
      "regression loss:  6091.4004\n",
      "episode: 838/1000000, returns: 0.83, epsilon: 0.66\n",
      "regression loss:  1211.7855\n",
      "regression loss:  4921.427\n",
      "episode: 839/1000000, returns: -0.45, epsilon: 0.66\n",
      "regression loss:  14366.8545\n",
      "regression loss:  9542.5625\n",
      "episode: 840/1000000, returns: 2.6, epsilon: 0.66\n",
      "regression loss:  11415.378\n",
      "regression loss:  4925.9404\n",
      "episode: 841/1000000, returns: 0.69, epsilon: 0.66\n",
      "regression loss:  5521.8213\n",
      "regression loss:  12601.801\n",
      "episode: 842/1000000, returns: -0.18, epsilon: 0.66\n",
      "regression loss:  7778.717\n",
      "regression loss:  7851.4497\n",
      "episode: 843/1000000, returns: -2.5, epsilon: 0.66\n",
      "regression loss:  2588.8716\n",
      "regression loss:  4463.7256\n",
      "episode: 844/1000000, returns: 1.3, epsilon: 0.66\n",
      "regression loss:  25892.572\n",
      "regression loss:  16031.333\n",
      "episode: 845/1000000, returns: 2.8, epsilon: 0.66\n",
      "regression loss:  22228.326\n",
      "regression loss:  7810.831\n",
      "episode: 846/1000000, returns: 6.2, epsilon: 0.66\n",
      "regression loss:  2259.9348\n",
      "regression loss:  33945.08\n",
      "episode: 847/1000000, returns: 2.5, epsilon: 0.65\n",
      "regression loss:  34168.082\n",
      "regression loss:  31592.287\n",
      "episode: 848/1000000, returns: 3.4, epsilon: 0.65\n",
      "regression loss:  36205.8\n",
      "regression loss:  35447.676\n",
      "episode: 849/1000000, returns: 2.4, epsilon: 0.65\n",
      "regression loss:  12392.768\n",
      "regression loss:  22950.219\n",
      "episode: 850/1000000, returns: 6.9, epsilon: 0.65\n",
      "regression loss:  29267.357\n",
      "regression loss:  43420.38\n",
      "episode: 851/1000000, returns: -0.93, epsilon: 0.65\n",
      "regression loss:  50332.977\n",
      "regression loss:  38271.01\n",
      "episode: 852/1000000, returns: -2.2, epsilon: 0.65\n",
      "regression loss:  15391.879\n",
      "regression loss:  44933.21\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 853/1000000, returns: 1.5, epsilon: 0.65\n",
      "regression loss:  7275.099\n",
      "regression loss:  45406.74\n",
      "episode: 854/1000000, returns: 3.9, epsilon: 0.65\n",
      "regression loss:  15702.849\n",
      "regression loss:  28265.27\n",
      "episode: 855/1000000, returns: 6.0, epsilon: 0.65\n",
      "regression loss:  12521.646\n",
      "regression loss:  30667.93\n",
      "episode: 856/1000000, returns: -6.0, epsilon: 0.65\n",
      "regression loss:  10529.732\n",
      "regression loss:  27524.777\n",
      "episode: 857/1000000, returns: 0.25, epsilon: 0.65\n",
      "regression loss:  20493.096\n",
      "regression loss:  52083.758\n",
      "episode: 858/1000000, returns: -3.7, epsilon: 0.65\n",
      "regression loss:  24987.629\n",
      "regression loss:  51392.52\n",
      "episode: 859/1000000, returns: 4.3, epsilon: 0.65\n",
      "regression loss:  4804.8564\n",
      "regression loss:  46896.984\n",
      "episode: 860/1000000, returns: -4.9, epsilon: 0.65\n",
      "regression loss:  13788.173\n",
      "regression loss:  32688.627\n",
      "episode: 861/1000000, returns: 0.6, epsilon: 0.65\n",
      "regression loss:  20661.502\n",
      "regression loss:  33545.395\n",
      "episode: 862/1000000, returns: 3.9, epsilon: 0.65\n",
      "regression loss:  4094.7888\n",
      "regression loss:  27513.67\n",
      "episode: 863/1000000, returns: -2.1, epsilon: 0.65\n",
      "regression loss:  11683.005\n",
      "regression loss:  34699.258\n",
      "episode: 864/1000000, returns: 1.4e+01, epsilon: 0.65\n",
      "regression loss:  5510.4443\n",
      "regression loss:  17903.156\n",
      "episode: 865/1000000, returns: 3.1, epsilon: 0.65\n",
      "regression loss:  21705.969\n",
      "regression loss:  34450.39\n",
      "episode: 866/1000000, returns: 1.0, epsilon: 0.65\n",
      "regression loss:  11670.611\n",
      "regression loss:  15017.003\n",
      "episode: 867/1000000, returns: -4.4, epsilon: 0.65\n",
      "regression loss:  8369.705\n",
      "regression loss:  26316.406\n",
      "episode: 868/1000000, returns: -1.9, epsilon: 0.65\n",
      "regression loss:  5905.6816\n",
      "regression loss:  18374.332\n",
      "episode: 869/1000000, returns: 1.1, epsilon: 0.65\n",
      "regression loss:  3325.1965\n",
      "regression loss:  12434.842\n",
      "episode: 870/1000000, returns: 0.72, epsilon: 0.65\n",
      "regression loss:  3276.1582\n",
      "regression loss:  11156.295\n",
      "episode: 871/1000000, returns: -0.83, epsilon: 0.65\n",
      "regression loss:  3280.9075\n",
      "regression loss:  9276.119\n",
      "episode: 872/1000000, returns: -6.2, epsilon: 0.65\n",
      "regression loss:  1211.8743\n",
      "regression loss:  6594.492\n",
      "episode: 873/1000000, returns: 0.47, epsilon: 0.65\n",
      "regression loss:  11251.912\n",
      "regression loss:  5678.0977\n",
      "episode: 874/1000000, returns: -1.4, epsilon: 0.65\n",
      "regression loss:  11111.277\n",
      "regression loss:  27949.744\n",
      "episode: 875/1000000, returns: -3.8, epsilon: 0.65\n",
      "regression loss:  7291.2954\n",
      "regression loss:  11079.0205\n",
      "episode: 876/1000000, returns: -2.2, epsilon: 0.65\n",
      "regression loss:  4959.7417\n",
      "regression loss:  16949.006\n",
      "episode: 877/1000000, returns: 2.6, epsilon: 0.64\n",
      "regression loss:  5246.703\n",
      "regression loss:  14220.342\n",
      "episode: 878/1000000, returns: -0.1, epsilon: 0.64\n",
      "regression loss:  18801.285\n",
      "regression loss:  33946.87\n",
      "episode: 879/1000000, returns: 0.89, epsilon: 0.64\n",
      "regression loss:  5126.9727\n",
      "regression loss:  16982.05\n",
      "episode: 880/1000000, returns: 2.0, epsilon: 0.64\n",
      "regression loss:  34856.918\n",
      "regression loss:  37103.164\n",
      "episode: 881/1000000, returns: 2.4, epsilon: 0.64\n",
      "regression loss:  22243.42\n",
      "regression loss:  32421.49\n",
      "episode: 882/1000000, returns: 7.1, epsilon: 0.64\n",
      "regression loss:  4681.1426\n",
      "regression loss:  27818.941\n",
      "episode: 883/1000000, returns: 1.5, epsilon: 0.64\n",
      "regression loss:  34285.59\n",
      "regression loss:  42566.914\n",
      "episode: 884/1000000, returns: -3.6, epsilon: 0.64\n",
      "regression loss:  52382.49\n",
      "regression loss:  33455.605\n",
      "episode: 885/1000000, returns: 0.71, epsilon: 0.64\n",
      "regression loss:  5669.629\n",
      "regression loss:  26081.871\n",
      "episode: 886/1000000, returns: -0.73, epsilon: 0.64\n",
      "regression loss:  13800.289\n",
      "regression loss:  64334.703\n",
      "episode: 887/1000000, returns: -0.81, epsilon: 0.64\n",
      "regression loss:  21012.764\n",
      "regression loss:  50049.445\n",
      "episode: 888/1000000, returns: 0.1, epsilon: 0.64\n",
      "regression loss:  6659.201\n",
      "regression loss:  41294.953\n",
      "episode: 889/1000000, returns: 1.8, epsilon: 0.64\n",
      "regression loss:  14235.064\n",
      "regression loss:  37368.74\n",
      "episode: 890/1000000, returns: 1.7, epsilon: 0.64\n",
      "regression loss:  33615.645\n",
      "regression loss:  47984.805\n",
      "episode: 891/1000000, returns: 1.8, epsilon: 0.64\n",
      "regression loss:  5702.4077\n",
      "regression loss:  36743.21\n",
      "episode: 892/1000000, returns: 1.6, epsilon: 0.64\n",
      "regression loss:  15813.348\n",
      "regression loss:  58271.387\n",
      "episode: 893/1000000, returns: 1.1, epsilon: 0.64\n",
      "regression loss:  15034.252\n",
      "regression loss:  33159.58\n",
      "episode: 894/1000000, returns: -0.046, epsilon: 0.64\n",
      "regression loss:  4483.971\n",
      "regression loss:  38366.715\n",
      "episode: 895/1000000, returns: 0.82, epsilon: 0.64\n",
      "regression loss:  3026.3496\n",
      "regression loss:  25269.367\n",
      "episode: 896/1000000, returns: -0.14, epsilon: 0.64\n",
      "regression loss:  3544.2302\n",
      "regression loss:  16999.684\n",
      "episode: 897/1000000, returns: -1.4, epsilon: 0.64\n",
      "regression loss:  4171.3013\n",
      "regression loss:  12422.66\n",
      "episode: 898/1000000, returns: 1.6, epsilon: 0.64\n",
      "regression loss:  2892.9915\n",
      "regression loss:  12558.559\n",
      "episode: 899/1000000, returns: 1.1, epsilon: 0.64\n",
      "regression loss:  2120.6682\n",
      "regression loss:  9217.786\n",
      "episode: 900/1000000, returns: -1.1, epsilon: 0.64\n",
      "regression loss:  3574.4302\n",
      "regression loss:  9426.288\n",
      "episode: 901/1000000, returns: -0.95, epsilon: 0.64\n",
      "regression loss:  1085.1132\n",
      "regression loss:  6092.576\n",
      "episode: 902/1000000, returns: 1.4, epsilon: 0.64\n",
      "regression loss:  13758.034\n",
      "regression loss:  6796.6123\n",
      "episode: 903/1000000, returns: 1.5, epsilon: 0.64\n",
      "regression loss:  11439.587\n",
      "regression loss:  32063.629\n",
      "episode: 904/1000000, returns: 0.61, epsilon: 0.64\n",
      "regression loss:  25574.72\n",
      "regression loss:  14695.934\n",
      "episode: 905/1000000, returns: -0.73, epsilon: 0.64\n",
      "regression loss:  4158.205\n",
      "regression loss:  16013.016\n",
      "episode: 906/1000000, returns: -2.2, epsilon: 0.64\n",
      "regression loss:  10521.683\n",
      "regression loss:  9805.832\n",
      "episode: 907/1000000, returns: 1.4, epsilon: 0.64\n",
      "regression loss:  17666.85\n",
      "regression loss:  8901.358\n",
      "episode: 908/1000000, returns: 3.7, epsilon: 0.64\n",
      "regression loss:  5124.4644\n",
      "regression loss:  12136.209\n",
      "episode: 909/1000000, returns: -0.17, epsilon: 0.63\n",
      "regression loss:  3533.8826\n",
      "regression loss:  10803.738\n",
      "episode: 910/1000000, returns: -3.2, epsilon: 0.63\n",
      "regression loss:  3819.1968\n",
      "regression loss:  10106.096\n",
      "episode: 911/1000000, returns: -3.7, epsilon: 0.63\n",
      "regression loss:  9333.196\n",
      "regression loss:  8563.178\n",
      "episode: 912/1000000, returns: 3.2, epsilon: 0.63\n",
      "regression loss:  3716.2693\n",
      "regression loss:  22419.023\n",
      "episode: 913/1000000, returns: 0.05, epsilon: 0.63\n",
      "regression loss:  11785.13\n",
      "regression loss:  30285.557\n",
      "episode: 914/1000000, returns: 0.044, epsilon: 0.63\n",
      "regression loss:  10904.174\n",
      "regression loss:  20268.568\n",
      "episode: 915/1000000, returns: -7.2, epsilon: 0.63\n",
      "regression loss:  3558.5708\n",
      "regression loss:  9404.373\n",
      "episode: 916/1000000, returns: 7.0, epsilon: 0.63\n",
      "regression loss:  35074.438\n",
      "regression loss:  35512.58\n",
      "episode: 917/1000000, returns: 9.7, epsilon: 0.63\n",
      "regression loss:  30556.705\n",
      "regression loss:  17144.432\n",
      "episode: 918/1000000, returns: 1.3, epsilon: 0.63\n",
      "regression loss:  4123.894\n",
      "regression loss:  20960.562\n",
      "episode: 919/1000000, returns: -0.0072, epsilon: 0.63\n",
      "regression loss:  12108.102\n",
      "regression loss:  31227.953\n",
      "episode: 920/1000000, returns: 2.0, epsilon: 0.63\n",
      "regression loss:  33814.75\n",
      "regression loss:  22769.514\n",
      "episode: 921/1000000, returns: 1.1, epsilon: 0.63\n",
      "regression loss:  5489.527\n",
      "regression loss:  16744.701\n",
      "episode: 922/1000000, returns: -1.9, epsilon: 0.63\n",
      "regression loss:  24157.227\n",
      "regression loss:  32618.842\n",
      "episode: 923/1000000, returns: 3.9, epsilon: 0.63\n",
      "regression loss:  33153.54\n",
      "regression loss:  37547.996\n",
      "episode: 924/1000000, returns: 0.48, epsilon: 0.63\n",
      "regression loss:  3460.8708\n",
      "regression loss:  22755.125\n",
      "episode: 925/1000000, returns: 0.81, epsilon: 0.63\n",
      "regression loss:  8994.436\n",
      "regression loss:  29927.447\n",
      "episode: 926/1000000, returns: 8.8, epsilon: 0.63\n",
      "regression loss:  10283.242\n",
      "regression loss:  7475.5117\n",
      "episode: 927/1000000, returns: -7.3, epsilon: 0.63\n",
      "regression loss:  6875.856\n",
      "regression loss:  23172.621\n",
      "episode: 928/1000000, returns: 3.1, epsilon: 0.63\n",
      "regression loss:  8804.652\n",
      "regression loss:  17069.69\n",
      "episode: 929/1000000, returns: -1.5, epsilon: 0.63\n",
      "regression loss:  13545.264\n",
      "regression loss:  38371.195\n",
      "episode: 930/1000000, returns: -0.61, epsilon: 0.63\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "regression loss:  9844.006\n",
      "regression loss:  22524.68\n",
      "episode: 931/1000000, returns: 0.5, epsilon: 0.63\n",
      "regression loss:  9896.055\n",
      "regression loss:  15250.621\n",
      "episode: 932/1000000, returns: -2.1, epsilon: 0.63\n",
      "regression loss:  18246.043\n",
      "regression loss:  20941.293\n",
      "episode: 933/1000000, returns: 7.7, epsilon: 0.63\n",
      "regression loss:  10152.227\n",
      "regression loss:  16790.137\n",
      "episode: 934/1000000, returns: -5.0, epsilon: 0.63\n",
      "regression loss:  3823.124\n",
      "regression loss:  12647.0625\n",
      "episode: 935/1000000, returns: 3.9, epsilon: 0.63\n",
      "regression loss:  6817.6724\n",
      "regression loss:  13179.5625\n",
      "episode: 936/1000000, returns: 2.3, epsilon: 0.63\n",
      "regression loss:  1207.7362\n",
      "regression loss:  23715.602\n",
      "episode: 937/1000000, returns: 2.4, epsilon: 0.63\n",
      "regression loss:  5118.275\n",
      "regression loss:  7912.925\n",
      "episode: 938/1000000, returns: -1.2, epsilon: 0.63\n",
      "regression loss:  2140.4673\n",
      "regression loss:  10443.975\n",
      "episode: 939/1000000, returns: -0.69, epsilon: 0.63\n",
      "regression loss:  5041.468\n",
      "regression loss:  11299.123\n",
      "episode: 940/1000000, returns: -0.41, epsilon: 0.62\n",
      "regression loss:  3504.1938\n",
      "regression loss:  15259.787\n",
      "episode: 941/1000000, returns: -1.4, epsilon: 0.62\n",
      "regression loss:  9372.96\n",
      "regression loss:  13264.189\n",
      "episode: 942/1000000, returns: -7.4, epsilon: 0.62\n",
      "regression loss:  4882.4487\n",
      "regression loss:  6291.6733\n",
      "episode: 943/1000000, returns: -6.4, epsilon: 0.62\n",
      "regression loss:  4414.8584\n",
      "regression loss:  11156.214\n",
      "episode: 944/1000000, returns: 1.8, epsilon: 0.62\n",
      "regression loss:  5249.676\n",
      "regression loss:  7055.919\n",
      "episode: 945/1000000, returns: 3.6, epsilon: 0.62\n",
      "regression loss:  4761.592\n",
      "regression loss:  15563.268\n",
      "episode: 946/1000000, returns: -6.0, epsilon: 0.62\n",
      "regression loss:  9423.533\n",
      "regression loss:  7595.901\n",
      "episode: 947/1000000, returns: -6.5, epsilon: 0.62\n",
      "regression loss:  1772.666\n",
      "regression loss:  10055.168\n",
      "episode: 948/1000000, returns: 7.1, epsilon: 0.62\n",
      "regression loss:  3768.273\n",
      "regression loss:  6950.1367\n",
      "episode: 949/1000000, returns: 0.58, epsilon: 0.62\n",
      "regression loss:  2998.6562\n",
      "regression loss:  8483.007\n",
      "episode: 950/1000000, returns: -0.064, epsilon: 0.62\n",
      "regression loss:  3119.427\n",
      "regression loss:  10691.083\n",
      "episode: 951/1000000, returns: 3.1, epsilon: 0.62\n",
      "regression loss:  2251.1611\n",
      "regression loss:  9808.259\n",
      "episode: 952/1000000, returns: -2.0, epsilon: 0.62\n",
      "regression loss:  1908.5349\n",
      "regression loss:  3906.9856\n",
      "episode: 953/1000000, returns: -0.83, epsilon: 0.62\n",
      "regression loss:  1720.0277\n",
      "regression loss:  6175.136\n",
      "episode: 954/1000000, returns: 0.89, epsilon: 0.62\n",
      "regression loss:  3443.0068\n",
      "regression loss:  7749.8154\n",
      "episode: 955/1000000, returns: 1.8, epsilon: 0.62\n",
      "regression loss:  2596.4473\n",
      "regression loss:  6837.5825\n",
      "episode: 956/1000000, returns: -1.0, epsilon: 0.62\n",
      "regression loss:  7147.596\n",
      "regression loss:  8719.5625\n",
      "episode: 957/1000000, returns: -2.1, epsilon: 0.62\n",
      "regression loss:  3918.6838\n",
      "regression loss:  5808.6177\n",
      "episode: 958/1000000, returns: 0.37, epsilon: 0.62\n",
      "regression loss:  2498.3174\n",
      "regression loss:  3943.942\n",
      "episode: 959/1000000, returns: -2.9, epsilon: 0.62\n",
      "regression loss:  16718.191\n",
      "regression loss:  11467.123\n",
      "episode: 960/1000000, returns: 1.2, epsilon: 0.62\n",
      "regression loss:  2265.894\n",
      "regression loss:  9628.543\n",
      "episode: 961/1000000, returns: -3.4, epsilon: 0.62\n",
      "regression loss:  21195.05\n",
      "regression loss:  16530.676\n",
      "episode: 962/1000000, returns: -5.4, epsilon: 0.62\n",
      "regression loss:  17908.941\n",
      "regression loss:  7999.13\n",
      "episode: 963/1000000, returns: 1.7, epsilon: 0.62\n",
      "regression loss:  2252.3142\n",
      "regression loss:  11361.138\n",
      "episode: 964/1000000, returns: 1.4, epsilon: 0.62\n",
      "regression loss:  14935.6875\n",
      "regression loss:  31056.285\n",
      "episode: 965/1000000, returns: 2.4, epsilon: 0.62\n",
      "regression loss:  8243.578\n",
      "regression loss:  16256.276\n",
      "episode: 966/1000000, returns: -7.1, epsilon: 0.62\n",
      "regression loss:  5878.049\n",
      "regression loss:  18370.639\n",
      "episode: 967/1000000, returns: -7.5, epsilon: 0.62\n",
      "regression loss:  21165.418\n",
      "regression loss:  17221.732\n",
      "episode: 968/1000000, returns: 0.59, epsilon: 0.62\n",
      "regression loss:  3678.3638\n",
      "regression loss:  19859.996\n",
      "episode: 969/1000000, returns: 3.2, epsilon: 0.62\n",
      "regression loss:  3242.9395\n",
      "regression loss:  19468.92\n",
      "episode: 970/1000000, returns: 0.62, epsilon: 0.62\n",
      "regression loss:  2264.1484\n",
      "regression loss:  14105.582\n",
      "episode: 971/1000000, returns: 0.36, epsilon: 0.62\n",
      "regression loss:  1901.0905\n",
      "regression loss:  12656.024\n",
      "episode: 972/1000000, returns: 1.3e+01, epsilon: 0.62\n",
      "regression loss:  1083.0139\n",
      "regression loss:  11369.98\n",
      "episode: 973/1000000, returns: -1.5, epsilon: 0.61\n",
      "regression loss:  3114.9387\n",
      "regression loss:  8490.479\n",
      "episode: 974/1000000, returns: -0.76, epsilon: 0.61\n",
      "regression loss:  1735.3685\n",
      "regression loss:  5601.161\n",
      "episode: 975/1000000, returns: -1.2, epsilon: 0.61\n",
      "regression loss:  6863.832\n",
      "regression loss:  4955.1006\n",
      "episode: 976/1000000, returns: 0.9, epsilon: 0.61\n",
      "regression loss:  3612.473\n",
      "regression loss:  4679.373\n",
      "episode: 977/1000000, returns: 7.3, epsilon: 0.61\n",
      "regression loss:  1869.0701\n",
      "regression loss:  3031.1333\n",
      "episode: 978/1000000, returns: -3.5, epsilon: 0.61\n",
      "regression loss:  580.4819\n",
      "regression loss:  3023.3992\n",
      "episode: 979/1000000, returns: -6.1, epsilon: 0.61\n",
      "regression loss:  318.94785\n",
      "regression loss:  3835.8584\n",
      "episode: 980/1000000, returns: 4.0, epsilon: 0.61\n",
      "regression loss:  1311.323\n",
      "regression loss:  1417.9493\n",
      "episode: 981/1000000, returns: -7.9, epsilon: 0.61\n",
      "regression loss:  3073.354\n",
      "regression loss:  5096.517\n",
      "episode: 982/1000000, returns: 1.4, epsilon: 0.61\n",
      "regression loss:  1364.9484\n",
      "regression loss:  4157.0244\n",
      "episode: 983/1000000, returns: 2.4, epsilon: 0.61\n",
      "regression loss:  1833.4768\n",
      "regression loss:  4461.517\n",
      "episode: 984/1000000, returns: 4.2, epsilon: 0.61\n",
      "regression loss:  709.0781\n",
      "regression loss:  3163.8193\n",
      "episode: 985/1000000, returns: -3.3, epsilon: 0.61\n",
      "regression loss:  1137.9692\n",
      "regression loss:  3127.7202\n",
      "episode: 986/1000000, returns: 0.76, epsilon: 0.61\n",
      "regression loss:  1396.6385\n",
      "regression loss:  2955.3381\n",
      "episode: 987/1000000, returns: -1.6, epsilon: 0.61\n",
      "regression loss:  727.0237\n",
      "regression loss:  2685.8984\n",
      "episode: 988/1000000, returns: -0.33, epsilon: 0.61\n",
      "regression loss:  3760.7905\n",
      "regression loss:  6116.2686\n",
      "episode: 989/1000000, returns: 0.33, epsilon: 0.61\n",
      "regression loss:  462.51184\n",
      "regression loss:  6332.1875\n",
      "episode: 990/1000000, returns: 3.6, epsilon: 0.61\n",
      "regression loss:  14200.504\n",
      "regression loss:  10592.318\n",
      "episode: 991/1000000, returns: -0.21, epsilon: 0.61\n",
      "regression loss:  2099.1748\n",
      "regression loss:  10812.252\n",
      "episode: 992/1000000, returns: -2.4, epsilon: 0.61\n",
      "regression loss:  15514.844\n",
      "regression loss:  27085.92\n",
      "episode: 993/1000000, returns: -0.17, epsilon: 0.61\n",
      "regression loss:  27062.463\n",
      "regression loss:  16346.714\n",
      "episode: 994/1000000, returns: 1.0, epsilon: 0.61\n",
      "regression loss:  1893.7716\n",
      "regression loss:  16155.963\n",
      "episode: 995/1000000, returns: 1.2, epsilon: 0.61\n",
      "regression loss:  25266.068\n",
      "regression loss:  27334.045\n",
      "episode: 996/1000000, returns: -1.3, epsilon: 0.61\n",
      "regression loss:  78370.586\n",
      "regression loss:  53534.746\n",
      "episode: 997/1000000, returns: 7.3, epsilon: 0.61\n",
      "regression loss:  18270.35\n",
      "regression loss:  28754.527\n",
      "episode: 998/1000000, returns: -1.0, epsilon: 0.61\n",
      "regression loss:  3515.0105\n",
      "regression loss:  39974.125\n",
      "episode: 999/1000000, returns: -2.9, epsilon: 0.61\n",
      "regression loss:  22388.521\n",
      "regression loss:  40841.633\n",
      "episode: 1000/1000000, returns: -5.4, epsilon: 0.61\n",
      "regression loss:  14537.219\n",
      "regression loss:  21175.373\n",
      "episode: 1001/1000000, returns: 0.53, epsilon: 0.61\n",
      "regression loss:  9857.8\n",
      "regression loss:  56632.957\n",
      "episode: 1002/1000000, returns: 2.8, epsilon: 0.61\n",
      "regression loss:  37955.082\n",
      "regression loss:  25556.84\n",
      "episode: 1003/1000000, returns: 5.8, epsilon: 0.61\n",
      "regression loss:  18636.816\n",
      "regression loss:  25482.176\n",
      "episode: 1004/1000000, returns: -0.2, epsilon: 0.61\n",
      "regression loss:  4900.9424\n",
      "regression loss:  43807.53\n",
      "episode: 1005/1000000, returns: 1.1, epsilon: 0.6\n",
      "regression loss:  23619.254\n",
      "regression loss:  43041.688\n",
      "episode: 1006/1000000, returns: 3.6, epsilon: 0.6\n",
      "regression loss:  10408.726\n",
      "regression loss:  29005.115\n",
      "episode: 1007/1000000, returns: 1.5e+01, epsilon: 0.6\n",
      "regression loss:  24882.902\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "regression loss:  38186.676\n",
      "episode: 1008/1000000, returns: -0.12, epsilon: 0.6\n",
      "regression loss:  44688.664\n",
      "regression loss:  69690.04\n",
      "episode: 1009/1000000, returns: 2.6, epsilon: 0.6\n",
      "regression loss:  14661.343\n",
      "regression loss:  31516.25\n",
      "episode: 1010/1000000, returns: 2.3, epsilon: 0.6\n",
      "regression loss:  2407.6475\n",
      "regression loss:  35040.82\n",
      "episode: 1011/1000000, returns: 0.99, epsilon: 0.6\n",
      "regression loss:  9150.687\n",
      "regression loss:  18164.492\n",
      "episode: 1012/1000000, returns: 0.92, epsilon: 0.6\n",
      "regression loss:  2825.307\n",
      "regression loss:  19892.6\n",
      "episode: 1013/1000000, returns: 4.5, epsilon: 0.6\n",
      "regression loss:  2001.0382\n",
      "regression loss:  13053.518\n",
      "episode: 1014/1000000, returns: -3.9, epsilon: 0.6\n",
      "regression loss:  1897.5681\n",
      "regression loss:  12345.024\n",
      "episode: 1015/1000000, returns: -0.32, epsilon: 0.6\n",
      "regression loss:  1742.8396\n",
      "regression loss:  10462.198\n",
      "episode: 1016/1000000, returns: -3.5, epsilon: 0.6\n",
      "regression loss:  3413.9087\n",
      "regression loss:  15046.189\n",
      "episode: 1017/1000000, returns: 1.7, epsilon: 0.6\n",
      "regression loss:  1391.707\n",
      "regression loss:  5551.329\n",
      "episode: 1018/1000000, returns: 3.3, epsilon: 0.6\n",
      "regression loss:  860.95874\n",
      "regression loss:  4926.513\n",
      "episode: 1019/1000000, returns: -3.3, epsilon: 0.6\n",
      "regression loss:  575.8796\n",
      "regression loss:  6135.0576\n",
      "episode: 1020/1000000, returns: 0.43, epsilon: 0.6\n",
      "regression loss:  4239.1436\n",
      "regression loss:  3468.7998\n",
      "episode: 1021/1000000, returns: -1.6, epsilon: 0.6\n",
      "regression loss:  2904.7107\n",
      "regression loss:  3436.0264\n",
      "episode: 1022/1000000, returns: -0.91, epsilon: 0.6\n",
      "regression loss:  12042.027\n",
      "regression loss:  10427.731\n",
      "episode: 1023/1000000, returns: 3.4, epsilon: 0.6\n",
      "regression loss:  6001.5225\n",
      "regression loss:  12934.114\n",
      "episode: 1024/1000000, returns: 7.7, epsilon: 0.6\n",
      "regression loss:  23305.832\n",
      "regression loss:  24917.03\n",
      "episode: 1025/1000000, returns: 1.7, epsilon: 0.6\n",
      "regression loss:  21177.084\n",
      "regression loss:  12433.307\n",
      "episode: 1026/1000000, returns: -3.8, epsilon: 0.6\n",
      "regression loss:  2516.7654\n",
      "regression loss:  13030.664\n",
      "episode: 1027/1000000, returns: 1.2, epsilon: 0.6\n",
      "regression loss:  20135.098\n",
      "regression loss:  27377.451\n",
      "episode: 1028/1000000, returns: 2.6, epsilon: 0.6\n",
      "regression loss:  24626.64\n",
      "regression loss:  17935.812\n",
      "episode: 1029/1000000, returns: -2.0, epsilon: 0.6\n",
      "regression loss:  1658.0796\n",
      "regression loss:  16488.969\n",
      "episode: 1030/1000000, returns: 4.2, epsilon: 0.6\n",
      "regression loss:  17590.037\n",
      "regression loss:  30488.426\n",
      "episode: 1031/1000000, returns: -0.74, epsilon: 0.6\n",
      "regression loss:  16679.166\n",
      "regression loss:  16932.062\n",
      "episode: 1032/1000000, returns: -4.4, epsilon: 0.6\n",
      "regression loss:  1591.3446\n",
      "regression loss:  18966.572\n",
      "episode: 1033/1000000, returns: -0.42, epsilon: 0.6\n",
      "regression loss:  46023.28\n",
      "regression loss:  39952.234\n",
      "episode: 1034/1000000, returns: -3.2, epsilon: 0.6\n",
      "regression loss:  55389.797\n",
      "regression loss:  38563.953\n",
      "episode: 1035/1000000, returns: -4.2, epsilon: 0.6\n",
      "regression loss:  10066.958\n",
      "regression loss:  30282.332\n",
      "episode: 1036/1000000, returns: 6.2, epsilon: 0.6\n",
      "regression loss:  10474.26\n",
      "regression loss:  46942.24\n",
      "episode: 1037/1000000, returns: 1.7, epsilon: 0.6\n",
      "regression loss:  15594.555\n",
      "regression loss:  31181.16\n",
      "episode: 1038/1000000, returns: -1.8, epsilon: 0.6\n",
      "regression loss:  6200.733\n",
      "regression loss:  27810.156\n",
      "episode: 1039/1000000, returns: 0.71, epsilon: 0.59\n",
      "regression loss:  12671.175\n",
      "regression loss:  37943.832\n",
      "episode: 1040/1000000, returns: 4.4, epsilon: 0.59\n",
      "regression loss:  47998.066\n",
      "regression loss:  36712.84\n",
      "episode: 1041/1000000, returns: -1.0, epsilon: 0.59\n",
      "regression loss:  10585.627\n",
      "regression loss:  21742.025\n",
      "episode: 1042/1000000, returns: 0.24, epsilon: 0.59\n",
      "regression loss:  4307.254\n",
      "regression loss:  32490.078\n",
      "episode: 1043/1000000, returns: 0.075, epsilon: 0.59\n",
      "regression loss:  13913.934\n",
      "regression loss:  19010.21\n",
      "episode: 1044/1000000, returns: 1.7, epsilon: 0.59\n",
      "regression loss:  2829.7273\n",
      "regression loss:  17023.207\n",
      "episode: 1045/1000000, returns: 1.7, epsilon: 0.59\n",
      "regression loss:  21404.719\n",
      "regression loss:  24634.205\n",
      "episode: 1046/1000000, returns: -7.4, epsilon: 0.59\n",
      "regression loss:  46667.074\n",
      "regression loss:  29999.059\n",
      "episode: 1047/1000000, returns: 4.5, epsilon: 0.59\n",
      "regression loss:  18295.52\n",
      "regression loss:  11836.185\n",
      "episode: 1048/1000000, returns: 0.57, epsilon: 0.59\n",
      "regression loss:  6225.296\n",
      "regression loss:  29599.738\n",
      "episode: 1049/1000000, returns: -6.1, epsilon: 0.59\n",
      "regression loss:  8150.71\n",
      "regression loss:  27409.742\n",
      "episode: 1050/1000000, returns: 2.0, epsilon: 0.59\n",
      "regression loss:  1615.7094\n",
      "regression loss:  20236.799\n",
      "episode: 1051/1000000, returns: 0.29, epsilon: 0.59\n",
      "regression loss:  3066.126\n",
      "regression loss:  16117.789\n",
      "episode: 1052/1000000, returns: 4.7, epsilon: 0.59\n",
      "regression loss:  6471.4375\n",
      "regression loss:  14750.238\n",
      "episode: 1053/1000000, returns: 5.7, epsilon: 0.59\n",
      "regression loss:  2328.884\n",
      "regression loss:  11886.963\n",
      "episode: 1054/1000000, returns: 1.0, epsilon: 0.59\n",
      "regression loss:  1860.3615\n",
      "regression loss:  12630.38\n",
      "episode: 1055/1000000, returns: 1.2, epsilon: 0.59\n",
      "regression loss:  850.369\n",
      "regression loss:  5852.597\n",
      "episode: 1056/1000000, returns: 0.85, epsilon: 0.59\n",
      "regression loss:  1208.1583\n",
      "regression loss:  6329.8633\n",
      "episode: 1057/1000000, returns: 2.7, epsilon: 0.59\n",
      "regression loss:  4695.7954\n",
      "regression loss:  5110.5464\n",
      "episode: 1058/1000000, returns: -1.2, epsilon: 0.59\n",
      "regression loss:  623.12213\n",
      "regression loss:  4089.358\n",
      "episode: 1059/1000000, returns: 1.7, epsilon: 0.59\n",
      "regression loss:  349.9645\n",
      "regression loss:  3766.8125\n",
      "episode: 1060/1000000, returns: 3.8, epsilon: 0.59\n",
      "regression loss:  1280.1349\n",
      "regression loss:  2460.1218\n",
      "episode: 1061/1000000, returns: -1.1e+01, epsilon: 0.59\n",
      "regression loss:  401.92282\n",
      "regression loss:  2316.2788\n",
      "episode: 1062/1000000, returns: 0.92, epsilon: 0.59\n",
      "regression loss:  223.96368\n",
      "regression loss:  1029.3491\n",
      "episode: 1063/1000000, returns: -0.1, epsilon: 0.59\n",
      "regression loss:  370.03625\n",
      "regression loss:  1256.192\n",
      "episode: 1064/1000000, returns: 5.9, epsilon: 0.59\n",
      "regression loss:  4451.679\n",
      "regression loss:  3948.7146\n",
      "episode: 1065/1000000, returns: 4.2, epsilon: 0.59\n",
      "regression loss:  2494.4763\n",
      "regression loss:  1130.6188\n",
      "episode: 1066/1000000, returns: 2.5, epsilon: 0.59\n",
      "regression loss:  540.1462\n",
      "regression loss:  3436.6575\n",
      "episode: 1067/1000000, returns: -1.9e+01, epsilon: 0.59\n",
      "regression loss:  3686.0205\n",
      "regression loss:  3394.975\n",
      "episode: 1068/1000000, returns: 5.5, epsilon: 0.59\n",
      "regression loss:  513.15515\n",
      "regression loss:  3954.299\n",
      "episode: 1069/1000000, returns: 1.8, epsilon: 0.59\n",
      "regression loss:  2812.4897\n",
      "regression loss:  2831.5928\n",
      "episode: 1070/1000000, returns: -4.7, epsilon: 0.59\n",
      "regression loss:  256.42606\n",
      "regression loss:  2905.1606\n",
      "episode: 1071/1000000, returns: -2.2, epsilon: 0.59\n",
      "regression loss:  139.89142\n",
      "regression loss:  2696.8428\n",
      "episode: 1072/1000000, returns: 3.9, epsilon: 0.59\n",
      "regression loss:  1601.5779\n",
      "regression loss:  1567.5974\n",
      "episode: 1073/1000000, returns: 0.22, epsilon: 0.58\n",
      "regression loss:  3168.614\n",
      "regression loss:  3527.145\n",
      "episode: 1074/1000000, returns: 0.53, epsilon: 0.58\n",
      "regression loss:  438.42798\n",
      "regression loss:  2790.2983\n",
      "episode: 1075/1000000, returns: -0.66, epsilon: 0.58\n",
      "regression loss:  4511.7563\n",
      "regression loss:  2130.312\n",
      "episode: 1076/1000000, returns: -1e+01, epsilon: 0.58\n",
      "regression loss:  1678.5432\n",
      "regression loss:  3324.3037\n",
      "episode: 1077/1000000, returns: -5.4, epsilon: 0.58\n",
      "regression loss:  4304.447\n",
      "regression loss:  4262.9746\n",
      "episode: 1078/1000000, returns: 0.17, epsilon: 0.58\n",
      "regression loss:  3518.1658\n",
      "regression loss:  2890.605\n",
      "episode: 1079/1000000, returns: 4.1, epsilon: 0.58\n",
      "regression loss:  3002.0566\n",
      "regression loss:  4175.994\n",
      "episode: 1080/1000000, returns: -3.7, epsilon: 0.58\n",
      "regression loss:  3872.9502\n",
      "regression loss:  6179.047\n",
      "episode: 1081/1000000, returns: 2.0, epsilon: 0.58\n",
      "regression loss:  9305.302\n",
      "regression loss:  11636.588\n",
      "episode: 1082/1000000, returns: 0.26, epsilon: 0.58\n",
      "regression loss:  7230.1353\n",
      "regression loss:  4352.918\n",
      "episode: 1083/1000000, returns: 0.068, epsilon: 0.58\n",
      "regression loss:  2537.28\n",
      "regression loss:  10046.504\n",
      "episode: 1084/1000000, returns: 2.6, epsilon: 0.58\n",
      "regression loss:  2920.825\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "regression loss:  9509.232\n",
      "episode: 1085/1000000, returns: -0.62, epsilon: 0.58\n",
      "regression loss:  2345.4426\n",
      "regression loss:  10789.191\n",
      "episode: 1086/1000000, returns: -1.1, epsilon: 0.58\n",
      "regression loss:  4382.991\n",
      "regression loss:  4479.633\n",
      "episode: 1087/1000000, returns: 2.3, epsilon: 0.58\n",
      "regression loss:  4176.91\n",
      "regression loss:  7814.1875\n",
      "episode: 1088/1000000, returns: -7.5, epsilon: 0.58\n",
      "regression loss:  3419.6821\n",
      "regression loss:  9274.872\n",
      "episode: 1089/1000000, returns: 1.7, epsilon: 0.58\n",
      "regression loss:  4005.7612\n",
      "regression loss:  10329.504\n",
      "episode: 1090/1000000, returns: 1.4, epsilon: 0.58\n",
      "regression loss:  3499.9902\n",
      "regression loss:  3417.5413\n",
      "episode: 1091/1000000, returns: 1.7, epsilon: 0.58\n",
      "regression loss:  922.0783\n",
      "regression loss:  10676.152\n",
      "episode: 1092/1000000, returns: 5.0, epsilon: 0.58\n",
      "regression loss:  1774.0925\n",
      "regression loss:  5464.84\n",
      "episode: 1093/1000000, returns: 3.0, epsilon: 0.58\n",
      "regression loss:  5377.71\n",
      "regression loss:  4632.202\n",
      "episode: 1094/1000000, returns: -1.2e+01, epsilon: 0.58\n",
      "regression loss:  1716.2158\n",
      "regression loss:  6874.7163\n",
      "episode: 1095/1000000, returns: -0.23, epsilon: 0.58\n",
      "regression loss:  3122.9692\n",
      "regression loss:  5237.027\n",
      "episode: 1096/1000000, returns: 7.2, epsilon: 0.58\n",
      "regression loss:  1081.4677\n",
      "regression loss:  3897.2007\n",
      "episode: 1097/1000000, returns: -3.9, epsilon: 0.58\n",
      "regression loss:  2863.6497\n",
      "regression loss:  4778.539\n",
      "episode: 1098/1000000, returns: 2.0, epsilon: 0.58\n",
      "regression loss:  553.0602\n",
      "regression loss:  4943.745\n",
      "episode: 1099/1000000, returns: 2.8, epsilon: 0.58\n",
      "regression loss:  2961.4763\n",
      "regression loss:  4416.022\n",
      "episode: 1100/1000000, returns: 5.5, epsilon: 0.58\n",
      "regression loss:  414.95703\n",
      "regression loss:  5029.3013\n",
      "episode: 1101/1000000, returns: 4.0, epsilon: 0.58\n",
      "regression loss:  5112.307\n",
      "regression loss:  5683.3457\n",
      "episode: 1102/1000000, returns: -1.3, epsilon: 0.58\n",
      "regression loss:  2405.9915\n",
      "regression loss:  7728.2803\n",
      "episode: 1103/1000000, returns: 1.5, epsilon: 0.58\n",
      "regression loss:  8241.597\n",
      "regression loss:  14023.174\n",
      "episode: 1104/1000000, returns: -3.2, epsilon: 0.58\n",
      "regression loss:  6350.731\n",
      "regression loss:  4210.0127\n",
      "episode: 1105/1000000, returns: -0.94, epsilon: 0.58\n",
      "regression loss:  1182.9657\n",
      "regression loss:  6724.9663\n",
      "episode: 1106/1000000, returns: 2.4, epsilon: 0.58\n",
      "regression loss:  5279.3496\n",
      "regression loss:  6741.149\n",
      "episode: 1107/1000000, returns: 4.2, epsilon: 0.57\n",
      "regression loss:  1164.1099\n",
      "regression loss:  5386.212\n",
      "episode: 1108/1000000, returns: 0.23, epsilon: 0.57\n",
      "regression loss:  1443.1667\n",
      "regression loss:  2696.8584\n",
      "episode: 1109/1000000, returns: 3.5, epsilon: 0.57\n",
      "regression loss:  1967.5424\n",
      "regression loss:  3903.043\n",
      "episode: 1110/1000000, returns: -1.1e+01, epsilon: 0.57\n",
      "regression loss:  7216.1636\n",
      "regression loss:  4075.0513\n",
      "episode: 1111/1000000, returns: 6.5, epsilon: 0.57\n",
      "regression loss:  1328.473\n",
      "regression loss:  9415.496\n",
      "episode: 1112/1000000, returns: -0.31, epsilon: 0.57\n",
      "regression loss:  13543.891\n",
      "regression loss:  18210.215\n",
      "episode: 1113/1000000, returns: -0.25, epsilon: 0.57\n",
      "regression loss:  3304.136\n",
      "regression loss:  7231.622\n",
      "episode: 1114/1000000, returns: 1.9, epsilon: 0.57\n",
      "regression loss:  2953.6504\n",
      "regression loss:  6262.54\n",
      "episode: 1115/1000000, returns: 1.3e+01, epsilon: 0.57\n",
      "regression loss:  29713.562\n",
      "regression loss:  9455.827\n",
      "episode: 1116/1000000, returns: 2.3, epsilon: 0.57\n",
      "regression loss:  4415.2837\n",
      "regression loss:  16530.012\n",
      "episode: 1117/1000000, returns: 1.4, epsilon: 0.57\n",
      "regression loss:  15294.942\n",
      "regression loss:  27109.47\n",
      "episode: 1118/1000000, returns: 0.41, epsilon: 0.57\n",
      "regression loss:  15545.847\n",
      "regression loss:  15805.477\n",
      "episode: 1119/1000000, returns: 1.6, epsilon: 0.57\n",
      "regression loss:  1852.1327\n",
      "regression loss:  12351.271\n",
      "episode: 1120/1000000, returns: -1.2, epsilon: 0.57\n",
      "regression loss:  10477.49\n",
      "regression loss:  7889.3145\n",
      "episode: 1121/1000000, returns: 4.1, epsilon: 0.57\n",
      "regression loss:  4361.968\n",
      "regression loss:  9629.168\n",
      "episode: 1122/1000000, returns: 0.43, epsilon: 0.57\n",
      "regression loss:  2102.0493\n",
      "regression loss:  12965.005\n",
      "episode: 1123/1000000, returns: -5.6, epsilon: 0.57\n",
      "regression loss:  1603.0098\n",
      "regression loss:  6892.538\n",
      "episode: 1124/1000000, returns: 0.74, epsilon: 0.57\n",
      "regression loss:  4298.293\n",
      "regression loss:  13944.805\n",
      "episode: 1125/1000000, returns: 4.3, epsilon: 0.57\n",
      "regression loss:  7648.0576\n",
      "regression loss:  5151.8604\n",
      "episode: 1126/1000000, returns: -1.8, epsilon: 0.57\n",
      "regression loss:  17994.13\n",
      "regression loss:  15189.656\n",
      "episode: 1127/1000000, returns: -0.19, epsilon: 0.57\n",
      "regression loss:  13888.413\n",
      "regression loss:  18140.684\n",
      "episode: 1128/1000000, returns: 1.7, epsilon: 0.57\n",
      "regression loss:  13238.388\n",
      "regression loss:  18930.18\n",
      "episode: 1129/1000000, returns: -0.014, epsilon: 0.57\n",
      "regression loss:  10612.596\n",
      "regression loss:  24500.148\n",
      "episode: 1130/1000000, returns: -0.087, epsilon: 0.57\n",
      "regression loss:  13521.02\n",
      "regression loss:  17887.258\n",
      "episode: 1131/1000000, returns: 1.6, epsilon: 0.57\n",
      "regression loss:  7089.773\n",
      "regression loss:  14451.653\n",
      "episode: 1132/1000000, returns: -0.76, epsilon: 0.57\n",
      "regression loss:  19767.705\n",
      "regression loss:  18259.041\n",
      "episode: 1133/1000000, returns: -8.9, epsilon: 0.57\n",
      "regression loss:  2854.3884\n",
      "regression loss:  21828.178\n",
      "episode: 1134/1000000, returns: 3.3, epsilon: 0.57\n",
      "regression loss:  6882.746\n",
      "regression loss:  21003.322\n",
      "episode: 1135/1000000, returns: -0.065, epsilon: 0.57\n",
      "regression loss:  4808.639\n",
      "regression loss:  16792.824\n",
      "episode: 1136/1000000, returns: -2.9, epsilon: 0.57\n",
      "regression loss:  19042.568\n",
      "regression loss:  20391.875\n",
      "episode: 1137/1000000, returns: -9.1, epsilon: 0.57\n",
      "regression loss:  21894.85\n",
      "regression loss:  15604.824\n",
      "episode: 1138/1000000, returns: -7.9, epsilon: 0.57\n",
      "regression loss:  1561.482\n",
      "regression loss:  11379.906\n",
      "episode: 1139/1000000, returns: 0.084, epsilon: 0.57\n",
      "regression loss:  10083.71\n",
      "regression loss:  27657.947\n",
      "episode: 1140/1000000, returns: 4.4, epsilon: 0.57\n",
      "regression loss:  8807.091\n",
      "regression loss:  9129.506\n",
      "episode: 1141/1000000, returns: -6.1, epsilon: 0.57\n",
      "regression loss:  7881.586\n",
      "regression loss:  15652.98\n",
      "episode: 1142/1000000, returns: 3.2, epsilon: 0.56\n",
      "regression loss:  13798.362\n",
      "regression loss:  37699.242\n",
      "episode: 1143/1000000, returns: 7.7, epsilon: 0.56\n",
      "regression loss:  1244.5626\n",
      "regression loss:  11988.545\n",
      "episode: 1144/1000000, returns: 8.2, epsilon: 0.56\n",
      "regression loss:  10820.814\n",
      "regression loss:  26373.523\n",
      "episode: 1145/1000000, returns: -2.3, epsilon: 0.56\n",
      "regression loss:  11193.356\n",
      "regression loss:  10514.702\n",
      "episode: 1146/1000000, returns: 0.68, epsilon: 0.56\n",
      "regression loss:  1072.7712\n",
      "regression loss:  7454.1885\n",
      "episode: 1147/1000000, returns: -0.9, epsilon: 0.56\n",
      "regression loss:  3826.1724\n",
      "regression loss:  9659.801\n",
      "episode: 1148/1000000, returns: -1.5, epsilon: 0.56\n",
      "regression loss:  2412.4883\n",
      "regression loss:  5919.8506\n",
      "episode: 1149/1000000, returns: -0.36, epsilon: 0.56\n",
      "regression loss:  3092.155\n",
      "regression loss:  5877.1553\n",
      "episode: 1150/1000000, returns: 4.2, epsilon: 0.56\n",
      "regression loss:  2970.6582\n",
      "regression loss:  3382.6338\n",
      "episode: 1151/1000000, returns: 4.0, epsilon: 0.56\n",
      "regression loss:  903.0542\n",
      "regression loss:  5852.8423\n",
      "episode: 1152/1000000, returns: 3.3, epsilon: 0.56\n",
      "regression loss:  2388.0115\n",
      "regression loss:  6501.8203\n",
      "episode: 1153/1000000, returns: -0.0049, epsilon: 0.56\n",
      "regression loss:  380.6899\n",
      "regression loss:  3956.796\n",
      "episode: 1154/1000000, returns: 0.81, epsilon: 0.56\n",
      "regression loss:  341.39847\n",
      "regression loss:  3318.7937\n",
      "episode: 1155/1000000, returns: 2.1, epsilon: 0.56\n",
      "regression loss:  1826.6216\n",
      "regression loss:  2722.2202\n",
      "episode: 1156/1000000, returns: -2.0, epsilon: 0.56\n",
      "regression loss:  1764.825\n",
      "regression loss:  4659.16\n",
      "episode: 1157/1000000, returns: 0.2, epsilon: 0.56\n",
      "regression loss:  825.95795\n",
      "regression loss:  3267.1821\n",
      "episode: 1158/1000000, returns: -1.2, epsilon: 0.56\n",
      "regression loss:  14650.756\n",
      "regression loss:  11144.411\n",
      "episode: 1159/1000000, returns: -0.32, epsilon: 0.56\n",
      "regression loss:  1872.6387\n",
      "regression loss:  7096.846\n",
      "episode: 1160/1000000, returns: -4.5, epsilon: 0.56\n",
      "regression loss:  2161.1128\n",
      "regression loss:  5683.811\n",
      "episode: 1161/1000000, returns: 2.3, epsilon: 0.56\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "regression loss:  5412.397\n",
      "regression loss:  8199.563\n",
      "episode: 1162/1000000, returns: 9.3, epsilon: 0.56\n",
      "regression loss:  20207.113\n",
      "regression loss:  20816.441\n",
      "episode: 1163/1000000, returns: 2.7, epsilon: 0.56\n",
      "regression loss:  7382.9736\n",
      "regression loss:  9101.075\n",
      "episode: 1164/1000000, returns: -0.77, epsilon: 0.56\n",
      "regression loss:  14149.453\n",
      "regression loss:  17775.137\n",
      "episode: 1165/1000000, returns: 3.9, epsilon: 0.56\n",
      "regression loss:  14811.942\n",
      "regression loss:  15407.896\n",
      "episode: 1166/1000000, returns: -0.6, epsilon: 0.56\n",
      "regression loss:  6473.0894\n",
      "regression loss:  8760.838\n",
      "episode: 1167/1000000, returns: 3.3, epsilon: 0.56\n",
      "regression loss:  2214.1362\n",
      "regression loss:  22135.227\n",
      "episode: 1168/1000000, returns: 0.91, epsilon: 0.56\n",
      "regression loss:  8837.234\n",
      "regression loss:  25111.75\n",
      "episode: 1169/1000000, returns: 7.3, epsilon: 0.56\n",
      "regression loss:  1346.7556\n",
      "regression loss:  14443.391\n",
      "episode: 1170/1000000, returns: 0.4, epsilon: 0.56\n",
      "regression loss:  15080.203\n",
      "regression loss:  13144.044\n",
      "episode: 1171/1000000, returns: -1e+01, epsilon: 0.56\n",
      "regression loss:  16671.18\n",
      "regression loss:  11953.32\n",
      "episode: 1172/1000000, returns: 0.97, epsilon: 0.56\n",
      "regression loss:  3668.8025\n",
      "regression loss:  8726.349\n",
      "episode: 1173/1000000, returns: -5.7, epsilon: 0.56\n",
      "regression loss:  21886.426\n",
      "regression loss:  29498.912\n",
      "episode: 1174/1000000, returns: 3.1, epsilon: 0.56\n",
      "regression loss:  14713.479\n",
      "regression loss:  25314.957\n",
      "episode: 1175/1000000, returns: 0.95, epsilon: 0.56\n",
      "regression loss:  5126.2188\n",
      "regression loss:  9919.383\n",
      "episode: 1176/1000000, returns: 8.6, epsilon: 0.56\n",
      "regression loss:  12286.843\n",
      "regression loss:  13624.011\n",
      "episode: 1177/1000000, returns: -3.7, epsilon: 0.56\n",
      "regression loss:  6540.1562\n",
      "regression loss:  12980.693\n",
      "episode: 1178/1000000, returns: 0.14, epsilon: 0.55\n",
      "regression loss:  3156.7083\n",
      "regression loss:  15963.076\n",
      "episode: 1179/1000000, returns: 3.1, epsilon: 0.55\n",
      "regression loss:  8726.41\n",
      "regression loss:  15549.054\n",
      "episode: 1180/1000000, returns: 2.4, epsilon: 0.55\n",
      "regression loss:  3892.535\n",
      "regression loss:  17022.232\n",
      "episode: 1181/1000000, returns: 1.7, epsilon: 0.55\n",
      "regression loss:  4521.087\n",
      "regression loss:  11514.226\n",
      "episode: 1182/1000000, returns: -3.4, epsilon: 0.55\n",
      "regression loss:  3049.0044\n",
      "regression loss:  8147.177\n",
      "episode: 1183/1000000, returns: 1.1, epsilon: 0.55\n",
      "regression loss:  2432.7356\n",
      "regression loss:  6327.417\n",
      "episode: 1184/1000000, returns: -2.4, epsilon: 0.55\n",
      "regression loss:  894.6419\n",
      "regression loss:  11644.232\n",
      "episode: 1185/1000000, returns: 1.7, epsilon: 0.55\n",
      "regression loss:  665.0212\n",
      "regression loss:  6805.5825\n",
      "episode: 1186/1000000, returns: 0.14, epsilon: 0.55\n",
      "regression loss:  846.33716\n",
      "regression loss:  3417.6824\n",
      "episode: 1187/1000000, returns: 7.5, epsilon: 0.55\n",
      "regression loss:  493.78436\n",
      "regression loss:  2253.8235\n",
      "episode: 1188/1000000, returns: 2.7, epsilon: 0.55\n",
      "regression loss:  2306.3037\n",
      "regression loss:  2615.9614\n",
      "episode: 1189/1000000, returns: 0.22, epsilon: 0.55\n",
      "regression loss:  124.648506\n",
      "regression loss:  2973.7263\n",
      "episode: 1190/1000000, returns: 2.8, epsilon: 0.55\n",
      "regression loss:  3812.6748\n",
      "regression loss:  1830.9116\n",
      "episode: 1191/1000000, returns: 2.6, epsilon: 0.55\n",
      "regression loss:  867.0895\n",
      "regression loss:  2480.2441\n",
      "episode: 1192/1000000, returns: -0.2, epsilon: 0.55\n",
      "regression loss:  2828.156\n",
      "regression loss:  2955.3118\n",
      "episode: 1193/1000000, returns: -0.54, epsilon: 0.55\n",
      "regression loss:  1785.2866\n",
      "regression loss:  2966.0278\n",
      "episode: 1194/1000000, returns: 8.9, epsilon: 0.55\n",
      "regression loss:  947.5212\n",
      "regression loss:  2570.6536\n",
      "episode: 1195/1000000, returns: -0.24, epsilon: 0.55\n",
      "regression loss:  2291.855\n",
      "regression loss:  5286.7217\n",
      "episode: 1196/1000000, returns: -5.8, epsilon: 0.55\n",
      "regression loss:  312.66037\n",
      "regression loss:  4811.29\n",
      "episode: 1197/1000000, returns: 0.27, epsilon: 0.55\n",
      "regression loss:  6977.802\n",
      "regression loss:  6369.7725\n",
      "episode: 1198/1000000, returns: -1.1e+01, epsilon: 0.55\n",
      "regression loss:  443.45428\n",
      "regression loss:  6101.0405\n",
      "episode: 1199/1000000, returns: 1.4, epsilon: 0.55\n",
      "regression loss:  18848.236\n",
      "regression loss:  12807.183\n",
      "episode: 1200/1000000, returns: -2.5, epsilon: 0.55\n",
      "regression loss:  2746.3875\n",
      "regression loss:  5842.3564\n",
      "episode: 1201/1000000, returns: -4.6, epsilon: 0.55\n",
      "regression loss:  5191.675\n",
      "regression loss:  14606.127\n",
      "episode: 1202/1000000, returns: -0.021, epsilon: 0.55\n",
      "regression loss:  5368.5884\n",
      "regression loss:  10606.9375\n",
      "episode: 1203/1000000, returns: 0.013, epsilon: 0.55\n",
      "regression loss:  7213.614\n",
      "regression loss:  16987.416\n",
      "episode: 1204/1000000, returns: -3.3, epsilon: 0.55\n",
      "regression loss:  2811.3523\n",
      "regression loss:  6436.343\n",
      "episode: 1205/1000000, returns: 3.5, epsilon: 0.55\n",
      "regression loss:  1757.2651\n",
      "regression loss:  5437.6885\n",
      "episode: 1206/1000000, returns: 3.2, epsilon: 0.55\n",
      "regression loss:  5542.501\n",
      "regression loss:  7070.891\n",
      "episode: 1207/1000000, returns: -1.1e+01, epsilon: 0.55\n",
      "regression loss:  1662.7501\n",
      "regression loss:  4775.778\n",
      "episode: 1208/1000000, returns: -3.9, epsilon: 0.55\n",
      "regression loss:  4052.455\n",
      "regression loss:  9413.041\n",
      "episode: 1209/1000000, returns: 2.1, epsilon: 0.55\n",
      "regression loss:  1842.8602\n",
      "regression loss:  9000.471\n",
      "episode: 1210/1000000, returns: 0.25, epsilon: 0.55\n",
      "regression loss:  12217.834\n",
      "regression loss:  16129.117\n",
      "episode: 1211/1000000, returns: 2.6, epsilon: 0.55\n",
      "regression loss:  14710.308\n",
      "regression loss:  8851.908\n",
      "episode: 1212/1000000, returns: 2.2, epsilon: 0.55\n",
      "regression loss:  2347.9631\n",
      "regression loss:  9931.457\n",
      "episode: 1213/1000000, returns: -0.061, epsilon: 0.55\n",
      "regression loss:  1504.7323\n",
      "regression loss:  6878.077\n",
      "episode: 1214/1000000, returns: 0.03, epsilon: 0.54\n",
      "regression loss:  1543.0876\n",
      "regression loss:  8801.395\n",
      "episode: 1215/1000000, returns: 1.3, epsilon: 0.54\n",
      "regression loss:  666.7568\n",
      "regression loss:  4217.0566\n",
      "episode: 1216/1000000, returns: 0.14, epsilon: 0.54\n",
      "regression loss:  564.14856\n",
      "regression loss:  3858.2283\n",
      "episode: 1217/1000000, returns: -8.4, epsilon: 0.54\n",
      "regression loss:  898.9126\n",
      "regression loss:  2628.1562\n",
      "episode: 1218/1000000, returns: -0.37, epsilon: 0.54\n",
      "regression loss:  508.5841\n",
      "regression loss:  2604.1382\n",
      "episode: 1219/1000000, returns: -0.27, epsilon: 0.54\n",
      "regression loss:  1184.6705\n",
      "regression loss:  3296.31\n",
      "episode: 1220/1000000, returns: 2.9, epsilon: 0.54\n",
      "regression loss:  322.45682\n",
      "regression loss:  2200.461\n",
      "episode: 1221/1000000, returns: -0.0055, epsilon: 0.54\n",
      "regression loss:  185.02972\n",
      "regression loss:  2584.0227\n",
      "episode: 1222/1000000, returns: -4.4, epsilon: 0.54\n",
      "regression loss:  245.03224\n",
      "regression loss:  2349.7207\n",
      "episode: 1223/1000000, returns: 0.84, epsilon: 0.54\n",
      "regression loss:  460.90417\n",
      "regression loss:  1367.7168\n",
      "episode: 1224/1000000, returns: -1.0, epsilon: 0.54\n",
      "regression loss:  3581.9646\n",
      "regression loss:  2674.3657\n",
      "episode: 1225/1000000, returns: 1.0, epsilon: 0.54\n",
      "regression loss:  214.37903\n",
      "regression loss:  4610.657\n",
      "episode: 1226/1000000, returns: 2.8, epsilon: 0.54\n",
      "regression loss:  2964.187\n",
      "regression loss:  973.76685\n",
      "episode: 1227/1000000, returns: 3.0, epsilon: 0.54\n",
      "regression loss:  204.86906\n",
      "regression loss:  3022.8955\n",
      "episode: 1228/1000000, returns: 2.0, epsilon: 0.54\n",
      "regression loss:  10728.196\n",
      "regression loss:  5712.021\n",
      "episode: 1229/1000000, returns: 0.096, epsilon: 0.54\n",
      "regression loss:  1181.964\n",
      "regression loss:  2264.169\n",
      "episode: 1230/1000000, returns: -4.0, epsilon: 0.54\n",
      "regression loss:  863.5311\n",
      "regression loss:  1018.3871\n",
      "episode: 1231/1000000, returns: 2.8, epsilon: 0.54\n",
      "regression loss:  1346.5708\n",
      "regression loss:  2008.4015\n",
      "episode: 1232/1000000, returns: -0.24, epsilon: 0.54\n",
      "regression loss:  1413.6127\n",
      "regression loss:  2391.0232\n",
      "episode: 1233/1000000, returns: -0.071, epsilon: 0.54\n",
      "regression loss:  766.42334\n",
      "regression loss:  4202.1895\n",
      "episode: 1234/1000000, returns: 4.7, epsilon: 0.54\n",
      "regression loss:  1421.6191\n",
      "regression loss:  1393.3772\n",
      "episode: 1235/1000000, returns: -8.2, epsilon: 0.54\n",
      "regression loss:  1316.2961\n",
      "regression loss:  2140.0986\n",
      "episode: 1236/1000000, returns: 5.0, epsilon: 0.54\n",
      "regression loss:  612.0084\n",
      "regression loss:  1391.2312\n",
      "episode: 1237/1000000, returns: -1.7, epsilon: 0.54\n",
      "regression loss:  10427.429\n",
      "regression loss:  7028.5547\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 1238/1000000, returns: 8.0, epsilon: 0.54\n",
      "regression loss:  1589.6797\n",
      "regression loss:  1670.1667\n",
      "episode: 1239/1000000, returns: 4.7, epsilon: 0.54\n",
      "regression loss:  2979.9468\n",
      "regression loss:  2257.3843\n",
      "episode: 1240/1000000, returns: -1.2e+01, epsilon: 0.54\n",
      "regression loss:  1031.3135\n",
      "regression loss:  5028.84\n",
      "episode: 1241/1000000, returns: -4.5, epsilon: 0.54\n",
      "regression loss:  4748.008\n",
      "regression loss:  5642.5273\n",
      "episode: 1242/1000000, returns: 1.8, epsilon: 0.54\n",
      "regression loss:  649.8128\n",
      "regression loss:  2974.0981\n",
      "episode: 1243/1000000, returns: 0.16, epsilon: 0.54\n",
      "regression loss:  1423.0817\n",
      "regression loss:  5026.982\n",
      "episode: 1244/1000000, returns: 5.0, epsilon: 0.54\n",
      "regression loss:  172.3166\n",
      "regression loss:  2445.3765\n",
      "episode: 1245/1000000, returns: -3.2, epsilon: 0.54\n",
      "regression loss:  590.6795\n",
      "regression loss:  1232.6896\n",
      "episode: 1246/1000000, returns: 0.89, epsilon: 0.54\n",
      "regression loss:  326.67242\n",
      "regression loss:  2892.0537\n",
      "episode: 1247/1000000, returns: 0.13, epsilon: 0.54\n",
      "regression loss:  1509.9181\n",
      "regression loss:  569.1687\n",
      "episode: 1248/1000000, returns: 2.4, epsilon: 0.54\n",
      "regression loss:  210.86107\n",
      "regression loss:  853.97064\n",
      "episode: 1249/1000000, returns: 1.7, epsilon: 0.54\n",
      "regression loss:  155.46771\n",
      "regression loss:  1183.8448\n",
      "episode: 1250/1000000, returns: 7.1, epsilon: 0.54\n",
      "regression loss:  295.66742\n",
      "regression loss:  1181.7089\n",
      "episode: 1251/1000000, returns: -0.56, epsilon: 0.53\n",
      "regression loss:  2163.609\n",
      "regression loss:  1877.2019\n",
      "episode: 1252/1000000, returns: 1.0, epsilon: 0.53\n",
      "regression loss:  467.9684\n",
      "regression loss:  3412.794\n",
      "episode: 1253/1000000, returns: -2.4, epsilon: 0.53\n",
      "regression loss:  5018.5015\n",
      "regression loss:  7417.3555\n",
      "episode: 1254/1000000, returns: -1.3, epsilon: 0.53\n",
      "regression loss:  2963.6548\n",
      "regression loss:  7603.9834\n",
      "episode: 1255/1000000, returns: 0.28, epsilon: 0.53\n",
      "regression loss:  9322.961\n",
      "regression loss:  13038.668\n",
      "episode: 1256/1000000, returns: 1.1, epsilon: 0.53\n",
      "regression loss:  11478.689\n",
      "regression loss:  9194.538\n",
      "episode: 1257/1000000, returns: 2.8, epsilon: 0.53\n",
      "regression loss:  613.4549\n",
      "regression loss:  6653.795\n",
      "episode: 1258/1000000, returns: 5.4, epsilon: 0.53\n",
      "regression loss:  19510.576\n",
      "regression loss:  23062.854\n",
      "episode: 1259/1000000, returns: 1.3, epsilon: 0.53\n",
      "regression loss:  5401.97\n",
      "regression loss:  6767.0947\n",
      "episode: 1260/1000000, returns: 0.011, epsilon: 0.53\n",
      "regression loss:  4304.94\n",
      "regression loss:  10858.007\n",
      "episode: 1261/1000000, returns: 1.7, epsilon: 0.53\n",
      "regression loss:  8065.4175\n",
      "regression loss:  9916.463\n",
      "episode: 1262/1000000, returns: 1e+01, epsilon: 0.53\n",
      "regression loss:  585.8707\n",
      "regression loss:  7437.8936\n",
      "episode: 1263/1000000, returns: -4.5, epsilon: 0.53\n",
      "regression loss:  2152.3787\n",
      "regression loss:  5700.7314\n",
      "episode: 1264/1000000, returns: 1.1, epsilon: 0.53\n",
      "regression loss:  642.30994\n",
      "regression loss:  3371.8125\n",
      "episode: 1265/1000000, returns: 1.0, epsilon: 0.53\n",
      "regression loss:  261.6603\n",
      "regression loss:  2855.3003\n",
      "episode: 1266/1000000, returns: 3.8, epsilon: 0.53\n",
      "regression loss:  1047.2433\n",
      "regression loss:  2273.0854\n",
      "episode: 1267/1000000, returns: -4.9, epsilon: 0.53\n",
      "regression loss:  624.113\n",
      "regression loss:  4862.8716\n",
      "episode: 1268/1000000, returns: 0.58, epsilon: 0.53\n",
      "regression loss:  14237.533\n",
      "regression loss:  15201.788\n",
      "episode: 1269/1000000, returns: -1.4, epsilon: 0.53\n",
      "regression loss:  7701.1914\n",
      "regression loss:  2349.3428\n",
      "episode: 1270/1000000, returns: 4.0, epsilon: 0.53\n",
      "regression loss:  838.5542\n",
      "regression loss:  9210.571\n",
      "episode: 1271/1000000, returns: -0.78, epsilon: 0.53\n",
      "regression loss:  16445.578\n",
      "regression loss:  21456.035\n",
      "episode: 1272/1000000, returns: 0.75, epsilon: 0.53\n",
      "regression loss:  21242.135\n",
      "regression loss:  19119.4\n",
      "episode: 1273/1000000, returns: 4.5, epsilon: 0.53\n",
      "regression loss:  753.1834\n",
      "regression loss:  16796.463\n",
      "episode: 1274/1000000, returns: -0.38, epsilon: 0.53\n",
      "regression loss:  2058.6729\n",
      "regression loss:  3075.122\n",
      "episode: 1275/1000000, returns: -3.7, epsilon: 0.53\n",
      "regression loss:  7918.303\n",
      "regression loss:  2688.996\n",
      "episode: 1276/1000000, returns: 5.0, epsilon: 0.53\n",
      "regression loss:  8783.364\n",
      "regression loss:  3869.5679\n",
      "episode: 1277/1000000, returns: 0.36, epsilon: 0.53\n",
      "regression loss:  982.241\n",
      "regression loss:  5894.001\n",
      "episode: 1278/1000000, returns: -0.54, epsilon: 0.53\n",
      "regression loss:  20378.906\n",
      "regression loss:  21991.69\n",
      "episode: 1279/1000000, returns: 0.25, epsilon: 0.53\n",
      "regression loss:  10640.316\n",
      "regression loss:  9414.039\n",
      "episode: 1280/1000000, returns: 0.42, epsilon: 0.53\n",
      "regression loss:  2516.4485\n",
      "regression loss:  17453.664\n",
      "episode: 1281/1000000, returns: 1.6, epsilon: 0.53\n",
      "regression loss:  5186.87\n",
      "regression loss:  8769.4375\n",
      "episode: 1282/1000000, returns: -1e+01, epsilon: 0.53\n",
      "regression loss:  3848.7014\n",
      "regression loss:  13650.073\n",
      "episode: 1283/1000000, returns: 2.9, epsilon: 0.53\n",
      "regression loss:  2659.0205\n",
      "regression loss:  11157.336\n",
      "episode: 1284/1000000, returns: -0.24, epsilon: 0.53\n",
      "regression loss:  13100.868\n",
      "regression loss:  17822.215\n",
      "episode: 1285/1000000, returns: 0.77, epsilon: 0.53\n",
      "regression loss:  8521.7\n",
      "regression loss:  7102.17\n",
      "episode: 1286/1000000, returns: -0.95, epsilon: 0.53\n",
      "regression loss:  1056.5538\n",
      "regression loss:  9916.733\n",
      "episode: 1287/1000000, returns: 1.5, epsilon: 0.53\n",
      "regression loss:  5354.7573\n",
      "regression loss:  11927.615\n",
      "episode: 1288/1000000, returns: 2.9, epsilon: 0.53\n",
      "regression loss:  617.3158\n",
      "regression loss:  7361.299\n",
      "episode: 1289/1000000, returns: 6.7, epsilon: 0.52\n",
      "regression loss:  5994.308\n",
      "regression loss:  10589.1875\n",
      "episode: 1290/1000000, returns: 2.6, epsilon: 0.52\n",
      "regression loss:  997.8142\n",
      "regression loss:  8397.096\n",
      "episode: 1291/1000000, returns: -2.3, epsilon: 0.52\n",
      "regression loss:  23568.047\n",
      "regression loss:  16160.709\n",
      "episode: 1292/1000000, returns: -3.6, epsilon: 0.52\n",
      "regression loss:  9480.985\n",
      "regression loss:  10730.603\n",
      "episode: 1293/1000000, returns: 0.4, epsilon: 0.52\n",
      "regression loss:  2773.616\n",
      "regression loss:  10931.551\n",
      "episode: 1294/1000000, returns: 0.52, epsilon: 0.52\n",
      "regression loss:  12711.397\n",
      "regression loss:  10160.916\n",
      "episode: 1295/1000000, returns: -6.9, epsilon: 0.52\n",
      "regression loss:  2512.258\n",
      "regression loss:  9050.577\n",
      "episode: 1296/1000000, returns: -0.71, epsilon: 0.52\n",
      "regression loss:  28305.865\n",
      "regression loss:  52731.887\n",
      "episode: 1297/1000000, returns: 3.1, epsilon: 0.52\n",
      "regression loss:  67893.07\n",
      "regression loss:  27889.348\n",
      "episode: 1298/1000000, returns: 3.3, epsilon: 0.52\n",
      "regression loss:  19931.438\n",
      "regression loss:  11546.391\n",
      "episode: 1299/1000000, returns: -3.3, epsilon: 0.52\n",
      "regression loss:  2642.6033\n",
      "regression loss:  22068.047\n",
      "episode: 1300/1000000, returns: 2.9, epsilon: 0.52\n",
      "regression loss:  8356.284\n",
      "regression loss:  12632.535\n",
      "episode: 1301/1000000, returns: -3.5, epsilon: 0.52\n",
      "regression loss:  1884.2064\n",
      "regression loss:  8114.754\n",
      "episode: 1302/1000000, returns: 0.37, epsilon: 0.52\n",
      "regression loss:  22238.363\n",
      "regression loss:  44816.184\n",
      "episode: 1303/1000000, returns: 7.3, epsilon: 0.52\n",
      "regression loss:  25540.36\n",
      "regression loss:  16268.638\n",
      "episode: 1304/1000000, returns: 3.6, epsilon: 0.52\n",
      "regression loss:  878.78253\n",
      "regression loss:  18809.164\n",
      "episode: 1305/1000000, returns: 1.4, epsilon: 0.52\n",
      "regression loss:  9475.592\n",
      "regression loss:  23424.695\n",
      "episode: 1306/1000000, returns: -0.69, epsilon: 0.52\n",
      "regression loss:  8786.168\n",
      "regression loss:  10910.336\n",
      "episode: 1307/1000000, returns: 3.4, epsilon: 0.52\n",
      "regression loss:  1843.4352\n",
      "regression loss:  20175.438\n",
      "episode: 1308/1000000, returns: 1.2, epsilon: 0.52\n",
      "regression loss:  4264.732\n",
      "regression loss:  9230.604\n",
      "episode: 1309/1000000, returns: 1.5, epsilon: 0.52\n",
      "regression loss:  2572.472\n",
      "regression loss:  13192.484\n",
      "episode: 1310/1000000, returns: -3.2, epsilon: 0.52\n",
      "regression loss:  3543.5522\n",
      "regression loss:  7167.3164\n",
      "episode: 1311/1000000, returns: 5.8, epsilon: 0.52\n",
      "regression loss:  2188.3098\n",
      "regression loss:  4496.444\n",
      "episode: 1312/1000000, returns: 3.1, epsilon: 0.52\n",
      "regression loss:  530.4664\n",
      "regression loss:  8178.913\n",
      "episode: 1313/1000000, returns: 1.5, epsilon: 0.52\n",
      "regression loss:  2844.051\n",
      "regression loss:  6493.269\n",
      "episode: 1314/1000000, returns: 0.11, epsilon: 0.52\n",
      "regression loss:  1468.5989\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "regression loss:  4852.2256\n",
      "episode: 1315/1000000, returns: 5.5, epsilon: 0.52\n",
      "regression loss:  5086.5996\n",
      "regression loss:  9177.443\n",
      "episode: 1316/1000000, returns: 0.98, epsilon: 0.52\n",
      "regression loss:  486.6043\n",
      "regression loss:  2857.9653\n",
      "episode: 1317/1000000, returns: 3.2, epsilon: 0.52\n",
      "regression loss:  298.7509\n",
      "regression loss:  3998.4536\n",
      "episode: 1318/1000000, returns: 0.22, epsilon: 0.52\n",
      "regression loss:  1650.2708\n",
      "regression loss:  2192.899\n",
      "episode: 1319/1000000, returns: 1.5, epsilon: 0.52\n",
      "regression loss:  203.42862\n",
      "regression loss:  2329.477\n",
      "episode: 1320/1000000, returns: 0.86, epsilon: 0.52\n",
      "regression loss:  1065.8778\n",
      "regression loss:  2528.579\n",
      "episode: 1321/1000000, returns: 1e+01, epsilon: 0.52\n",
      "regression loss:  3463.4995\n",
      "regression loss:  3937.7307\n",
      "episode: 1322/1000000, returns: 5.1, epsilon: 0.52\n",
      "regression loss:  1816.9476\n",
      "regression loss:  2699.7356\n",
      "episode: 1323/1000000, returns: 1.4, epsilon: 0.52\n",
      "regression loss:  390.5864\n",
      "regression loss:  1327.0017\n",
      "episode: 1324/1000000, returns: 1.3, epsilon: 0.52\n",
      "regression loss:  2799.3423\n",
      "regression loss:  1745.1125\n",
      "episode: 1325/1000000, returns: -0.87, epsilon: 0.52\n",
      "regression loss:  1965.9796\n",
      "regression loss:  2663.6797\n",
      "episode: 1326/1000000, returns: 1.6, epsilon: 0.52\n",
      "regression loss:  777.4945\n",
      "regression loss:  3866.5388\n",
      "episode: 1327/1000000, returns: -1.3, epsilon: 0.51\n",
      "regression loss:  9461.317\n",
      "regression loss:  12586.607\n",
      "episode: 1328/1000000, returns: 2.9, epsilon: 0.51\n",
      "regression loss:  9837.673\n",
      "regression loss:  6088.9043\n",
      "episode: 1329/1000000, returns: 4.3, epsilon: 0.51\n",
      "regression loss:  2335.7568\n",
      "regression loss:  3146.0107\n",
      "episode: 1330/1000000, returns: -6.9, epsilon: 0.51\n",
      "regression loss:  12236.183\n",
      "regression loss:  19646.408\n",
      "episode: 1331/1000000, returns: -3.9, epsilon: 0.51\n",
      "regression loss:  15783.049\n",
      "regression loss:  6632.257\n",
      "episode: 1332/1000000, returns: -5.2, epsilon: 0.51\n",
      "regression loss:  1383.2789\n",
      "regression loss:  9127.866\n",
      "episode: 1333/1000000, returns: 1.1, epsilon: 0.51\n",
      "regression loss:  2882.231\n",
      "regression loss:  6072.917\n",
      "episode: 1334/1000000, returns: 1e+01, epsilon: 0.51\n",
      "regression loss:  1473.3972\n",
      "regression loss:  8241.271\n",
      "episode: 1335/1000000, returns: -3.5, epsilon: 0.51\n",
      "regression loss:  3311.0078\n",
      "regression loss:  8771.998\n",
      "episode: 1336/1000000, returns: 0.04, epsilon: 0.51\n",
      "regression loss:  1096.7434\n",
      "regression loss:  8886.762\n",
      "episode: 1337/1000000, returns: 1.8, epsilon: 0.51\n",
      "regression loss:  921.7373\n",
      "regression loss:  5928.314\n",
      "episode: 1338/1000000, returns: 3.3, epsilon: 0.51\n",
      "regression loss:  1653.3579\n",
      "regression loss:  3605.1946\n",
      "episode: 1339/1000000, returns: 1.5, epsilon: 0.51\n",
      "regression loss:  1740.6895\n",
      "regression loss:  5385.132\n",
      "episode: 1340/1000000, returns: -1.8, epsilon: 0.51\n",
      "regression loss:  980.35583\n",
      "regression loss:  3909.1155\n",
      "episode: 1341/1000000, returns: 0.33, epsilon: 0.51\n",
      "regression loss:  3535.0085\n",
      "regression loss:  6151.3584\n",
      "episode: 1342/1000000, returns: 4.6, epsilon: 0.51\n",
      "regression loss:  631.74835\n",
      "regression loss:  1943.6451\n",
      "episode: 1343/1000000, returns: -0.56, epsilon: 0.51\n",
      "regression loss:  1288.9667\n",
      "regression loss:  2252.431\n",
      "episode: 1344/1000000, returns: 6.2, epsilon: 0.51\n",
      "regression loss:  568.7922\n",
      "regression loss:  2691.7417\n",
      "episode: 1345/1000000, returns: -1.3, epsilon: 0.51\n",
      "regression loss:  2528.676\n",
      "regression loss:  3250.0935\n",
      "episode: 1346/1000000, returns: 0.69, epsilon: 0.51\n",
      "regression loss:  1827.1796\n",
      "regression loss:  2177.8154\n",
      "episode: 1347/1000000, returns: 0.61, epsilon: 0.51\n",
      "regression loss:  3909.791\n",
      "regression loss:  5592.379\n",
      "episode: 1348/1000000, returns: 2.3e+01, epsilon: 0.51\n",
      "regression loss:  1444.4106\n",
      "regression loss:  4616.711\n",
      "episode: 1349/1000000, returns: 4.2, epsilon: 0.51\n",
      "regression loss:  1918.742\n",
      "regression loss:  6017.247\n",
      "episode: 1350/1000000, returns: 5.0, epsilon: 0.51\n",
      "regression loss:  6210.646\n",
      "regression loss:  4358.202\n",
      "episode: 1351/1000000, returns: -2.6, epsilon: 0.51\n",
      "regression loss:  3347.28\n",
      "regression loss:  4329.5713\n",
      "episode: 1352/1000000, returns: 0.37, epsilon: 0.51\n",
      "regression loss:  3116.537\n",
      "regression loss:  7096.708\n",
      "episode: 1353/1000000, returns: 2.7, epsilon: 0.51\n",
      "regression loss:  1747.611\n",
      "regression loss:  5043.726\n",
      "episode: 1354/1000000, returns: -0.61, epsilon: 0.51\n",
      "regression loss:  2674.3096\n",
      "regression loss:  4319.946\n",
      "episode: 1355/1000000, returns: 3.1, epsilon: 0.51\n",
      "regression loss:  1356.5928\n",
      "regression loss:  6476.0005\n",
      "episode: 1356/1000000, returns: 1.3, epsilon: 0.51\n",
      "regression loss:  1313.6708\n",
      "regression loss:  3610.901\n",
      "episode: 1357/1000000, returns: 0.97, epsilon: 0.51\n",
      "regression loss:  1517.1466\n",
      "regression loss:  11502.573\n",
      "episode: 1358/1000000, returns: 0.78, epsilon: 0.51\n",
      "regression loss:  804.5902\n",
      "regression loss:  1833.8862\n",
      "episode: 1359/1000000, returns: 3.0, epsilon: 0.51\n",
      "regression loss:  2501.7668\n",
      "regression loss:  2550.9136\n",
      "episode: 1360/1000000, returns: 1.3, epsilon: 0.51\n",
      "regression loss:  563.96045\n",
      "regression loss:  2238.509\n",
      "episode: 1361/1000000, returns: 2.4, epsilon: 0.51\n",
      "regression loss:  8435.397\n",
      "regression loss:  4210.6724\n",
      "episode: 1362/1000000, returns: 4.7, epsilon: 0.51\n",
      "regression loss:  210.70143\n",
      "regression loss:  5763.0776\n",
      "episode: 1363/1000000, returns: -1.3, epsilon: 0.51\n",
      "regression loss:  4422.8843\n",
      "regression loss:  5083.0537\n",
      "episode: 1364/1000000, returns: 1.4, epsilon: 0.51\n",
      "regression loss:  690.7231\n",
      "regression loss:  5619.464\n",
      "episode: 1365/1000000, returns: 3.9, epsilon: 0.51\n",
      "regression loss:  12639.383\n",
      "regression loss:  18050.383\n",
      "episode: 1366/1000000, returns: 1.4, epsilon: 0.51\n",
      "regression loss:  12706.311\n",
      "regression loss:  4477.5303\n",
      "episode: 1367/1000000, returns: -3.9, epsilon: 0.5\n",
      "regression loss:  947.2383\n",
      "regression loss:  13127.098\n",
      "episode: 1368/1000000, returns: 1.6e+01, epsilon: 0.5\n",
      "regression loss:  3031.4915\n",
      "regression loss:  4253.6646\n",
      "episode: 1369/1000000, returns: 3.0, epsilon: 0.5\n",
      "regression loss:  1885.3269\n",
      "regression loss:  8916.537\n",
      "episode: 1370/1000000, returns: 0.02, epsilon: 0.5\n",
      "regression loss:  2261.8857\n",
      "regression loss:  4077.1758\n",
      "episode: 1371/1000000, returns: -0.068, epsilon: 0.5\n",
      "regression loss:  3107.586\n",
      "regression loss:  5364.9385\n",
      "episode: 1372/1000000, returns: -5.6, epsilon: 0.5\n",
      "regression loss:  2150.1086\n",
      "regression loss:  3447.956\n",
      "episode: 1373/1000000, returns: -5.3, epsilon: 0.5\n",
      "regression loss:  398.928\n",
      "regression loss:  5513.2817\n",
      "episode: 1374/1000000, returns: -1.1, epsilon: 0.5\n",
      "regression loss:  3133.4485\n",
      "regression loss:  7103.453\n",
      "episode: 1375/1000000, returns: 0.37, epsilon: 0.5\n",
      "regression loss:  490.2649\n",
      "regression loss:  4545.3047\n",
      "episode: 1376/1000000, returns: -0.037, epsilon: 0.5\n",
      "regression loss:  740.8176\n",
      "regression loss:  3792.7993\n",
      "episode: 1377/1000000, returns: 2.6, epsilon: 0.5\n",
      "regression loss:  1360.9336\n",
      "regression loss:  4688.819\n",
      "episode: 1378/1000000, returns: -1.6, epsilon: 0.5\n",
      "regression loss:  6145.213\n",
      "regression loss:  7313.216\n",
      "episode: 1379/1000000, returns: 0.93, epsilon: 0.5\n",
      "regression loss:  1195.1185\n",
      "regression loss:  3094.664\n",
      "episode: 1380/1000000, returns: -8.6, epsilon: 0.5\n",
      "regression loss:  15255.112\n",
      "regression loss:  25174.477\n",
      "episode: 1381/1000000, returns: -0.33, epsilon: 0.5\n",
      "regression loss:  21202.246\n",
      "regression loss:  10361.287\n",
      "episode: 1382/1000000, returns: -1.4, epsilon: 0.5\n",
      "regression loss:  767.415\n",
      "regression loss:  7179.0654\n",
      "episode: 1383/1000000, returns: -0.67, epsilon: 0.5\n",
      "regression loss:  23169.168\n",
      "regression loss:  41785.492\n",
      "episode: 1384/1000000, returns: 0.48, epsilon: 0.5\n",
      "regression loss:  45255.47\n",
      "regression loss:  30995.38\n",
      "episode: 1385/1000000, returns: 9.3, epsilon: 0.5\n",
      "regression loss:  4795.7886\n",
      "regression loss:  6552.376\n",
      "episode: 1386/1000000, returns: 6.1, epsilon: 0.5\n",
      "regression loss:  29823.873\n",
      "regression loss:  45497.887\n",
      "episode: 1387/1000000, returns: -0.68, epsilon: 0.5\n",
      "regression loss:  34841.254\n",
      "regression loss:  26582.965\n",
      "episode: 1388/1000000, returns: -2.9, epsilon: 0.5\n",
      "regression loss:  12933.668\n",
      "regression loss:  5338.6377\n",
      "episode: 1389/1000000, returns: 1.2, epsilon: 0.5\n",
      "regression loss:  1436.459\n",
      "regression loss:  17953.557\n",
      "episode: 1390/1000000, returns: 3.0, epsilon: 0.5\n",
      "regression loss:  10094.1\n",
      "regression loss:  19795.867\n",
      "episode: 1391/1000000, returns: -0.29, epsilon: 0.5\n",
      "regression loss:  2983.025\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "regression loss:  7635.4785\n",
      "episode: 1392/1000000, returns: -3.6, epsilon: 0.5\n",
      "regression loss:  4234.0293\n",
      "regression loss:  12651.375\n",
      "episode: 1393/1000000, returns: 1e+01, epsilon: 0.5\n",
      "regression loss:  7261.2437\n",
      "regression loss:  3598.0605\n",
      "episode: 1394/1000000, returns: 6.8, epsilon: 0.5\n",
      "regression loss:  3852.6868\n",
      "regression loss:  5566.3315\n",
      "episode: 1395/1000000, returns: -2.1, epsilon: 0.5\n",
      "regression loss:  1179.7927\n",
      "regression loss:  5801.6484\n",
      "episode: 1396/1000000, returns: 3.4, epsilon: 0.5\n",
      "regression loss:  1476.2411\n",
      "regression loss:  5297.863\n",
      "episode: 1397/1000000, returns: 1.2e+01, epsilon: 0.5\n",
      "regression loss:  1112.9867\n",
      "regression loss:  2853.507\n",
      "episode: 1398/1000000, returns: 8.2, epsilon: 0.5\n",
      "regression loss:  1103.6526\n",
      "regression loss:  5126.255\n",
      "episode: 1399/1000000, returns: 5.1, epsilon: 0.5\n",
      "regression loss:  237.05281\n",
      "regression loss:  3050.828\n",
      "episode: 1400/1000000, returns: 1.2, epsilon: 0.5\n",
      "regression loss:  354.87592\n",
      "regression loss:  2168.8857\n",
      "episode: 1401/1000000, returns: -0.57, epsilon: 0.5\n",
      "regression loss:  108.80798\n",
      "regression loss:  2001.398\n",
      "episode: 1402/1000000, returns: 0.024, epsilon: 0.5\n",
      "regression loss:  146.1597\n",
      "regression loss:  1829.3992\n",
      "episode: 1403/1000000, returns: 1.5, epsilon: 0.5\n",
      "regression loss:  198.52682\n",
      "regression loss:  1652.8995\n",
      "episode: 1404/1000000, returns: 4.1, epsilon: 0.5\n",
      "regression loss:  171.0898\n",
      "regression loss:  3980.689\n",
      "episode: 1405/1000000, returns: 5.2, epsilon: 0.5\n",
      "regression loss:  391.77124\n",
      "regression loss:  1366.017\n",
      "episode: 1406/1000000, returns: 0.62, epsilon: 0.5\n",
      "regression loss:  192.83336\n",
      "regression loss:  1110.5306\n",
      "episode: 1407/1000000, returns: -0.19, epsilon: 0.49\n",
      "regression loss:  1521.8628\n",
      "regression loss:  1832.3176\n",
      "episode: 1408/1000000, returns: 0.03, epsilon: 0.49\n",
      "regression loss:  116.41821\n",
      "regression loss:  3509.397\n",
      "episode: 1409/1000000, returns: 0.97, epsilon: 0.49\n",
      "regression loss:  5356.7617\n",
      "regression loss:  3436.9165\n",
      "episode: 1410/1000000, returns: -1.5, epsilon: 0.49\n",
      "regression loss:  90.36246\n",
      "regression loss:  4448.4795\n",
      "episode: 1411/1000000, returns: -3.4, epsilon: 0.49\n",
      "regression loss:  8365.9795\n",
      "regression loss:  3440.0498\n",
      "episode: 1412/1000000, returns: -9.3, epsilon: 0.49\n",
      "regression loss:  502.89667\n",
      "regression loss:  2551.4446\n",
      "episode: 1413/1000000, returns: 0.6, epsilon: 0.49\n",
      "regression loss:  4094.1472\n",
      "regression loss:  6475.29\n",
      "episode: 1414/1000000, returns: -1.8, epsilon: 0.49\n",
      "regression loss:  2082.8782\n",
      "regression loss:  3724.9343\n",
      "episode: 1415/1000000, returns: -2.5, epsilon: 0.49\n",
      "regression loss:  504.22836\n",
      "regression loss:  1823.0598\n",
      "episode: 1416/1000000, returns: -1.1e+01, epsilon: 0.49\n",
      "regression loss:  623.8664\n",
      "regression loss:  2538.2056\n",
      "episode: 1417/1000000, returns: 9.6, epsilon: 0.49\n",
      "regression loss:  192.28989\n",
      "regression loss:  3866.4622\n",
      "episode: 1418/1000000, returns: 0.75, epsilon: 0.49\n",
      "regression loss:  160.5836\n",
      "regression loss:  3341.6846\n",
      "episode: 1419/1000000, returns: 2.4, epsilon: 0.49\n",
      "regression loss:  3654.3445\n",
      "regression loss:  2113.0881\n",
      "episode: 1420/1000000, returns: -2.0, epsilon: 0.49\n",
      "regression loss:  576.9675\n",
      "regression loss:  7742.7056\n",
      "episode: 1421/1000000, returns: 6.7, epsilon: 0.49\n",
      "regression loss:  5597.4604\n",
      "regression loss:  3165.058\n",
      "episode: 1422/1000000, returns: 1.8, epsilon: 0.49\n",
      "regression loss:  893.75134\n",
      "regression loss:  5183.407\n",
      "episode: 1423/1000000, returns: -8.5, epsilon: 0.49\n",
      "regression loss:  4834.697\n",
      "regression loss:  5298.503\n",
      "episode: 1424/1000000, returns: 0.93, epsilon: 0.49\n",
      "regression loss:  317.43048\n",
      "regression loss:  5164.4365\n",
      "episode: 1425/1000000, returns: 1.4, epsilon: 0.49\n",
      "regression loss:  1805.7589\n",
      "regression loss:  5363.285\n",
      "episode: 1426/1000000, returns: 1.8e+01, epsilon: 0.49\n",
      "regression loss:  1074.8561\n",
      "regression loss:  5177.4087\n",
      "episode: 1427/1000000, returns: 0.33, epsilon: 0.49\n",
      "regression loss:  5827.4766\n",
      "regression loss:  8866.151\n",
      "episode: 1428/1000000, returns: 2.2, epsilon: 0.49\n",
      "regression loss:  5214.1216\n",
      "regression loss:  2853.3162\n",
      "episode: 1429/1000000, returns: 4.9, epsilon: 0.49\n",
      "regression loss:  3065.3828\n",
      "regression loss:  7778.172\n",
      "episode: 1430/1000000, returns: -2.2, epsilon: 0.49\n",
      "regression loss:  10501.593\n",
      "regression loss:  3223.5527\n",
      "episode: 1431/1000000, returns: 3.4, epsilon: 0.49\n",
      "regression loss:  526.5966\n",
      "regression loss:  3453.009\n",
      "episode: 1432/1000000, returns: -4.2, epsilon: 0.49\n",
      "regression loss:  4420.035\n",
      "regression loss:  3311.3267\n",
      "episode: 1433/1000000, returns: -7.5, epsilon: 0.49\n",
      "regression loss:  1789.3019\n",
      "regression loss:  9632.963\n",
      "episode: 1434/1000000, returns: 4.8, epsilon: 0.49\n",
      "regression loss:  5333.1733\n",
      "regression loss:  1764.4626\n",
      "episode: 1435/1000000, returns: -3.4, epsilon: 0.49\n",
      "regression loss:  2256.6282\n",
      "regression loss:  2973.8704\n",
      "episode: 1436/1000000, returns: -2.4, epsilon: 0.49\n",
      "regression loss:  2459.3396\n",
      "regression loss:  3746.2412\n",
      "episode: 1437/1000000, returns: -1.6, epsilon: 0.49\n",
      "regression loss:  2089.9834\n",
      "regression loss:  5949.5786\n",
      "episode: 1438/1000000, returns: 3.0, epsilon: 0.49\n",
      "regression loss:  1780.9827\n",
      "regression loss:  2285.9272\n",
      "episode: 1439/1000000, returns: -1.2e+01, epsilon: 0.49\n",
      "regression loss:  3119.4644\n",
      "regression loss:  4312.7334\n",
      "episode: 1440/1000000, returns: 3.6, epsilon: 0.49\n",
      "regression loss:  751.36145\n",
      "regression loss:  6210.5015\n",
      "episode: 1441/1000000, returns: 1.3, epsilon: 0.49\n",
      "regression loss:  5090.321\n",
      "regression loss:  5979.0186\n",
      "episode: 1442/1000000, returns: 0.95, epsilon: 0.49\n",
      "regression loss:  1658.943\n",
      "regression loss:  3378.9292\n",
      "episode: 1443/1000000, returns: 1.1, epsilon: 0.49\n",
      "regression loss:  7733.8364\n",
      "regression loss:  6571.98\n",
      "episode: 1444/1000000, returns: 0.65, epsilon: 0.49\n",
      "regression loss:  2682.2625\n",
      "regression loss:  5003.0947\n",
      "episode: 1445/1000000, returns: 2.2, epsilon: 0.49\n",
      "regression loss:  3490.8367\n",
      "regression loss:  5964.552\n",
      "episode: 1446/1000000, returns: 0.95, epsilon: 0.49\n",
      "regression loss:  4070.357\n",
      "regression loss:  7696.9336\n",
      "episode: 1447/1000000, returns: 5.9, epsilon: 0.48\n",
      "regression loss:  13429.075\n",
      "regression loss:  26055.79\n",
      "episode: 1448/1000000, returns: -1.1, epsilon: 0.48\n",
      "regression loss:  18491.404\n",
      "regression loss:  10972.971\n",
      "episode: 1449/1000000, returns: 0.14, epsilon: 0.48\n",
      "regression loss:  1233.833\n",
      "regression loss:  17255.04\n",
      "episode: 1450/1000000, returns: -7.8, epsilon: 0.48\n",
      "regression loss:  4758.0254\n",
      "regression loss:  12241.48\n",
      "episode: 1451/1000000, returns: 1.5, epsilon: 0.48\n",
      "regression loss:  3453.594\n",
      "regression loss:  5529.532\n",
      "episode: 1452/1000000, returns: 2.9, epsilon: 0.48\n",
      "regression loss:  4887.808\n",
      "regression loss:  6428.0586\n",
      "episode: 1453/1000000, returns: 5.1, epsilon: 0.48\n",
      "regression loss:  3511.0076\n",
      "regression loss:  5981.9297\n",
      "episode: 1454/1000000, returns: -1.7, epsilon: 0.48\n",
      "regression loss:  2000.0967\n",
      "regression loss:  9114.155\n",
      "episode: 1455/1000000, returns: -0.88, epsilon: 0.48\n",
      "regression loss:  1022.1508\n",
      "regression loss:  6933.983\n",
      "episode: 1456/1000000, returns: 0.58, epsilon: 0.48\n",
      "regression loss:  7945.8516\n",
      "regression loss:  8927.755\n",
      "episode: 1457/1000000, returns: -0.29, epsilon: 0.48\n",
      "regression loss:  4688.153\n",
      "regression loss:  3574.2136\n",
      "episode: 1458/1000000, returns: 1.3, epsilon: 0.48\n",
      "regression loss:  3732.8228\n",
      "regression loss:  10904.859\n",
      "episode: 1459/1000000, returns: -0.11, epsilon: 0.48\n",
      "regression loss:  9760.516\n",
      "regression loss:  8654.916\n",
      "episode: 1460/1000000, returns: 1.6, epsilon: 0.48\n",
      "regression loss:  685.3466\n",
      "regression loss:  6414.7207\n",
      "episode: 1461/1000000, returns: -3.6, epsilon: 0.48\n",
      "regression loss:  935.82715\n",
      "regression loss:  3504.901\n",
      "episode: 1462/1000000, returns: 3.5, epsilon: 0.48\n",
      "regression loss:  1460.7543\n",
      "regression loss:  3077.2742\n",
      "episode: 1463/1000000, returns: -3.5, epsilon: 0.48\n",
      "regression loss:  547.7324\n",
      "regression loss:  6418.379\n",
      "episode: 1464/1000000, returns: -4.0, epsilon: 0.48\n",
      "regression loss:  1229.2485\n",
      "regression loss:  4637.2803\n",
      "episode: 1465/1000000, returns: -1.3, epsilon: 0.48\n",
      "regression loss:  1199.486\n",
      "regression loss:  1470.4863\n",
      "episode: 1466/1000000, returns: -2.5, epsilon: 0.48\n",
      "regression loss:  9060.21\n",
      "regression loss:  3376.6235\n",
      "episode: 1467/1000000, returns: 0.47, epsilon: 0.48\n",
      "regression loss:  3524.1987\n",
      "regression loss:  6974.6294\n",
      "episode: 1468/1000000, returns: 5.3, epsilon: 0.48\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "regression loss:  1344.8414\n",
      "regression loss:  4445.9663\n",
      "episode: 1469/1000000, returns: 2.8, epsilon: 0.48\n",
      "regression loss:  577.5993\n",
      "regression loss:  2103.722\n",
      "episode: 1470/1000000, returns: -4.2, epsilon: 0.48\n",
      "regression loss:  5961.641\n",
      "regression loss:  3674.0366\n",
      "episode: 1471/1000000, returns: -4.7, epsilon: 0.48\n",
      "regression loss:  1676.4987\n",
      "regression loss:  5954.9463\n",
      "episode: 1472/1000000, returns: 0.54, epsilon: 0.48\n",
      "regression loss:  6449.969\n",
      "regression loss:  3938.3535\n",
      "episode: 1473/1000000, returns: -0.95, epsilon: 0.48\n",
      "regression loss:  883.56714\n",
      "regression loss:  5273.751\n",
      "episode: 1474/1000000, returns: -5.8, epsilon: 0.48\n",
      "regression loss:  9916.641\n",
      "regression loss:  7975.1562\n",
      "episode: 1475/1000000, returns: 5.9, epsilon: 0.48\n",
      "regression loss:  23733.117\n",
      "regression loss:  5140.1084\n",
      "episode: 1476/1000000, returns: 1.1, epsilon: 0.48\n",
      "regression loss:  1486.7299\n",
      "regression loss:  8458.23\n",
      "episode: 1477/1000000, returns: -5.6, epsilon: 0.48\n",
      "regression loss:  466.4214\n",
      "regression loss:  3967.386\n",
      "episode: 1478/1000000, returns: -2.7, epsilon: 0.48\n",
      "regression loss:  314.0915\n",
      "regression loss:  7834.2188\n",
      "episode: 1479/1000000, returns: 2.5, epsilon: 0.48\n",
      "regression loss:  2127.8638\n",
      "regression loss:  2870.3484\n",
      "episode: 1480/1000000, returns: 0.14, epsilon: 0.48\n",
      "regression loss:  3701.4834\n",
      "regression loss:  2492.651\n",
      "episode: 1481/1000000, returns: 1.2, epsilon: 0.48\n",
      "regression loss:  1262.4875\n",
      "regression loss:  7069.6553\n",
      "episode: 1482/1000000, returns: -4.1, epsilon: 0.48\n",
      "regression loss:  5146.201\n",
      "regression loss:  5073.1396\n",
      "episode: 1483/1000000, returns: 1.1, epsilon: 0.48\n",
      "regression loss:  1248.4218\n",
      "regression loss:  6871.787\n",
      "episode: 1484/1000000, returns: 1.5, epsilon: 0.48\n",
      "regression loss:  24349.686\n",
      "regression loss:  28078.986\n",
      "episode: 1485/1000000, returns: -2.2, epsilon: 0.48\n",
      "regression loss:  34406.453\n",
      "regression loss:  23446.453\n",
      "episode: 1486/1000000, returns: 0.22, epsilon: 0.48\n",
      "regression loss:  4452.9326\n",
      "regression loss:  8752.766\n",
      "episode: 1487/1000000, returns: 2.1, epsilon: 0.48\n",
      "regression loss:  15910.28\n",
      "regression loss:  26149.742\n",
      "episode: 1488/1000000, returns: -0.3, epsilon: 0.48\n",
      "regression loss:  7713.4976\n",
      "regression loss:  17006.35\n",
      "episode: 1489/1000000, returns: 3.7, epsilon: 0.47\n",
      "regression loss:  1439.8295\n",
      "regression loss:  9681.3125\n",
      "episode: 1490/1000000, returns: -0.9, epsilon: 0.47\n",
      "regression loss:  17508.656\n",
      "regression loss:  38246.766\n",
      "episode: 1491/1000000, returns: -3.9, epsilon: 0.47\n",
      "regression loss:  39635.367\n",
      "regression loss:  20760.525\n",
      "episode: 1492/1000000, returns: 0.53, epsilon: 0.47\n",
      "regression loss:  4219.737\n",
      "regression loss:  8970.523\n",
      "episode: 1493/1000000, returns: 0.87, epsilon: 0.47\n",
      "regression loss:  19837.52\n",
      "regression loss:  31376.89\n",
      "episode: 1494/1000000, returns: 0.91, epsilon: 0.47\n",
      "regression loss:  10230.871\n",
      "regression loss:  14284.475\n",
      "episode: 1495/1000000, returns: 0.35, epsilon: 0.47\n",
      "regression loss:  4170.855\n",
      "regression loss:  7117.393\n",
      "episode: 1496/1000000, returns: 0.61, epsilon: 0.47\n",
      "regression loss:  11598.1\n",
      "regression loss:  26275.88\n",
      "episode: 1497/1000000, returns: -8.2, epsilon: 0.47\n",
      "regression loss:  30539.805\n",
      "regression loss:  17988.863\n",
      "episode: 1498/1000000, returns: -3.2, epsilon: 0.47\n",
      "regression loss:  12122.747\n",
      "regression loss:  23651.426\n",
      "episode: 1499/1000000, returns: 3.5, epsilon: 0.47\n",
      "regression loss:  3643.8496\n",
      "regression loss:  17619.914\n",
      "episode: 1500/1000000, returns: 0.019, epsilon: 0.47\n",
      "regression loss:  3499.6777\n",
      "regression loss:  6329.3945\n",
      "episode: 1501/1000000, returns: 2.3, epsilon: 0.47\n",
      "regression loss:  2468.2913\n",
      "regression loss:  4439.713\n",
      "episode: 1502/1000000, returns: -6.3, epsilon: 0.47\n",
      "regression loss:  2280.0684\n",
      "regression loss:  3542.4746\n",
      "episode: 1503/1000000, returns: -4.7, epsilon: 0.47\n",
      "regression loss:  1773.4849\n",
      "regression loss:  3585.8496\n",
      "episode: 1504/1000000, returns: 4.2, epsilon: 0.47\n",
      "regression loss:  842.5189\n",
      "regression loss:  3464.408\n",
      "episode: 1505/1000000, returns: 0.77, epsilon: 0.47\n",
      "regression loss:  293.39465\n",
      "regression loss:  7031.9136\n",
      "episode: 1506/1000000, returns: -2.5, epsilon: 0.47\n",
      "regression loss:  248.8295\n",
      "regression loss:  1716.057\n",
      "episode: 1507/1000000, returns: -0.53, epsilon: 0.47\n",
      "regression loss:  1107.2172\n",
      "regression loss:  668.88983\n",
      "episode: 1508/1000000, returns: 0.17, epsilon: 0.47\n",
      "regression loss:  336.05682\n",
      "regression loss:  680.4066\n",
      "episode: 1509/1000000, returns: 3.3, epsilon: 0.47\n",
      "regression loss:  58.395332\n",
      "regression loss:  799.01404\n",
      "episode: 1510/1000000, returns: 4.2, epsilon: 0.47\n",
      "regression loss:  468.4149\n",
      "regression loss:  527.84735\n",
      "episode: 1511/1000000, returns: 0.61, epsilon: 0.47\n",
      "regression loss:  205.89917\n",
      "regression loss:  2088.9897\n",
      "episode: 1512/1000000, returns: -0.15, epsilon: 0.47\n",
      "regression loss:  1352.544\n",
      "regression loss:  994.184\n",
      "episode: 1513/1000000, returns: 4.5, epsilon: 0.47\n",
      "regression loss:  373.54483\n",
      "regression loss:  1894.003\n",
      "episode: 1514/1000000, returns: -4.7, epsilon: 0.47\n",
      "regression loss:  2091.4834\n",
      "regression loss:  1149.826\n",
      "episode: 1515/1000000, returns: 7.1, epsilon: 0.47\n",
      "regression loss:  1640.9341\n",
      "regression loss:  3172.1995\n",
      "episode: 1516/1000000, returns: 7.1, epsilon: 0.47\n",
      "regression loss:  1143.955\n",
      "regression loss:  1947.25\n",
      "episode: 1517/1000000, returns: -4.1, epsilon: 0.47\n",
      "regression loss:  6459.003\n",
      "regression loss:  6603.5796\n",
      "episode: 1518/1000000, returns: 2.5, epsilon: 0.47\n",
      "regression loss:  446.1643\n",
      "regression loss:  4965.317\n",
      "episode: 1519/1000000, returns: 0.32, epsilon: 0.47\n",
      "regression loss:  10771.617\n",
      "regression loss:  9706.639\n",
      "episode: 1520/1000000, returns: 0.93, epsilon: 0.47\n",
      "regression loss:  715.2375\n",
      "regression loss:  8654.381\n",
      "episode: 1521/1000000, returns: -3.5, epsilon: 0.47\n",
      "regression loss:  13258.782\n",
      "regression loss:  14793.59\n",
      "episode: 1522/1000000, returns: 4.7, epsilon: 0.47\n",
      "regression loss:  6834.9375\n",
      "regression loss:  3683.1187\n",
      "episode: 1523/1000000, returns: -0.57, epsilon: 0.47\n",
      "regression loss:  2191.9\n",
      "regression loss:  8000.6416\n",
      "episode: 1524/1000000, returns: 1.3, epsilon: 0.47\n",
      "regression loss:  3560.7566\n",
      "regression loss:  6990.778\n",
      "episode: 1525/1000000, returns: 1.5, epsilon: 0.47\n",
      "regression loss:  1947.4001\n",
      "regression loss:  5284.4907\n",
      "episode: 1526/1000000, returns: 0.95, epsilon: 0.47\n",
      "regression loss:  2012.3663\n",
      "regression loss:  4308.9297\n",
      "episode: 1527/1000000, returns: -4.0, epsilon: 0.47\n",
      "regression loss:  1590.6091\n",
      "regression loss:  6503.6025\n",
      "episode: 1528/1000000, returns: -4.3, epsilon: 0.47\n",
      "regression loss:  1410.1013\n",
      "regression loss:  4140.555\n",
      "episode: 1529/1000000, returns: 0.38, epsilon: 0.47\n",
      "regression loss:  766.0285\n",
      "regression loss:  2791.0422\n",
      "episode: 1530/1000000, returns: 6.9, epsilon: 0.47\n",
      "regression loss:  1904.0154\n",
      "regression loss:  3444.15\n",
      "episode: 1531/1000000, returns: 3.6, epsilon: 0.47\n",
      "regression loss:  507.68997\n",
      "regression loss:  2095.4004\n",
      "episode: 1532/1000000, returns: 0.64, epsilon: 0.46\n",
      "regression loss:  770.3057\n",
      "regression loss:  1229.7869\n",
      "episode: 1533/1000000, returns: 1.4, epsilon: 0.46\n",
      "regression loss:  814.68243\n",
      "regression loss:  1419.1339\n",
      "episode: 1534/1000000, returns: 5.4, epsilon: 0.46\n",
      "regression loss:  1133.89\n",
      "regression loss:  2782.894\n",
      "episode: 1535/1000000, returns: -1e+01, epsilon: 0.46\n",
      "regression loss:  146.4468\n",
      "regression loss:  1366.844\n",
      "episode: 1536/1000000, returns: -1.7, epsilon: 0.46\n",
      "regression loss:  112.56478\n",
      "regression loss:  1725.3108\n",
      "episode: 1537/1000000, returns: -1.7, epsilon: 0.46\n",
      "regression loss:  458.43616\n",
      "regression loss:  522.56836\n",
      "episode: 1538/1000000, returns: -0.3, epsilon: 0.46\n",
      "regression loss:  381.15314\n",
      "regression loss:  1732.5483\n",
      "episode: 1539/1000000, returns: 2.0, epsilon: 0.46\n",
      "regression loss:  4464.632\n",
      "regression loss:  3212.4856\n",
      "episode: 1540/1000000, returns: -3.3, epsilon: 0.46\n",
      "regression loss:  5913.278\n",
      "regression loss:  4531.5786\n",
      "episode: 1541/1000000, returns: -0.57, epsilon: 0.46\n",
      "regression loss:  1792.0496\n",
      "regression loss:  5073.672\n",
      "episode: 1542/1000000, returns: 1.3, epsilon: 0.46\n",
      "regression loss:  6214.229\n",
      "regression loss:  9602.857\n",
      "episode: 1543/1000000, returns: 4.3, epsilon: 0.46\n",
      "regression loss:  1810.7758\n",
      "regression loss:  2062.0112\n",
      "episode: 1544/1000000, returns: 9.1, epsilon: 0.46\n",
      "regression loss:  1546.1177\n",
      "regression loss:  7377.58\n",
      "episode: 1545/1000000, returns: 0.24, epsilon: 0.46\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "regression loss:  4616.09\n",
      "regression loss:  6786.94\n",
      "episode: 1546/1000000, returns: 1.5, epsilon: 0.46\n",
      "regression loss:  8650.552\n",
      "regression loss:  11598.158\n",
      "episode: 1547/1000000, returns: 0.78, epsilon: 0.46\n",
      "regression loss:  917.71484\n",
      "regression loss:  2464.7954\n",
      "episode: 1548/1000000, returns: 0.82, epsilon: 0.46\n",
      "regression loss:  2867.3875\n",
      "regression loss:  4440.4873\n",
      "episode: 1549/1000000, returns: -8.7, epsilon: 0.46\n",
      "regression loss:  462.64334\n",
      "regression loss:  3868.0654\n",
      "episode: 1550/1000000, returns: 1.2, epsilon: 0.46\n",
      "regression loss:  583.1906\n",
      "regression loss:  3213.2954\n",
      "episode: 1551/1000000, returns: 1.2, epsilon: 0.46\n",
      "regression loss:  944.6398\n",
      "regression loss:  4095.7424\n",
      "episode: 1552/1000000, returns: 0.056, epsilon: 0.46\n",
      "regression loss:  1302.8641\n",
      "regression loss:  2686.893\n",
      "episode: 1553/1000000, returns: 0.95, epsilon: 0.46\n",
      "regression loss:  529.078\n",
      "regression loss:  2713.6768\n",
      "episode: 1554/1000000, returns: 2.0, epsilon: 0.46\n",
      "regression loss:  398.13718\n",
      "regression loss:  1079.758\n",
      "episode: 1555/1000000, returns: -4.6, epsilon: 0.46\n",
      "regression loss:  5821.995\n",
      "regression loss:  4778.593\n",
      "episode: 1556/1000000, returns: 9.2, epsilon: 0.46\n",
      "regression loss:  3528.2954\n",
      "regression loss:  1881.3098\n",
      "episode: 1557/1000000, returns: -7.1, epsilon: 0.46\n",
      "regression loss:  3294.2083\n",
      "regression loss:  4003.5686\n",
      "episode: 1558/1000000, returns: 2.7, epsilon: 0.46\n",
      "regression loss:  4142.6577\n",
      "regression loss:  2828.5754\n",
      "episode: 1559/1000000, returns: 1.4, epsilon: 0.46\n",
      "regression loss:  3324.9648\n",
      "regression loss:  4748.056\n",
      "episode: 1560/1000000, returns: -2.2, epsilon: 0.46\n",
      "regression loss:  2306.696\n",
      "regression loss:  4859.603\n",
      "episode: 1561/1000000, returns: 6.1, epsilon: 0.46\n",
      "regression loss:  4535.2437\n",
      "regression loss:  5093.521\n",
      "episode: 1562/1000000, returns: -4.5, epsilon: 0.46\n",
      "regression loss:  7356.346\n",
      "regression loss:  4452.827\n",
      "episode: 1563/1000000, returns: 0.47, epsilon: 0.46\n",
      "regression loss:  1127.1265\n",
      "regression loss:  8676.51\n",
      "episode: 1564/1000000, returns: -8.9, epsilon: 0.46\n",
      "regression loss:  12143.721\n",
      "regression loss:  4485.278\n",
      "episode: 1565/1000000, returns: -1e+01, epsilon: 0.46\n",
      "regression loss:  1730.9749\n",
      "regression loss:  5382.847\n",
      "episode: 1566/1000000, returns: 3.4, epsilon: 0.46\n",
      "regression loss:  4638.722\n",
      "regression loss:  6752.2676\n",
      "episode: 1567/1000000, returns: 1.1, epsilon: 0.46\n",
      "regression loss:  3204.313\n",
      "regression loss:  7091.928\n",
      "episode: 1568/1000000, returns: -3.1, epsilon: 0.46\n",
      "regression loss:  2672.8574\n",
      "regression loss:  5916.8994\n",
      "episode: 1569/1000000, returns: 1e+01, epsilon: 0.46\n",
      "regression loss:  5252.833\n",
      "regression loss:  14295.541\n",
      "episode: 1570/1000000, returns: 1e+01, epsilon: 0.46\n",
      "regression loss:  22248.598\n",
      "regression loss:  14549.57\n",
      "episode: 1571/1000000, returns: -2.7, epsilon: 0.46\n",
      "regression loss:  4343.0605\n",
      "regression loss:  21407.594\n",
      "episode: 1572/1000000, returns: 0.97, epsilon: 0.46\n",
      "regression loss:  18140.633\n",
      "regression loss:  44907.652\n",
      "episode: 1573/1000000, returns: 2.8, epsilon: 0.46\n",
      "regression loss:  46257.28\n",
      "regression loss:  18441.195\n",
      "episode: 1574/1000000, returns: -6.6, epsilon: 0.46\n",
      "regression loss:  13323.976\n",
      "regression loss:  14165.582\n",
      "episode: 1575/1000000, returns: 0.4, epsilon: 0.45\n",
      "regression loss:  7736.2036\n",
      "regression loss:  33358.086\n",
      "episode: 1576/1000000, returns: 4.3, epsilon: 0.45\n",
      "regression loss:  17770.383\n",
      "regression loss:  26416.402\n",
      "episode: 1577/1000000, returns: 2.1, epsilon: 0.45\n",
      "regression loss:  2034.3215\n",
      "regression loss:  17673.29\n",
      "episode: 1578/1000000, returns: 5.6, epsilon: 0.45\n",
      "regression loss:  2789.7676\n",
      "regression loss:  9753.457\n",
      "episode: 1579/1000000, returns: 0.41, epsilon: 0.45\n",
      "regression loss:  1296.9015\n",
      "regression loss:  10193.392\n",
      "episode: 1580/1000000, returns: 1.2, epsilon: 0.45\n",
      "regression loss:  807.5324\n",
      "regression loss:  11258.955\n",
      "episode: 1581/1000000, returns: 0.42, epsilon: 0.45\n",
      "regression loss:  731.6202\n",
      "regression loss:  5587.804\n",
      "episode: 1582/1000000, returns: 1.3, epsilon: 0.45\n",
      "regression loss:  621.15826\n",
      "regression loss:  6723.003\n",
      "episode: 1583/1000000, returns: 2.8, epsilon: 0.45\n",
      "regression loss:  500.5091\n",
      "regression loss:  5047.878\n",
      "episode: 1584/1000000, returns: -0.33, epsilon: 0.45\n",
      "regression loss:  470.894\n",
      "regression loss:  1213.3024\n",
      "episode: 1585/1000000, returns: 4.6, epsilon: 0.45\n",
      "regression loss:  552.57086\n",
      "regression loss:  3255.6147\n",
      "episode: 1586/1000000, returns: -2.9, epsilon: 0.45\n",
      "regression loss:  505.08975\n",
      "regression loss:  1072.8506\n",
      "episode: 1587/1000000, returns: 0.8, epsilon: 0.45\n",
      "regression loss:  841.7612\n",
      "regression loss:  1740.1084\n",
      "episode: 1588/1000000, returns: -0.57, epsilon: 0.45\n",
      "regression loss:  1985.3278\n",
      "regression loss:  2631.701\n",
      "episode: 1589/1000000, returns: 2.5, epsilon: 0.45\n",
      "regression loss:  474.92688\n",
      "regression loss:  1204.4991\n",
      "episode: 1590/1000000, returns: -0.37, epsilon: 0.45\n",
      "regression loss:  1207.541\n",
      "regression loss:  2166.1997\n",
      "episode: 1591/1000000, returns: 0.82, epsilon: 0.45\n",
      "regression loss:  286.7333\n",
      "regression loss:  1383.9954\n",
      "episode: 1592/1000000, returns: -6.6, epsilon: 0.45\n",
      "regression loss:  325.12665\n",
      "regression loss:  1469.8167\n",
      "episode: 1593/1000000, returns: 0.45, epsilon: 0.45\n",
      "regression loss:  169.3897\n",
      "regression loss:  1626.5189\n",
      "episode: 1594/1000000, returns: 1.6, epsilon: 0.45\n",
      "regression loss:  1084.6128\n",
      "regression loss:  1550.9175\n",
      "episode: 1595/1000000, returns: -0.043, epsilon: 0.45\n",
      "regression loss:  991.32385\n",
      "regression loss:  6657.8213\n",
      "episode: 1596/1000000, returns: -0.35, epsilon: 0.45\n",
      "regression loss:  2762.5676\n",
      "regression loss:  5965.3027\n",
      "episode: 1597/1000000, returns: -0.94, epsilon: 0.45\n",
      "regression loss:  892.4725\n",
      "regression loss:  1447.907\n",
      "episode: 1598/1000000, returns: -0.81, epsilon: 0.45\n",
      "regression loss:  2078.0513\n",
      "regression loss:  6030.095\n",
      "episode: 1599/1000000, returns: -2.8, epsilon: 0.45\n",
      "regression loss:  516.32886\n",
      "regression loss:  3222.7126\n",
      "episode: 1600/1000000, returns: 1.9, epsilon: 0.45\n",
      "regression loss:  494.50015\n",
      "regression loss:  1667.9053\n",
      "episode: 1601/1000000, returns: 4.7, epsilon: 0.45\n",
      "regression loss:  288.25275\n",
      "regression loss:  2758.3777\n",
      "episode: 1602/1000000, returns: -1.1, epsilon: 0.45\n",
      "regression loss:  204.70882\n",
      "regression loss:  1802.9229\n",
      "episode: 1603/1000000, returns: -0.32, epsilon: 0.45\n",
      "regression loss:  209.66997\n",
      "regression loss:  2446.5327\n",
      "episode: 1604/1000000, returns: -1.2, epsilon: 0.45\n",
      "regression loss:  280.23438\n",
      "regression loss:  1152.7063\n",
      "episode: 1605/1000000, returns: 0.87, epsilon: 0.45\n",
      "regression loss:  480.06113\n",
      "regression loss:  559.2383\n",
      "episode: 1606/1000000, returns: -4.0, epsilon: 0.45\n",
      "regression loss:  4835.927\n",
      "regression loss:  4278.52\n",
      "episode: 1607/1000000, returns: 2.5, epsilon: 0.45\n",
      "regression loss:  4107.0645\n",
      "regression loss:  1635.8987\n",
      "episode: 1608/1000000, returns: 1.2, epsilon: 0.45\n",
      "regression loss:  1782.0153\n",
      "regression loss:  2145.3308\n",
      "episode: 1609/1000000, returns: 0.99, epsilon: 0.45\n",
      "regression loss:  3153.3547\n",
      "regression loss:  1972.0034\n",
      "episode: 1610/1000000, returns: 3.0, epsilon: 0.45\n",
      "regression loss:  5268.9424\n",
      "regression loss:  2656.5688\n",
      "episode: 1611/1000000, returns: 0.57, epsilon: 0.45\n",
      "regression loss:  3932.9326\n",
      "regression loss:  3390.9143\n",
      "episode: 1612/1000000, returns: -5.7, epsilon: 0.45\n",
      "regression loss:  1183.6318\n",
      "regression loss:  1992.924\n",
      "episode: 1613/1000000, returns: -3.0, epsilon: 0.45\n",
      "regression loss:  4569.491\n",
      "regression loss:  8942.749\n",
      "episode: 1614/1000000, returns: 0.57, epsilon: 0.45\n",
      "regression loss:  3100.375\n",
      "regression loss:  1127.3604\n",
      "episode: 1615/1000000, returns: 9.6, epsilon: 0.45\n",
      "regression loss:  746.70215\n",
      "regression loss:  2423.1038\n",
      "episode: 1616/1000000, returns: 3.1, epsilon: 0.45\n",
      "regression loss:  766.547\n",
      "regression loss:  1583.9498\n",
      "episode: 1617/1000000, returns: -0.51, epsilon: 0.45\n",
      "regression loss:  744.2985\n",
      "regression loss:  1983.4421\n",
      "episode: 1618/1000000, returns: -0.086, epsilon: 0.45\n",
      "regression loss:  137.40285\n",
      "regression loss:  6194.172\n",
      "episode: 1619/1000000, returns: -0.61, epsilon: 0.44\n",
      "regression loss:  4878.6055\n",
      "regression loss:  4819.504\n",
      "episode: 1620/1000000, returns: -3.0, epsilon: 0.44\n",
      "regression loss:  291.3096\n",
      "regression loss:  4089.5254\n",
      "episode: 1621/1000000, returns: 4.5, epsilon: 0.44\n",
      "regression loss:  2544.8972\n",
      "regression loss:  3822.9365\n",
      "episode: 1622/1000000, returns: 0.19, epsilon: 0.44\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "regression loss:  8415.656\n",
      "regression loss:  4004.57\n",
      "episode: 1623/1000000, returns: 0.14, epsilon: 0.44\n",
      "regression loss:  3779.4468\n",
      "regression loss:  17500.152\n",
      "episode: 1624/1000000, returns: 3.1, epsilon: 0.44\n",
      "regression loss:  8832.038\n",
      "regression loss:  4845.49\n",
      "episode: 1625/1000000, returns: -2.5, epsilon: 0.44\n",
      "regression loss:  2596.9316\n",
      "regression loss:  5038.1934\n",
      "episode: 1626/1000000, returns: 4.9, epsilon: 0.44\n",
      "regression loss:  54959.945\n",
      "regression loss:  42073.95\n",
      "episode: 1627/1000000, returns: -2.4, epsilon: 0.44\n",
      "regression loss:  53352.13\n",
      "regression loss:  22789.457\n",
      "episode: 1628/1000000, returns: -0.89, epsilon: 0.44\n",
      "regression loss:  2638.1152\n",
      "regression loss:  14805.7\n",
      "episode: 1629/1000000, returns: -5.2, epsilon: 0.44\n",
      "regression loss:  21089.43\n",
      "regression loss:  57358.996\n",
      "episode: 1630/1000000, returns: 0.5, epsilon: 0.44\n",
      "regression loss:  35429.51\n",
      "regression loss:  16807.24\n",
      "episode: 1631/1000000, returns: -6.7, epsilon: 0.44\n",
      "regression loss:  4100.092\n",
      "regression loss:  13301.514\n",
      "episode: 1632/1000000, returns: 2.1, epsilon: 0.44\n",
      "regression loss:  8027.2305\n",
      "regression loss:  27078.574\n",
      "episode: 1633/1000000, returns: 0.017, epsilon: 0.44\n",
      "regression loss:  29106.848\n",
      "regression loss:  30465.738\n",
      "episode: 1634/1000000, returns: -0.27, epsilon: 0.44\n",
      "regression loss:  5743.9688\n",
      "regression loss:  23975.355\n",
      "episode: 1635/1000000, returns: -4.8, epsilon: 0.44\n",
      "regression loss:  22794.043\n",
      "regression loss:  42442.67\n",
      "episode: 1636/1000000, returns: 1.2, epsilon: 0.44\n",
      "regression loss:  32639.258\n",
      "regression loss:  31076.557\n",
      "episode: 1637/1000000, returns: 2.1, epsilon: 0.44\n",
      "regression loss:  5641.027\n",
      "regression loss:  10027.777\n",
      "episode: 1638/1000000, returns: 2.1, epsilon: 0.44\n",
      "regression loss:  10988.417\n",
      "regression loss:  30950.004\n",
      "episode: 1639/1000000, returns: 0.67, epsilon: 0.44\n",
      "regression loss:  21864.592\n",
      "regression loss:  28005.35\n",
      "episode: 1640/1000000, returns: -1.2, epsilon: 0.44\n",
      "regression loss:  6105.5005\n",
      "regression loss:  23552.363\n",
      "episode: 1641/1000000, returns: 0.41, epsilon: 0.44\n",
      "regression loss:  15781.896\n",
      "regression loss:  33601.414\n",
      "episode: 1642/1000000, returns: 2.5e+01, epsilon: 0.44\n",
      "regression loss:  44438.387\n",
      "regression loss:  33626.83\n",
      "episode: 1643/1000000, returns: -0.93, epsilon: 0.44\n",
      "regression loss:  13475.02\n",
      "regression loss:  9356.228\n",
      "episode: 1644/1000000, returns: -2.4, epsilon: 0.44\n",
      "regression loss:  2932.6707\n",
      "regression loss:  14730.369\n",
      "episode: 1645/1000000, returns: -6.7, epsilon: 0.44\n",
      "regression loss:  2266.0789\n",
      "regression loss:  8536.839\n",
      "episode: 1646/1000000, returns: 1.4, epsilon: 0.44\n",
      "regression loss:  1456.1364\n",
      "regression loss:  14695.857\n",
      "episode: 1647/1000000, returns: 1.0, epsilon: 0.44\n",
      "regression loss:  1732.3768\n",
      "regression loss:  8011.3306\n",
      "episode: 1648/1000000, returns: -1.2, epsilon: 0.44\n",
      "regression loss:  2934.8647\n",
      "regression loss:  8548.426\n",
      "episode: 1649/1000000, returns: 0.71, epsilon: 0.44\n",
      "regression loss:  972.27167\n",
      "regression loss:  4642.408\n",
      "episode: 1650/1000000, returns: -4.6, epsilon: 0.44\n",
      "regression loss:  15655.414\n",
      "regression loss:  10180.585\n",
      "episode: 1651/1000000, returns: -0.2, epsilon: 0.44\n",
      "regression loss:  2315.1772\n",
      "regression loss:  4165.6885\n",
      "episode: 1652/1000000, returns: 0.77, epsilon: 0.44\n",
      "regression loss:  5220.0586\n",
      "regression loss:  8589.678\n",
      "episode: 1653/1000000, returns: -5.4, epsilon: 0.44\n",
      "regression loss:  10961.993\n",
      "regression loss:  8105.218\n",
      "episode: 1654/1000000, returns: 4.8, epsilon: 0.44\n",
      "regression loss:  576.7247\n",
      "regression loss:  2947.2847\n",
      "episode: 1655/1000000, returns: 0.16, epsilon: 0.44\n",
      "regression loss:  8824.539\n",
      "regression loss:  12036.826\n",
      "episode: 1656/1000000, returns: 6.6, epsilon: 0.44\n",
      "regression loss:  2326.2969\n",
      "regression loss:  13104.566\n",
      "episode: 1657/1000000, returns: 1.9, epsilon: 0.44\n",
      "regression loss:  3560.1113\n",
      "regression loss:  3718.5405\n",
      "episode: 1658/1000000, returns: 0.24, epsilon: 0.44\n",
      "regression loss:  3303.6582\n",
      "regression loss:  6229.005\n",
      "episode: 1659/1000000, returns: 5.0, epsilon: 0.44\n",
      "regression loss:  1824.392\n",
      "regression loss:  2392.0737\n",
      "episode: 1660/1000000, returns: -1.7, epsilon: 0.44\n",
      "regression loss:  972.0018\n",
      "regression loss:  3766.9744\n",
      "episode: 1661/1000000, returns: 3.0, epsilon: 0.44\n",
      "regression loss:  530.2392\n",
      "regression loss:  1988.1901\n",
      "episode: 1662/1000000, returns: 0.25, epsilon: 0.44\n",
      "regression loss:  681.3972\n",
      "regression loss:  1761.9521\n",
      "episode: 1663/1000000, returns: 7.6, epsilon: 0.44\n",
      "regression loss:  424.41107\n",
      "regression loss:  706.61237\n",
      "episode: 1664/1000000, returns: 0.15, epsilon: 0.44\n",
      "regression loss:  924.241\n",
      "regression loss:  1458.4363\n",
      "episode: 1665/1000000, returns: -5.9, epsilon: 0.43\n",
      "regression loss:  552.336\n",
      "regression loss:  1888.4634\n",
      "episode: 1666/1000000, returns: -8.2, epsilon: 0.43\n",
      "regression loss:  242.29582\n",
      "regression loss:  1017.21533\n",
      "episode: 1667/1000000, returns: -4.9, epsilon: 0.43\n",
      "regression loss:  302.25256\n",
      "regression loss:  508.50507\n",
      "episode: 1668/1000000, returns: -0.77, epsilon: 0.43\n",
      "regression loss:  1094.3707\n",
      "regression loss:  1703.5302\n",
      "episode: 1669/1000000, returns: -2.0, epsilon: 0.43\n",
      "regression loss:  1699.5845\n",
      "regression loss:  638.16925\n",
      "episode: 1670/1000000, returns: -6.3, epsilon: 0.43\n",
      "regression loss:  4966.2266\n",
      "regression loss:  4964.0137\n",
      "episode: 1671/1000000, returns: -1.0, epsilon: 0.43\n",
      "regression loss:  865.4451\n",
      "regression loss:  3987.1685\n",
      "episode: 1672/1000000, returns: 1.4, epsilon: 0.43\n",
      "regression loss:  6598.5015\n",
      "regression loss:  14355.81\n",
      "episode: 1673/1000000, returns: 5.6, epsilon: 0.43\n",
      "regression loss:  3604.3665\n",
      "regression loss:  2651.6099\n",
      "episode: 1674/1000000, returns: 2.1, epsilon: 0.43\n",
      "regression loss:  7744.648\n",
      "regression loss:  19873.371\n",
      "episode: 1675/1000000, returns: -0.49, epsilon: 0.43\n",
      "regression loss:  4065.6184\n",
      "regression loss:  12451.874\n",
      "episode: 1676/1000000, returns: -0.28, epsilon: 0.43\n",
      "regression loss:  2621.8708\n",
      "regression loss:  7877.3535\n",
      "episode: 1677/1000000, returns: -2.7, epsilon: 0.43\n",
      "regression loss:  4314.4004\n",
      "regression loss:  3561.0176\n",
      "episode: 1678/1000000, returns: 0.47, epsilon: 0.43\n",
      "regression loss:  566.4897\n",
      "regression loss:  9888.738\n",
      "episode: 1679/1000000, returns: 3.2, epsilon: 0.43\n",
      "regression loss:  4892.1406\n",
      "regression loss:  9410.604\n",
      "episode: 1680/1000000, returns: 1.1, epsilon: 0.43\n",
      "regression loss:  688.8357\n",
      "regression loss:  6080.241\n",
      "episode: 1681/1000000, returns: -2.4, epsilon: 0.43\n",
      "regression loss:  1747.9346\n",
      "regression loss:  4154.33\n",
      "episode: 1682/1000000, returns: -1.1e+01, epsilon: 0.43\n",
      "regression loss:  550.46716\n",
      "regression loss:  4620.661\n",
      "episode: 1683/1000000, returns: 1.2, epsilon: 0.43\n",
      "regression loss:  2791.1057\n",
      "regression loss:  4197.948\n",
      "episode: 1684/1000000, returns: -0.057, epsilon: 0.43\n",
      "regression loss:  432.77692\n",
      "regression loss:  3273.0718\n",
      "episode: 1685/1000000, returns: 2.4, epsilon: 0.43\n",
      "regression loss:  544.966\n",
      "regression loss:  3599.199\n",
      "episode: 1686/1000000, returns: 1.1e+01, epsilon: 0.43\n",
      "regression loss:  833.9917\n",
      "regression loss:  2192.3267\n",
      "episode: 1687/1000000, returns: -4.2, epsilon: 0.43\n",
      "regression loss:  2022.6754\n",
      "regression loss:  4440.6885\n",
      "episode: 1688/1000000, returns: -7.6, epsilon: 0.43\n",
      "regression loss:  764.39136\n",
      "regression loss:  1717.3845\n",
      "episode: 1689/1000000, returns: 2.1, epsilon: 0.43\n",
      "regression loss:  156.28757\n",
      "regression loss:  1373.2615\n",
      "episode: 1690/1000000, returns: -2.2, epsilon: 0.43\n",
      "regression loss:  516.95355\n",
      "regression loss:  1783.3257\n",
      "episode: 1691/1000000, returns: 3.7, epsilon: 0.43\n",
      "regression loss:  496.00192\n",
      "regression loss:  2846.7854\n",
      "episode: 1692/1000000, returns: -0.6, epsilon: 0.43\n",
      "regression loss:  318.06656\n",
      "regression loss:  1767.0322\n",
      "episode: 1693/1000000, returns: 4.6, epsilon: 0.43\n",
      "regression loss:  2136.356\n",
      "regression loss:  4127.153\n",
      "episode: 1694/1000000, returns: 0.72, epsilon: 0.43\n",
      "regression loss:  207.52174\n",
      "regression loss:  5789.509\n",
      "episode: 1695/1000000, returns: 3.4, epsilon: 0.43\n",
      "regression loss:  2181.6807\n",
      "regression loss:  2062.2007\n",
      "episode: 1696/1000000, returns: 2.8, epsilon: 0.43\n",
      "regression loss:  646.8015\n",
      "regression loss:  3038.0962\n",
      "episode: 1697/1000000, returns: -3.3, epsilon: 0.43\n",
      "regression loss:  1074.3953\n",
      "regression loss:  2010.405\n",
      "episode: 1698/1000000, returns: 1.9, epsilon: 0.43\n",
      "regression loss:  805.47485\n",
      "regression loss:  2202.4695\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 1699/1000000, returns: -2.1, epsilon: 0.43\n",
      "regression loss:  142.69046\n",
      "regression loss:  1225.2114\n",
      "episode: 1700/1000000, returns: 0.77, epsilon: 0.43\n",
      "regression loss:  125.21485\n",
      "regression loss:  2601.7488\n",
      "episode: 1701/1000000, returns: 0.22, epsilon: 0.43\n",
      "regression loss:  827.84045\n",
      "regression loss:  1846.0779\n",
      "episode: 1702/1000000, returns: -6.6, epsilon: 0.43\n",
      "regression loss:  3595.1736\n",
      "regression loss:  3381.344\n",
      "episode: 1703/1000000, returns: 2.2, epsilon: 0.43\n",
      "regression loss:  618.0843\n",
      "regression loss:  1528.2379\n",
      "episode: 1704/1000000, returns: -1.6, epsilon: 0.43\n",
      "regression loss:  2014.6337\n",
      "regression loss:  2385.7798\n",
      "episode: 1705/1000000, returns: 0.36, epsilon: 0.43\n",
      "regression loss:  1606.6843\n",
      "regression loss:  4865.2266\n",
      "episode: 1706/1000000, returns: 4.2, epsilon: 0.43\n",
      "regression loss:  4406.416\n",
      "regression loss:  7918.33\n",
      "episode: 1707/1000000, returns: 4.1, epsilon: 0.43\n",
      "regression loss:  2317.0532\n",
      "regression loss:  5467.7666\n",
      "episode: 1708/1000000, returns: -0.69, epsilon: 0.43\n",
      "regression loss:  21925.555\n",
      "regression loss:  16777.068\n",
      "episode: 1709/1000000, returns: -3.4, epsilon: 0.43\n",
      "regression loss:  8021.9614\n",
      "regression loss:  6089.817\n",
      "episode: 1710/1000000, returns: 0.92, epsilon: 0.43\n",
      "regression loss:  476.91965\n",
      "regression loss:  8036.8555\n",
      "episode: 1711/1000000, returns: 1.2, epsilon: 0.42\n",
      "regression loss:  1720.5509\n",
      "regression loss:  7696.312\n",
      "episode: 1712/1000000, returns: -0.39, epsilon: 0.42\n",
      "regression loss:  2982.3635\n",
      "regression loss:  3168.0137\n",
      "episode: 1713/1000000, returns: 1.9, epsilon: 0.42\n",
      "regression loss:  1257.1548\n",
      "regression loss:  6072.8735\n",
      "episode: 1714/1000000, returns: -0.6, epsilon: 0.42\n",
      "regression loss:  3127.0098\n",
      "regression loss:  3474.7295\n",
      "episode: 1715/1000000, returns: 2.0, epsilon: 0.42\n",
      "regression loss:  13433.737\n",
      "regression loss:  17935.078\n",
      "episode: 1716/1000000, returns: -0.088, epsilon: 0.42\n",
      "regression loss:  18798.203\n",
      "regression loss:  9473.565\n",
      "episode: 1717/1000000, returns: -1.3, epsilon: 0.42\n",
      "regression loss:  1034.1436\n",
      "regression loss:  8913.355\n",
      "episode: 1718/1000000, returns: -3.1, epsilon: 0.42\n",
      "regression loss:  1003.8814\n",
      "regression loss:  2500.7046\n",
      "episode: 1719/1000000, returns: 2.5, epsilon: 0.42\n",
      "regression loss:  2553.0195\n",
      "regression loss:  2800.3604\n",
      "episode: 1720/1000000, returns: -3.4, epsilon: 0.42\n",
      "regression loss:  3686.7349\n",
      "regression loss:  5715.6777\n",
      "episode: 1721/1000000, returns: 0.21, epsilon: 0.42\n",
      "regression loss:  528.81396\n",
      "regression loss:  1549.6611\n",
      "episode: 1722/1000000, returns: -0.015, epsilon: 0.42\n",
      "regression loss:  363.64456\n",
      "regression loss:  3008.752\n",
      "episode: 1723/1000000, returns: 3.2, epsilon: 0.42\n",
      "regression loss:  283.58505\n",
      "regression loss:  1022.783\n",
      "episode: 1724/1000000, returns: 0.22, epsilon: 0.42\n",
      "regression loss:  3771.981\n",
      "regression loss:  3830.1343\n",
      "episode: 1725/1000000, returns: 0.12, epsilon: 0.42\n",
      "regression loss:  1040.1226\n",
      "regression loss:  10850.37\n",
      "episode: 1726/1000000, returns: -0.048, epsilon: 0.42\n",
      "regression loss:  3219.9536\n",
      "regression loss:  2660.4924\n",
      "episode: 1727/1000000, returns: 9.5, epsilon: 0.42\n",
      "regression loss:  3128.76\n",
      "regression loss:  2031.7662\n",
      "episode: 1728/1000000, returns: 5.5, epsilon: 0.42\n",
      "regression loss:  5022.8896\n",
      "regression loss:  2473.169\n",
      "episode: 1729/1000000, returns: -1.4, epsilon: 0.42\n",
      "regression loss:  370.82114\n",
      "regression loss:  6080.9355\n",
      "episode: 1730/1000000, returns: -0.63, epsilon: 0.42\n",
      "regression loss:  3788.7417\n",
      "regression loss:  3297.4255\n",
      "episode: 1731/1000000, returns: 2.0, epsilon: 0.42\n",
      "regression loss:  668.2271\n",
      "regression loss:  4409.205\n",
      "episode: 1732/1000000, returns: 0.95, epsilon: 0.42\n",
      "regression loss:  9584.271\n",
      "regression loss:  9882.292\n",
      "episode: 1733/1000000, returns: -7.3, epsilon: 0.42\n",
      "regression loss:  10998.516\n",
      "regression loss:  6870.9873\n",
      "episode: 1734/1000000, returns: 3.7, epsilon: 0.42\n",
      "regression loss:  617.1221\n",
      "regression loss:  4397.384\n",
      "episode: 1735/1000000, returns: 0.64, epsilon: 0.42\n",
      "regression loss:  757.9868\n",
      "regression loss:  4248.158\n",
      "episode: 1736/1000000, returns: -1.0, epsilon: 0.42\n",
      "regression loss:  2103.123\n",
      "regression loss:  2028.1996\n",
      "episode: 1737/1000000, returns: 5.6, epsilon: 0.42\n",
      "regression loss:  315.9287\n",
      "regression loss:  2417.1797\n",
      "episode: 1738/1000000, returns: 3.0, epsilon: 0.42\n",
      "regression loss:  3403.5447\n",
      "regression loss:  1331.0344\n",
      "episode: 1739/1000000, returns: 8.2, epsilon: 0.42\n",
      "regression loss:  640.72363\n",
      "regression loss:  1460.0548\n",
      "episode: 1740/1000000, returns: 0.91, epsilon: 0.42\n",
      "regression loss:  357.70502\n",
      "regression loss:  3473.8188\n",
      "episode: 1741/1000000, returns: 5.7, epsilon: 0.42\n",
      "regression loss:  2022.6907\n",
      "regression loss:  2120.369\n",
      "episode: 1742/1000000, returns: 0.39, epsilon: 0.42\n",
      "regression loss:  506.19666\n",
      "regression loss:  1965.9126\n",
      "episode: 1743/1000000, returns: 2.7, epsilon: 0.42\n",
      "regression loss:  2195.886\n",
      "regression loss:  3958.4365\n",
      "episode: 1744/1000000, returns: -4.6, epsilon: 0.42\n",
      "regression loss:  762.99414\n",
      "regression loss:  2934.8481\n",
      "episode: 1745/1000000, returns: 0.29, epsilon: 0.42\n",
      "regression loss:  3910.7576\n",
      "regression loss:  11389.897\n",
      "episode: 1746/1000000, returns: 1.4, epsilon: 0.42\n",
      "regression loss:  4625.6807\n",
      "regression loss:  4133.417\n",
      "episode: 1747/1000000, returns: 2.1, epsilon: 0.42\n",
      "regression loss:  548.79553\n",
      "regression loss:  4257.606\n",
      "episode: 1748/1000000, returns: -0.4, epsilon: 0.42\n",
      "regression loss:  1032.1608\n",
      "regression loss:  1392.8499\n",
      "episode: 1749/1000000, returns: 5.8, epsilon: 0.42\n",
      "regression loss:  3086.0027\n",
      "regression loss:  6530.214\n",
      "episode: 1750/1000000, returns: 4.7, epsilon: 0.42\n",
      "regression loss:  2555.752\n",
      "regression loss:  6709.2456\n",
      "episode: 1751/1000000, returns: 0.48, epsilon: 0.42\n",
      "regression loss:  6922.1807\n",
      "regression loss:  10247.234\n",
      "episode: 1752/1000000, returns: 0.26, epsilon: 0.42\n",
      "regression loss:  8302.895\n",
      "regression loss:  4154.4478\n",
      "episode: 1753/1000000, returns: -2.9, epsilon: 0.42\n",
      "regression loss:  1019.3852\n",
      "regression loss:  6224.917\n",
      "episode: 1754/1000000, returns: -0.00082, epsilon: 0.42\n",
      "regression loss:  3804.8567\n",
      "regression loss:  2628.2441\n",
      "episode: 1755/1000000, returns: 2.7, epsilon: 0.42\n",
      "regression loss:  795.3611\n",
      "regression loss:  6350.5938\n",
      "episode: 1756/1000000, returns: 0.33, epsilon: 0.42\n",
      "regression loss:  587.76025\n",
      "regression loss:  3963.9744\n",
      "episode: 1757/1000000, returns: 1.0, epsilon: 0.42\n",
      "regression loss:  1233.982\n",
      "regression loss:  5769.292\n",
      "episode: 1758/1000000, returns: -0.39, epsilon: 0.42\n",
      "regression loss:  1542.5828\n",
      "regression loss:  1719.2268\n",
      "episode: 1759/1000000, returns: 2.2, epsilon: 0.41\n",
      "regression loss:  1354.3425\n",
      "regression loss:  2482.775\n",
      "episode: 1760/1000000, returns: 1.7e+01, epsilon: 0.41\n",
      "regression loss:  706.3831\n",
      "regression loss:  2891.043\n",
      "episode: 1761/1000000, returns: 5.0, epsilon: 0.41\n",
      "regression loss:  3535.7231\n",
      "regression loss:  4899.7417\n",
      "episode: 1762/1000000, returns: -0.8, epsilon: 0.41\n",
      "regression loss:  3437.0134\n",
      "regression loss:  16072.846\n",
      "episode: 1763/1000000, returns: -0.17, epsilon: 0.41\n",
      "regression loss:  2427.964\n",
      "regression loss:  10156.944\n",
      "episode: 1764/1000000, returns: 0.1, epsilon: 0.41\n",
      "regression loss:  6561.3657\n",
      "regression loss:  3641.3442\n",
      "episode: 1765/1000000, returns: -1.1e+01, epsilon: 0.41\n",
      "regression loss:  7491.906\n",
      "regression loss:  1795.1356\n",
      "episode: 1766/1000000, returns: -5.5, epsilon: 0.41\n",
      "regression loss:  1152.1584\n",
      "regression loss:  6249.593\n",
      "episode: 1767/1000000, returns: 5.0, epsilon: 0.41\n",
      "regression loss:  1850.7864\n",
      "regression loss:  3763.6812\n",
      "episode: 1768/1000000, returns: -0.046, epsilon: 0.41\n",
      "regression loss:  1311.4302\n",
      "regression loss:  2348.3496\n",
      "episode: 1769/1000000, returns: -0.53, epsilon: 0.41\n",
      "regression loss:  389.63046\n",
      "regression loss:  5949.881\n",
      "episode: 1770/1000000, returns: 0.71, epsilon: 0.41\n",
      "regression loss:  310.26767\n",
      "regression loss:  3761.9385\n",
      "episode: 1771/1000000, returns: -2.0, epsilon: 0.41\n",
      "regression loss:  226.34589\n",
      "regression loss:  3365.1895\n",
      "episode: 1772/1000000, returns: 6.8, epsilon: 0.41\n",
      "regression loss:  1325.2184\n",
      "regression loss:  1798.0917\n",
      "episode: 1773/1000000, returns: -1.6, epsilon: 0.41\n",
      "regression loss:  615.08356\n",
      "regression loss:  592.89246\n",
      "episode: 1774/1000000, returns: -0.3, epsilon: 0.41\n",
      "regression loss:  924.8388\n",
      "regression loss:  2501.2134\n",
      "episode: 1775/1000000, returns: -3.3, epsilon: 0.41\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "regression loss:  350.27048\n",
      "regression loss:  1675.3545\n",
      "episode: 1776/1000000, returns: 0.53, epsilon: 0.41\n",
      "regression loss:  845.8643\n",
      "regression loss:  1075.5663\n",
      "episode: 1777/1000000, returns: 2.4, epsilon: 0.41\n",
      "regression loss:  388.20993\n",
      "regression loss:  950.4425\n",
      "episode: 1778/1000000, returns: -2.3, epsilon: 0.41\n",
      "regression loss:  1768.1144\n",
      "regression loss:  2295.131\n",
      "episode: 1779/1000000, returns: -6.2, epsilon: 0.41\n",
      "regression loss:  293.15405\n",
      "regression loss:  285.47433\n",
      "episode: 1780/1000000, returns: -0.12, epsilon: 0.41\n",
      "regression loss:  1603.8651\n",
      "regression loss:  1182.1569\n",
      "episode: 1781/1000000, returns: -2.3, epsilon: 0.41\n",
      "regression loss:  230.67435\n",
      "regression loss:  113.47891\n",
      "episode: 1782/1000000, returns: -3.1, epsilon: 0.41\n",
      "regression loss:  26.365229\n",
      "regression loss:  78.07523\n",
      "episode: 1783/1000000, returns: 3.3, epsilon: 0.41\n",
      "regression loss:  1240.9246\n",
      "regression loss:  1051.8575\n",
      "episode: 1784/1000000, returns: -0.9, epsilon: 0.41\n",
      "regression loss:  376.6068\n",
      "regression loss:  204.16682\n",
      "episode: 1785/1000000, returns: 1.4, epsilon: 0.41\n",
      "regression loss:  188.5116\n",
      "regression loss:  395.90973\n",
      "episode: 1786/1000000, returns: 1.4, epsilon: 0.41\n",
      "regression loss:  75.721664\n",
      "regression loss:  227.44797\n",
      "episode: 1787/1000000, returns: 2.9, epsilon: 0.41\n",
      "regression loss:  191.66783\n",
      "regression loss:  718.65106\n",
      "episode: 1788/1000000, returns: 3.4, epsilon: 0.41\n",
      "regression loss:  632.02155\n",
      "regression loss:  774.88464\n",
      "episode: 1789/1000000, returns: 1.2, epsilon: 0.41\n",
      "regression loss:  13.962604\n",
      "regression loss:  99.52074\n",
      "episode: 1790/1000000, returns: -1.6e+01, epsilon: 0.41\n",
      "regression loss:  603.4293\n",
      "regression loss:  2361.8523\n",
      "episode: 1791/1000000, returns: 4.4, epsilon: 0.41\n",
      "regression loss:  7568.1313\n",
      "regression loss:  7895.4824\n",
      "episode: 1792/1000000, returns: 1.9, epsilon: 0.41\n",
      "regression loss:  10667.259\n",
      "regression loss:  3821.6143\n",
      "episode: 1793/1000000, returns: 1.1, epsilon: 0.41\n",
      "regression loss:  539.91406\n",
      "regression loss:  1872.2312\n",
      "episode: 1794/1000000, returns: 2.4, epsilon: 0.41\n",
      "regression loss:  1453.7932\n",
      "regression loss:  2666.7932\n",
      "episode: 1795/1000000, returns: 8.0, epsilon: 0.41\n",
      "regression loss:  3703.8604\n",
      "regression loss:  2281.6697\n",
      "episode: 1796/1000000, returns: 0.31, epsilon: 0.41\n",
      "regression loss:  4200.2163\n",
      "regression loss:  13546.805\n",
      "episode: 1797/1000000, returns: 0.25, epsilon: 0.41\n",
      "regression loss:  5613.5713\n",
      "regression loss:  8017.881\n",
      "episode: 1798/1000000, returns: 3.0, epsilon: 0.41\n",
      "regression loss:  2136.3774\n",
      "regression loss:  10932.057\n",
      "episode: 1799/1000000, returns: -1.3, epsilon: 0.41\n",
      "regression loss:  3747.934\n",
      "regression loss:  4524.788\n",
      "episode: 1800/1000000, returns: -2.8, epsilon: 0.41\n",
      "regression loss:  777.7946\n",
      "regression loss:  5777.754\n",
      "episode: 1801/1000000, returns: -7.8, epsilon: 0.41\n",
      "regression loss:  2814.594\n",
      "regression loss:  6224.1914\n",
      "episode: 1802/1000000, returns: 3.0, epsilon: 0.41\n",
      "regression loss:  1697.3164\n",
      "regression loss:  1602.241\n",
      "episode: 1803/1000000, returns: 2.0, epsilon: 0.41\n",
      "regression loss:  3947.3484\n",
      "regression loss:  4473.2954\n",
      "episode: 1804/1000000, returns: 4.0, epsilon: 0.41\n",
      "regression loss:  7326.5557\n",
      "regression loss:  6558.948\n",
      "episode: 1805/1000000, returns: -2.5, epsilon: 0.41\n",
      "regression loss:  821.38837\n",
      "regression loss:  7372.301\n",
      "episode: 1806/1000000, returns: 0.021, epsilon: 0.41\n",
      "regression loss:  23304.25\n",
      "regression loss:  19627.725\n",
      "episode: 1807/1000000, returns: 2.4, epsilon: 0.41\n",
      "regression loss:  16968.908\n",
      "regression loss:  19608.867\n",
      "episode: 1808/1000000, returns: 1.6, epsilon: 0.4\n",
      "regression loss:  6389.9253\n",
      "regression loss:  9773.017\n",
      "episode: 1809/1000000, returns: 4.9, epsilon: 0.4\n",
      "regression loss:  23954.152\n",
      "regression loss:  38943.97\n",
      "episode: 1810/1000000, returns: -0.88, epsilon: 0.4\n",
      "regression loss:  12697.942\n",
      "regression loss:  18133.834\n",
      "episode: 1811/1000000, returns: 0.57, epsilon: 0.4\n",
      "regression loss:  3711.5957\n",
      "regression loss:  14684.739\n",
      "episode: 1812/1000000, returns: -1.2, epsilon: 0.4\n",
      "regression loss:  6022.902\n",
      "regression loss:  10344.561\n",
      "episode: 1813/1000000, returns: -1.7, epsilon: 0.4\n",
      "regression loss:  7310.4316\n",
      "regression loss:  11146.352\n",
      "episode: 1814/1000000, returns: 0.61, epsilon: 0.4\n",
      "regression loss:  5572.153\n",
      "regression loss:  12398.375\n",
      "episode: 1815/1000000, returns: 0.092, epsilon: 0.4\n",
      "regression loss:  1799.7849\n",
      "regression loss:  15268.316\n",
      "episode: 1816/1000000, returns: -0.29, epsilon: 0.4\n",
      "regression loss:  2482.0935\n",
      "regression loss:  4501.6147\n",
      "episode: 1817/1000000, returns: 3.3, epsilon: 0.4\n",
      "regression loss:  1652.0631\n",
      "regression loss:  4682.1094\n",
      "episode: 1818/1000000, returns: -3.3, epsilon: 0.4\n",
      "regression loss:  1346.0618\n",
      "regression loss:  4974.9375\n",
      "episode: 1819/1000000, returns: -3.6, epsilon: 0.4\n",
      "regression loss:  2474.0195\n",
      "regression loss:  3813.0654\n",
      "episode: 1820/1000000, returns: 1.6, epsilon: 0.4\n",
      "regression loss:  401.69232\n",
      "regression loss:  3467.7388\n",
      "episode: 1821/1000000, returns: 1.6, epsilon: 0.4\n",
      "regression loss:  834.78516\n",
      "regression loss:  2447.0178\n",
      "episode: 1822/1000000, returns: 3.5, epsilon: 0.4\n",
      "regression loss:  726.9829\n",
      "regression loss:  2862.3809\n",
      "episode: 1823/1000000, returns: 2.4, epsilon: 0.4\n",
      "regression loss:  376.2892\n",
      "regression loss:  1044.3646\n",
      "episode: 1824/1000000, returns: -0.49, epsilon: 0.4\n",
      "regression loss:  138.1821\n",
      "regression loss:  3852.895\n",
      "episode: 1825/1000000, returns: -1.2, epsilon: 0.4\n",
      "regression loss:  876.6236\n",
      "regression loss:  2707.688\n",
      "episode: 1826/1000000, returns: -1.1, epsilon: 0.4\n",
      "regression loss:  1681.591\n",
      "regression loss:  3423.6257\n",
      "episode: 1827/1000000, returns: -6.1, epsilon: 0.4\n",
      "regression loss:  3835.227\n",
      "regression loss:  2351.6875\n",
      "episode: 1828/1000000, returns: -7.1, epsilon: 0.4\n",
      "regression loss:  145.44437\n",
      "regression loss:  7484.4844\n",
      "episode: 1829/1000000, returns: -0.42, epsilon: 0.4\n",
      "regression loss:  6961.861\n",
      "regression loss:  3665.1807\n",
      "episode: 1830/1000000, returns: 0.38, epsilon: 0.4\n",
      "regression loss:  767.8407\n",
      "regression loss:  1606.3289\n",
      "episode: 1831/1000000, returns: 1.9, epsilon: 0.4\n",
      "regression loss:  2612.3286\n",
      "regression loss:  15966.289\n",
      "episode: 1832/1000000, returns: 6.0, epsilon: 0.4\n",
      "regression loss:  10265.781\n",
      "regression loss:  6618.6343\n",
      "episode: 1833/1000000, returns: 2.9, epsilon: 0.4\n",
      "regression loss:  9203.446\n",
      "regression loss:  5293.9404\n",
      "episode: 1834/1000000, returns: 1.0, epsilon: 0.4\n",
      "regression loss:  3688.167\n",
      "regression loss:  3342.9026\n",
      "episode: 1835/1000000, returns: 7.4, epsilon: 0.4\n",
      "regression loss:  10564.366\n",
      "regression loss:  5379.9844\n",
      "episode: 1836/1000000, returns: -2.6, epsilon: 0.4\n",
      "regression loss:  588.1433\n",
      "regression loss:  3043.597\n",
      "episode: 1837/1000000, returns: -0.14, epsilon: 0.4\n",
      "regression loss:  6108.1885\n",
      "regression loss:  15975.321\n",
      "episode: 1838/1000000, returns: 4.5, epsilon: 0.4\n",
      "regression loss:  9493.231\n",
      "regression loss:  7344.5977\n",
      "episode: 1839/1000000, returns: 0.13, epsilon: 0.4\n",
      "regression loss:  469.38177\n",
      "regression loss:  6075.311\n",
      "episode: 1840/1000000, returns: -0.27, epsilon: 0.4\n",
      "regression loss:  1068.3711\n",
      "regression loss:  1286.9495\n",
      "episode: 1841/1000000, returns: -1.3, epsilon: 0.4\n",
      "regression loss:  1496.7365\n",
      "regression loss:  3516.7068\n",
      "episode: 1842/1000000, returns: 0.47, epsilon: 0.4\n",
      "regression loss:  594.99286\n",
      "regression loss:  4396.6343\n",
      "episode: 1843/1000000, returns: 1.5, epsilon: 0.4\n",
      "regression loss:  858.5525\n",
      "regression loss:  1660.8396\n",
      "episode: 1844/1000000, returns: 4.4, epsilon: 0.4\n",
      "regression loss:  341.7278\n",
      "regression loss:  1602.7412\n",
      "episode: 1845/1000000, returns: -0.25, epsilon: 0.4\n",
      "regression loss:  661.6721\n",
      "regression loss:  1807.9623\n",
      "episode: 1846/1000000, returns: -1.2, epsilon: 0.4\n",
      "regression loss:  2772.6458\n",
      "regression loss:  800.11523\n",
      "episode: 1847/1000000, returns: 3.0, epsilon: 0.4\n",
      "regression loss:  3556.2317\n",
      "regression loss:  4056.0566\n",
      "episode: 1848/1000000, returns: -8.2, epsilon: 0.4\n",
      "regression loss:  4770.788\n",
      "regression loss:  4768.2188\n",
      "episode: 1849/1000000, returns: -0.46, epsilon: 0.4\n",
      "regression loss:  3388.743\n",
      "regression loss:  1361.931\n",
      "episode: 1850/1000000, returns: 1.6, epsilon: 0.4\n",
      "regression loss:  1727.64\n",
      "regression loss:  287.57315\n",
      "episode: 1851/1000000, returns: -2.9e+01, epsilon: 0.4\n",
      "regression loss:  1116.8069\n",
      "regression loss:  4257.717\n",
      "episode: 1852/1000000, returns: 0.38, epsilon: 0.4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "regression loss:  3382.8826\n",
      "regression loss:  5970.5034\n",
      "episode: 1853/1000000, returns: 1.1, epsilon: 0.4\n",
      "regression loss:  1001.5874\n",
      "regression loss:  878.4429\n",
      "episode: 1854/1000000, returns: -0.46, epsilon: 0.4\n",
      "regression loss:  340.65527\n",
      "regression loss:  354.5249\n",
      "episode: 1855/1000000, returns: 0.23, epsilon: 0.4\n",
      "regression loss:  479.69498\n",
      "regression loss:  416.4078\n",
      "episode: 1856/1000000, returns: 3.0, epsilon: 0.4\n",
      "regression loss:  496.6412\n",
      "regression loss:  220.22151\n",
      "episode: 1857/1000000, returns: -1.0, epsilon: 0.4\n",
      "regression loss:  908.0735\n",
      "regression loss:  743.6242\n",
      "episode: 1858/1000000, returns: -1.3, epsilon: 0.39\n",
      "regression loss:  356.61908\n",
      "regression loss:  559.574\n",
      "episode: 1859/1000000, returns: -0.59, epsilon: 0.39\n",
      "regression loss:  381.87195\n",
      "regression loss:  104.62207\n",
      "episode: 1860/1000000, returns: 3.9, epsilon: 0.39\n",
      "regression loss:  47.403366\n",
      "regression loss:  912.8201\n",
      "episode: 1861/1000000, returns: 0.13, epsilon: 0.39\n",
      "regression loss:  1498.0837\n",
      "regression loss:  337.27454\n",
      "episode: 1862/1000000, returns: 1.3, epsilon: 0.39\n",
      "regression loss:  815.3304\n",
      "regression loss:  423.20804\n",
      "episode: 1863/1000000, returns: 0.82, epsilon: 0.39\n",
      "regression loss:  817.54974\n",
      "regression loss:  2690.54\n",
      "episode: 1864/1000000, returns: 1.4, epsilon: 0.39\n",
      "regression loss:  705.61633\n",
      "regression loss:  746.75525\n",
      "episode: 1865/1000000, returns: -0.63, epsilon: 0.39\n",
      "regression loss:  620.7528\n",
      "regression loss:  2166.9978\n",
      "episode: 1866/1000000, returns: 0.29, epsilon: 0.39\n",
      "regression loss:  1729.7461\n",
      "regression loss:  692.1496\n",
      "episode: 1867/1000000, returns: 2.0, epsilon: 0.39\n",
      "regression loss:  142.08821\n",
      "regression loss:  1077.4292\n",
      "episode: 1868/1000000, returns: 1.2e+01, epsilon: 0.39\n",
      "regression loss:  1023.1651\n",
      "regression loss:  1790.949\n",
      "episode: 1869/1000000, returns: -9.3, epsilon: 0.39\n",
      "regression loss:  1379.828\n",
      "regression loss:  1237.7676\n",
      "episode: 1870/1000000, returns: 2.3, epsilon: 0.39\n",
      "regression loss:  2774.9004\n",
      "regression loss:  1746.1636\n",
      "episode: 1871/1000000, returns: 1.2e+01, epsilon: 0.39\n",
      "regression loss:  771.86206\n",
      "regression loss:  1914.1146\n",
      "episode: 1872/1000000, returns: 0.4, epsilon: 0.39\n",
      "regression loss:  2877.1438\n",
      "regression loss:  1394.2815\n",
      "episode: 1873/1000000, returns: 7.3, epsilon: 0.39\n",
      "regression loss:  33657.85\n",
      "regression loss:  9859.473\n",
      "episode: 1874/1000000, returns: 0.33, epsilon: 0.39\n",
      "regression loss:  9523.448\n",
      "regression loss:  7227.9077\n",
      "episode: 1875/1000000, returns: -4.0, epsilon: 0.39\n",
      "regression loss:  223.69524\n",
      "regression loss:  4235.32\n",
      "episode: 1876/1000000, returns: 1.8, epsilon: 0.39\n",
      "regression loss:  7345.5557\n",
      "regression loss:  7277.7266\n",
      "episode: 1877/1000000, returns: -0.13, epsilon: 0.39\n",
      "regression loss:  1957.5415\n",
      "regression loss:  4406.71\n",
      "episode: 1878/1000000, returns: 3.1, epsilon: 0.39\n",
      "regression loss:  10755.386\n",
      "regression loss:  10757.959\n",
      "episode: 1879/1000000, returns: -1.6, epsilon: 0.39\n",
      "regression loss:  23566.283\n",
      "regression loss:  12467.144\n",
      "episode: 1880/1000000, returns: -2.1, epsilon: 0.39\n",
      "regression loss:  6354.5967\n",
      "regression loss:  1944.4673\n",
      "episode: 1881/1000000, returns: -0.2, epsilon: 0.39\n",
      "regression loss:  990.22314\n",
      "regression loss:  9626.869\n",
      "episode: 1882/1000000, returns: 1.2, epsilon: 0.39\n",
      "regression loss:  5064.1226\n",
      "regression loss:  1893.6826\n",
      "episode: 1883/1000000, returns: 0.85, epsilon: 0.39\n",
      "regression loss:  1826.464\n",
      "regression loss:  6370.298\n",
      "episode: 1884/1000000, returns: 1.3, epsilon: 0.39\n",
      "regression loss:  1596.7681\n",
      "regression loss:  4469.2607\n",
      "episode: 1885/1000000, returns: -0.19, epsilon: 0.39\n",
      "regression loss:  2558.9543\n",
      "regression loss:  4574.403\n",
      "episode: 1886/1000000, returns: 0.28, epsilon: 0.39\n",
      "regression loss:  602.59906\n",
      "regression loss:  6509.578\n",
      "episode: 1887/1000000, returns: -0.18, epsilon: 0.39\n",
      "regression loss:  1189.2405\n",
      "regression loss:  7541.962\n",
      "episode: 1888/1000000, returns: -0.24, epsilon: 0.39\n",
      "regression loss:  808.89276\n",
      "regression loss:  4883.1943\n",
      "episode: 1889/1000000, returns: -3.7, epsilon: 0.39\n",
      "regression loss:  939.162\n",
      "regression loss:  1989.7399\n",
      "episode: 1890/1000000, returns: -0.094, epsilon: 0.39\n",
      "regression loss:  4516.7617\n",
      "regression loss:  4201.8887\n",
      "episode: 1891/1000000, returns: 1.7, epsilon: 0.39\n",
      "regression loss:  2732.0083\n",
      "regression loss:  4035.9485\n",
      "episode: 1892/1000000, returns: -1.4, epsilon: 0.39\n",
      "regression loss:  985.1747\n",
      "regression loss:  4784.8047\n",
      "episode: 1893/1000000, returns: -0.37, epsilon: 0.39\n",
      "regression loss:  1570.6014\n",
      "regression loss:  2537.1416\n",
      "episode: 1894/1000000, returns: -2.2, epsilon: 0.39\n",
      "regression loss:  1529.2202\n",
      "regression loss:  2975.7466\n",
      "episode: 1895/1000000, returns: 6.2, epsilon: 0.39\n",
      "regression loss:  2109.7512\n",
      "regression loss:  1492.0671\n",
      "episode: 1896/1000000, returns: 1.3, epsilon: 0.39\n",
      "regression loss:  1027.2043\n",
      "regression loss:  1385.357\n",
      "episode: 1897/1000000, returns: 0.68, epsilon: 0.39\n",
      "regression loss:  670.1785\n",
      "regression loss:  3235.3022\n",
      "episode: 1898/1000000, returns: 0.4, epsilon: 0.39\n",
      "regression loss:  273.0422\n",
      "regression loss:  3021.5112\n",
      "episode: 1899/1000000, returns: -0.6, epsilon: 0.39\n",
      "regression loss:  374.9168\n",
      "regression loss:  860.21564\n",
      "episode: 1900/1000000, returns: 0.63, epsilon: 0.39\n",
      "regression loss:  2830.3918\n",
      "regression loss:  867.7153\n",
      "episode: 1901/1000000, returns: -3.6, epsilon: 0.39\n",
      "regression loss:  2986.6606\n",
      "regression loss:  3222.2422\n",
      "episode: 1902/1000000, returns: -0.91, epsilon: 0.39\n",
      "regression loss:  408.34842\n",
      "regression loss:  2753.7734\n",
      "episode: 1903/1000000, returns: 0.54, epsilon: 0.39\n",
      "regression loss:  1945.3341\n",
      "regression loss:  1163.3242\n",
      "episode: 1904/1000000, returns: 5.2, epsilon: 0.39\n",
      "regression loss:  1221.1672\n",
      "regression loss:  3475.4072\n",
      "episode: 1905/1000000, returns: -2.7, epsilon: 0.39\n",
      "regression loss:  1752.8436\n",
      "regression loss:  2963.872\n",
      "episode: 1906/1000000, returns: 0.12, epsilon: 0.39\n",
      "regression loss:  866.9842\n",
      "regression loss:  3598.1797\n",
      "episode: 1907/1000000, returns: 3.7, epsilon: 0.39\n",
      "regression loss:  3420.2178\n",
      "regression loss:  3383.4167\n",
      "episode: 1908/1000000, returns: -2.2, epsilon: 0.39\n",
      "regression loss:  1998.7526\n",
      "regression loss:  2811.9087\n",
      "episode: 1909/1000000, returns: -5.3, epsilon: 0.38\n",
      "regression loss:  3738.9434\n",
      "regression loss:  2677.5137\n",
      "episode: 1910/1000000, returns: 0.7, epsilon: 0.38\n",
      "regression loss:  938.67017\n",
      "regression loss:  4386.6177\n",
      "episode: 1911/1000000, returns: -0.62, epsilon: 0.38\n",
      "regression loss:  1029.3491\n",
      "regression loss:  2241.1926\n",
      "episode: 1912/1000000, returns: 0.71, epsilon: 0.38\n",
      "regression loss:  305.29404\n",
      "regression loss:  6803.231\n",
      "episode: 1913/1000000, returns: 0.5, epsilon: 0.38\n",
      "regression loss:  6995.742\n",
      "regression loss:  7542.0103\n",
      "episode: 1914/1000000, returns: 0.33, epsilon: 0.38\n",
      "regression loss:  1683.4312\n",
      "regression loss:  10379.43\n",
      "episode: 1915/1000000, returns: 1.2, epsilon: 0.38\n",
      "regression loss:  7777.2803\n",
      "regression loss:  12900.15\n",
      "episode: 1916/1000000, returns: -0.5, epsilon: 0.38\n",
      "regression loss:  30365.305\n",
      "regression loss:  14543.716\n",
      "episode: 1917/1000000, returns: -5.3, epsilon: 0.38\n",
      "regression loss:  7394.156\n",
      "regression loss:  11781.371\n",
      "episode: 1918/1000000, returns: -1.2e+01, epsilon: 0.38\n",
      "regression loss:  1388.814\n",
      "regression loss:  6661.32\n",
      "episode: 1919/1000000, returns: 5.4, epsilon: 0.38\n",
      "regression loss:  464.96844\n",
      "regression loss:  5454.713\n",
      "episode: 1920/1000000, returns: -0.17, epsilon: 0.38\n",
      "regression loss:  636.8535\n",
      "regression loss:  3491.6646\n",
      "episode: 1921/1000000, returns: 1.6, epsilon: 0.38\n",
      "regression loss:  422.46664\n",
      "regression loss:  3836.042\n",
      "episode: 1922/1000000, returns: -0.79, epsilon: 0.38\n",
      "regression loss:  1750.5615\n",
      "regression loss:  1997.6117\n",
      "episode: 1923/1000000, returns: 2.2, epsilon: 0.38\n",
      "regression loss:  197.96622\n",
      "regression loss:  4243.904\n",
      "episode: 1924/1000000, returns: 0.75, epsilon: 0.38\n",
      "regression loss:  2345.6724\n",
      "regression loss:  6095.503\n",
      "episode: 1925/1000000, returns: -0.77, epsilon: 0.38\n",
      "regression loss:  1374.7783\n",
      "regression loss:  2655.7712\n",
      "episode: 1926/1000000, returns: 9.1, epsilon: 0.38\n",
      "regression loss:  11262.531\n",
      "regression loss:  7514.04\n",
      "episode: 1927/1000000, returns: 3.5, epsilon: 0.38\n",
      "regression loss:  10061.384\n",
      "regression loss:  818.83746\n",
      "episode: 1928/1000000, returns: 2.7e+01, epsilon: 0.38\n",
      "regression loss:  1847.4109\n",
      "regression loss:  4781.13\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 1929/1000000, returns: -0.65, epsilon: 0.38\n",
      "regression loss:  1496.0763\n",
      "regression loss:  1116.2454\n",
      "episode: 1930/1000000, returns: 0.8, epsilon: 0.38\n",
      "regression loss:  804.84656\n",
      "regression loss:  1080.6125\n",
      "episode: 1931/1000000, returns: 1.4, epsilon: 0.38\n",
      "regression loss:  1718.0566\n",
      "regression loss:  804.4892\n",
      "episode: 1932/1000000, returns: -7.7, epsilon: 0.38\n",
      "regression loss:  933.1603\n",
      "regression loss:  3988.664\n",
      "episode: 1933/1000000, returns: 4.4, epsilon: 0.38\n",
      "regression loss:  915.81055\n",
      "regression loss:  3259.8806\n",
      "episode: 1934/1000000, returns: -1.9, epsilon: 0.38\n",
      "regression loss:  508.99393\n",
      "regression loss:  1174.3528\n",
      "episode: 1935/1000000, returns: 0.41, epsilon: 0.38\n",
      "regression loss:  399.38632\n",
      "regression loss:  2094.5085\n",
      "episode: 1936/1000000, returns: -1.7, epsilon: 0.38\n",
      "regression loss:  990.69727\n",
      "regression loss:  876.16394\n",
      "episode: 1937/1000000, returns: 0.85, epsilon: 0.38\n",
      "regression loss:  2247.7412\n",
      "regression loss:  3412.6824\n",
      "episode: 1938/1000000, returns: -2.5, epsilon: 0.38\n",
      "regression loss:  425.7017\n",
      "regression loss:  2520.724\n",
      "episode: 1939/1000000, returns: 0.51, epsilon: 0.38\n",
      "regression loss:  5264.442\n",
      "regression loss:  2437.284\n",
      "episode: 1940/1000000, returns: 9.7, epsilon: 0.38\n",
      "regression loss:  254.43106\n",
      "regression loss:  2683.214\n",
      "episode: 1941/1000000, returns: 4.9, epsilon: 0.38\n",
      "regression loss:  4694.6934\n",
      "regression loss:  962.97217\n",
      "episode: 1942/1000000, returns: 1.5, epsilon: 0.38\n",
      "regression loss:  3496.272\n",
      "regression loss:  4411.266\n",
      "episode: 1943/1000000, returns: -7.9, epsilon: 0.38\n",
      "regression loss:  3077.632\n",
      "regression loss:  6188.492\n",
      "episode: 1944/1000000, returns: -1.1, epsilon: 0.38\n",
      "regression loss:  6596.4185\n",
      "regression loss:  3903.1716\n",
      "episode: 1945/1000000, returns: -4.2, epsilon: 0.38\n",
      "regression loss:  1029.7651\n",
      "regression loss:  2463.0342\n",
      "episode: 1946/1000000, returns: 1.2, epsilon: 0.38\n",
      "regression loss:  7068.101\n",
      "regression loss:  3185.8823\n",
      "episode: 1947/1000000, returns: 0.71, epsilon: 0.38\n",
      "regression loss:  821.74786\n",
      "regression loss:  1502.4663\n",
      "episode: 1948/1000000, returns: 0.44, epsilon: 0.38\n",
      "regression loss:  763.4058\n",
      "regression loss:  2500.5742\n",
      "episode: 1949/1000000, returns: 1.2, epsilon: 0.38\n",
      "regression loss:  5150.714\n",
      "regression loss:  2276.9268\n",
      "episode: 1950/1000000, returns: 9.3, epsilon: 0.38\n",
      "regression loss:  5374.8794\n",
      "regression loss:  1860.1611\n",
      "episode: 1951/1000000, returns: -2.1, epsilon: 0.38\n",
      "regression loss:  3987.7437\n",
      "regression loss:  3851.4287\n",
      "episode: 1952/1000000, returns: 0.17, epsilon: 0.38\n",
      "regression loss:  4825.2295\n",
      "regression loss:  6071.1807\n",
      "episode: 1953/1000000, returns: -2.6, epsilon: 0.38\n",
      "regression loss:  1151.74\n",
      "regression loss:  1307.3928\n",
      "episode: 1954/1000000, returns: 0.33, epsilon: 0.38\n",
      "regression loss:  1271.3984\n",
      "regression loss:  2582.9136\n",
      "episode: 1955/1000000, returns: 0.89, epsilon: 0.38\n",
      "regression loss:  603.679\n",
      "regression loss:  730.1482\n",
      "episode: 1956/1000000, returns: 0.51, epsilon: 0.38\n",
      "regression loss:  3807.1792\n",
      "regression loss:  2139.7817\n",
      "episode: 1957/1000000, returns: 1.1e+01, epsilon: 0.38\n",
      "regression loss:  8925.646\n",
      "regression loss:  3213.8003\n",
      "episode: 1958/1000000, returns: 4.8, epsilon: 0.38\n",
      "regression loss:  462.0988\n",
      "regression loss:  8132.6606\n",
      "episode: 1959/1000000, returns: 3.4, epsilon: 0.38\n",
      "regression loss:  3323.3733\n",
      "regression loss:  4724.2554\n",
      "episode: 1960/1000000, returns: 0.24, epsilon: 0.38\n",
      "regression loss:  3152.0493\n",
      "regression loss:  1294.9805\n",
      "episode: 1961/1000000, returns: -0.083, epsilon: 0.38\n",
      "regression loss:  1117.3701\n",
      "regression loss:  7080.3564\n",
      "episode: 1962/1000000, returns: -0.83, epsilon: 0.37\n",
      "regression loss:  3461.7644\n",
      "regression loss:  3796.5483\n",
      "episode: 1963/1000000, returns: 6.8, epsilon: 0.37\n",
      "regression loss:  742.6185\n",
      "regression loss:  4493.622\n",
      "episode: 1964/1000000, returns: -0.57, epsilon: 0.37\n",
      "regression loss:  1182.1079\n",
      "regression loss:  2361.338\n",
      "episode: 1965/1000000, returns: 0.91, epsilon: 0.37\n",
      "regression loss:  1788.0764\n",
      "regression loss:  1429.4705\n",
      "episode: 1966/1000000, returns: 3.4, epsilon: 0.37\n",
      "regression loss:  291.92172\n",
      "regression loss:  5522.619\n",
      "episode: 1967/1000000, returns: -1.6, epsilon: 0.37\n",
      "regression loss:  5838.9004\n",
      "regression loss:  4011.9575\n",
      "episode: 1968/1000000, returns: 0.57, epsilon: 0.37\n",
      "regression loss:  387.79343\n",
      "regression loss:  4752.9434\n",
      "episode: 1969/1000000, returns: 2.2, epsilon: 0.37\n",
      "regression loss:  4603.4116\n",
      "regression loss:  12004.668\n",
      "episode: 1970/1000000, returns: 1.3, epsilon: 0.37\n",
      "regression loss:  9024.954\n",
      "regression loss:  3847.6147\n",
      "episode: 1971/1000000, returns: 3.6, epsilon: 0.37\n",
      "regression loss:  2967.5706\n",
      "regression loss:  3955.7837\n",
      "episode: 1972/1000000, returns: -4.2, epsilon: 0.37\n",
      "regression loss:  2353.984\n",
      "regression loss:  3807.3066\n",
      "episode: 1973/1000000, returns: 0.5, epsilon: 0.37\n",
      "regression loss:  536.80554\n",
      "regression loss:  3369.395\n",
      "episode: 1974/1000000, returns: 3.8, epsilon: 0.37\n",
      "regression loss:  2531.1506\n",
      "regression loss:  3263.129\n",
      "episode: 1975/1000000, returns: -4.2, epsilon: 0.37\n",
      "regression loss:  493.15393\n",
      "regression loss:  3002.2698\n",
      "episode: 1976/1000000, returns: -1.1, epsilon: 0.37\n",
      "regression loss:  4768.695\n",
      "regression loss:  4039.0608\n",
      "episode: 1977/1000000, returns: 5.8, epsilon: 0.37\n",
      "regression loss:  201.96964\n",
      "regression loss:  4976.506\n",
      "episode: 1978/1000000, returns: 7.0, epsilon: 0.37\n",
      "regression loss:  5404.7915\n",
      "regression loss:  5622.595\n",
      "episode: 1979/1000000, returns: 6.9, epsilon: 0.37\n",
      "regression loss:  1731.1205\n",
      "regression loss:  4708.627\n",
      "episode: 1980/1000000, returns: 0.63, epsilon: 0.37\n",
      "regression loss:  12145.424\n",
      "regression loss:  18324.02\n",
      "episode: 1981/1000000, returns: 2.6, epsilon: 0.37\n",
      "regression loss:  29384.371\n",
      "regression loss:  17721.12\n",
      "episode: 1982/1000000, returns: -1.2, epsilon: 0.37\n",
      "regression loss:  3166.8997\n",
      "regression loss:  6891.626\n",
      "episode: 1983/1000000, returns: -0.34, epsilon: 0.37\n",
      "regression loss:  2328.528\n",
      "regression loss:  16293.059\n",
      "episode: 1984/1000000, returns: 0.34, epsilon: 0.37\n",
      "regression loss:  14447.456\n",
      "regression loss:  10797.883\n",
      "episode: 1985/1000000, returns: -0.49, epsilon: 0.37\n",
      "regression loss:  1322.811\n",
      "regression loss:  10927.499\n",
      "episode: 1986/1000000, returns: -1.2, epsilon: 0.37\n",
      "regression loss:  1777.3557\n",
      "regression loss:  4103.873\n",
      "episode: 1987/1000000, returns: 2.5, epsilon: 0.37\n",
      "regression loss:  5957.251\n",
      "regression loss:  9252.349\n",
      "episode: 1988/1000000, returns: -0.11, epsilon: 0.37\n",
      "regression loss:  9011.53\n",
      "regression loss:  9149.676\n",
      "episode: 1989/1000000, returns: -1.4, epsilon: 0.37\n",
      "regression loss:  393.33453\n",
      "regression loss:  5977.622\n",
      "episode: 1990/1000000, returns: -0.14, epsilon: 0.37\n",
      "regression loss:  4335.3574\n",
      "regression loss:  3669.6467\n",
      "episode: 1991/1000000, returns: 0.52, epsilon: 0.37\n",
      "regression loss:  1033.7255\n",
      "regression loss:  4168.6123\n",
      "episode: 1992/1000000, returns: -4.5, epsilon: 0.37\n",
      "regression loss:  1254.4672\n",
      "regression loss:  3522.7139\n",
      "episode: 1993/1000000, returns: -0.74, epsilon: 0.37\n",
      "regression loss:  825.0679\n",
      "regression loss:  2508.9204\n",
      "episode: 1994/1000000, returns: 1.4, epsilon: 0.37\n",
      "regression loss:  441.9275\n",
      "regression loss:  2770.6501\n",
      "episode: 1995/1000000, returns: 0.16, epsilon: 0.37\n",
      "regression loss:  1276.516\n",
      "regression loss:  1712.7874\n",
      "episode: 1996/1000000, returns: 1.4, epsilon: 0.37\n",
      "regression loss:  1536.2664\n",
      "regression loss:  3250.583\n",
      "episode: 1997/1000000, returns: 5.4, epsilon: 0.37\n",
      "regression loss:  222.31882\n",
      "regression loss:  4398.5137\n",
      "episode: 1998/1000000, returns: 1.4, epsilon: 0.37\n",
      "regression loss:  5209.36\n",
      "regression loss:  4934.1963\n",
      "episode: 1999/1000000, returns: 3.8, epsilon: 0.37\n",
      "regression loss:  562.2636\n",
      "regression loss:  3621.4795\n",
      "episode: 2000/1000000, returns: 2.9, epsilon: 0.37\n",
      "regression loss:  3800.107\n",
      "regression loss:  4167.5576\n",
      "episode: 2001/1000000, returns: 4.6, epsilon: 0.37\n",
      "regression loss:  1391.2285\n",
      "regression loss:  7354.1934\n",
      "episode: 2002/1000000, returns: -0.48, epsilon: 0.37\n",
      "regression loss:  18895.336\n",
      "regression loss:  23455.201\n",
      "episode: 2003/1000000, returns: -3.4, epsilon: 0.37\n",
      "regression loss:  25400.1\n",
      "regression loss:  8424.807\n",
      "episode: 2004/1000000, returns: 2.9, epsilon: 0.37\n",
      "regression loss:  1687.435\n",
      "regression loss:  9892.162\n",
      "episode: 2005/1000000, returns: 4.9, epsilon: 0.37\n",
      "regression loss:  8652.576\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "regression loss:  8732.692\n",
      "episode: 2006/1000000, returns: 1.1, epsilon: 0.37\n",
      "regression loss:  27869.895\n",
      "regression loss:  12666.977\n",
      "episode: 2007/1000000, returns: 0.94, epsilon: 0.37\n",
      "regression loss:  669.83734\n",
      "regression loss:  12095.321\n",
      "episode: 2008/1000000, returns: 1.4, epsilon: 0.37\n",
      "regression loss:  4593.0796\n",
      "regression loss:  11909.129\n",
      "episode: 2009/1000000, returns: 1.4, epsilon: 0.37\n",
      "regression loss:  3010.3115\n",
      "regression loss:  7050.4883\n",
      "episode: 2010/1000000, returns: -4.2, epsilon: 0.37\n",
      "regression loss:  3305.4783\n",
      "regression loss:  10047.352\n",
      "episode: 2011/1000000, returns: -0.8, epsilon: 0.37\n",
      "regression loss:  1223.9387\n",
      "regression loss:  5849.869\n",
      "episode: 2012/1000000, returns: 2.0, epsilon: 0.37\n",
      "regression loss:  4541.013\n",
      "regression loss:  13189.084\n",
      "episode: 2013/1000000, returns: 0.6, epsilon: 0.37\n",
      "regression loss:  1965.3007\n",
      "regression loss:  5400.4355\n",
      "episode: 2014/1000000, returns: 4.3, epsilon: 0.37\n",
      "regression loss:  8086.679\n",
      "regression loss:  13565.699\n",
      "episode: 2015/1000000, returns: -2.1, epsilon: 0.37\n",
      "regression loss:  5418.6597\n",
      "regression loss:  5782.1597\n",
      "episode: 2016/1000000, returns: 6.1, epsilon: 0.36\n",
      "regression loss:  996.15857\n",
      "regression loss:  5559.839\n",
      "episode: 2017/1000000, returns: -1.7, epsilon: 0.36\n",
      "regression loss:  981.9058\n",
      "regression loss:  4532.415\n",
      "episode: 2018/1000000, returns: 5.9, epsilon: 0.36\n",
      "regression loss:  2495.4363\n",
      "regression loss:  3805.2173\n",
      "episode: 2019/1000000, returns: 0.8, epsilon: 0.36\n",
      "regression loss:  1877.2048\n",
      "regression loss:  6092.7954\n",
      "episode: 2020/1000000, returns: 2.6, epsilon: 0.36\n",
      "regression loss:  1069.912\n",
      "regression loss:  2537.6167\n",
      "episode: 2021/1000000, returns: 0.44, epsilon: 0.36\n",
      "regression loss:  1387.0579\n",
      "regression loss:  3186.8213\n",
      "episode: 2022/1000000, returns: 2.1, epsilon: 0.36\n",
      "regression loss:  590.7012\n",
      "regression loss:  2157.391\n",
      "episode: 2023/1000000, returns: 2.2, epsilon: 0.36\n",
      "regression loss:  384.00238\n",
      "regression loss:  1824.4385\n",
      "episode: 2024/1000000, returns: 5.3, epsilon: 0.36\n",
      "regression loss:  599.9438\n",
      "regression loss:  1594.3433\n",
      "episode: 2025/1000000, returns: -1.1e+01, epsilon: 0.36\n",
      "regression loss:  1113.5845\n",
      "regression loss:  10512.909\n",
      "episode: 2026/1000000, returns: 1.3, epsilon: 0.36\n",
      "regression loss:  1869.0436\n",
      "regression loss:  793.19244\n",
      "episode: 2027/1000000, returns: 5.5, epsilon: 0.36\n",
      "regression loss:  1278.0918\n",
      "regression loss:  5033.733\n",
      "episode: 2028/1000000, returns: 0.77, epsilon: 0.36\n",
      "regression loss:  2078.7014\n",
      "regression loss:  4574.6865\n",
      "episode: 2029/1000000, returns: 1.7e+01, epsilon: 0.36\n",
      "regression loss:  1276.5078\n",
      "regression loss:  1446.4148\n",
      "episode: 2030/1000000, returns: 3.3, epsilon: 0.36\n",
      "regression loss:  2478.191\n",
      "regression loss:  7711.7876\n",
      "episode: 2031/1000000, returns: 0.4, epsilon: 0.36\n",
      "regression loss:  628.6845\n",
      "regression loss:  5190.641\n",
      "episode: 2032/1000000, returns: 3.0, epsilon: 0.36\n",
      "regression loss:  6628.298\n",
      "regression loss:  7412.6436\n",
      "episode: 2033/1000000, returns: -2.6, epsilon: 0.36\n",
      "regression loss:  2356.9238\n",
      "regression loss:  5086.532\n",
      "episode: 2034/1000000, returns: 3.8, epsilon: 0.36\n",
      "regression loss:  8023.242\n",
      "regression loss:  20041.938\n",
      "episode: 2035/1000000, returns: 0.9, epsilon: 0.36\n",
      "regression loss:  18755.355\n",
      "regression loss:  16552.104\n",
      "episode: 2036/1000000, returns: -1.3, epsilon: 0.36\n",
      "regression loss:  2328.0498\n",
      "regression loss:  16849.053\n",
      "episode: 2037/1000000, returns: 1.8, epsilon: 0.36\n",
      "regression loss:  17534.943\n",
      "regression loss:  16514.19\n",
      "episode: 2038/1000000, returns: 6.1, epsilon: 0.36\n",
      "regression loss:  10767.925\n",
      "regression loss:  33000.83\n",
      "episode: 2039/1000000, returns: 1.4, epsilon: 0.36\n",
      "regression loss:  15464.837\n",
      "regression loss:  5824.9688\n",
      "episode: 2040/1000000, returns: 0.56, epsilon: 0.36\n",
      "regression loss:  4987.339\n",
      "regression loss:  9195.464\n",
      "episode: 2041/1000000, returns: -1.8, epsilon: 0.36\n",
      "regression loss:  29638.484\n",
      "regression loss:  19466.492\n",
      "episode: 2042/1000000, returns: -3.5, epsilon: 0.36\n",
      "regression loss:  5854.115\n",
      "regression loss:  12129.352\n",
      "episode: 2043/1000000, returns: -0.082, epsilon: 0.36\n",
      "regression loss:  2393.6821\n",
      "regression loss:  7402.0747\n",
      "episode: 2044/1000000, returns: -0.13, epsilon: 0.36\n",
      "regression loss:  1536.6426\n",
      "regression loss:  4459.195\n",
      "episode: 2045/1000000, returns: 1.0, epsilon: 0.36\n",
      "regression loss:  2508.8828\n",
      "regression loss:  4662.5015\n",
      "episode: 2046/1000000, returns: 1.0, epsilon: 0.36\n",
      "regression loss:  901.3299\n",
      "regression loss:  4377.3735\n",
      "episode: 2047/1000000, returns: -0.26, epsilon: 0.36\n",
      "regression loss:  1742.8689\n",
      "regression loss:  6973.935\n",
      "episode: 2048/1000000, returns: 2.0, epsilon: 0.36\n",
      "regression loss:  914.3467\n",
      "regression loss:  2581.5486\n",
      "episode: 2049/1000000, returns: 1.5, epsilon: 0.36\n",
      "regression loss:  1883.6254\n",
      "regression loss:  1411.626\n",
      "episode: 2050/1000000, returns: 3.3, epsilon: 0.36\n",
      "regression loss:  736.68097\n",
      "regression loss:  5061.287\n",
      "episode: 2051/1000000, returns: 2.8, epsilon: 0.36\n",
      "regression loss:  1101.6362\n",
      "regression loss:  1956.0706\n",
      "episode: 2052/1000000, returns: -0.2, epsilon: 0.36\n",
      "regression loss:  3712.805\n",
      "regression loss:  4748.1836\n",
      "episode: 2053/1000000, returns: -0.91, epsilon: 0.36\n",
      "regression loss:  177.89873\n",
      "regression loss:  657.46643\n",
      "episode: 2054/1000000, returns: 9.7, epsilon: 0.36\n",
      "regression loss:  254.44283\n",
      "regression loss:  185.84499\n",
      "episode: 2055/1000000, returns: 3.7, epsilon: 0.36\n",
      "regression loss:  416.35526\n",
      "regression loss:  37.211716\n",
      "episode: 2056/1000000, returns: -1.1, epsilon: 0.36\n",
      "regression loss:  52.06132\n",
      "regression loss:  47.75264\n",
      "episode: 2057/1000000, returns: 5.8, epsilon: 0.36\n",
      "regression loss:  158.55373\n",
      "regression loss:  56.519558\n",
      "episode: 2058/1000000, returns: -2.4, epsilon: 0.36\n",
      "regression loss:  39.51319\n",
      "regression loss:  46.60243\n",
      "episode: 2059/1000000, returns: -2.0, epsilon: 0.36\n",
      "regression loss:  10.7855425\n",
      "regression loss:  45.51139\n",
      "episode: 2060/1000000, returns: 8.0, epsilon: 0.36\n",
      "regression loss:  511.59076\n",
      "regression loss:  424.2174\n",
      "episode: 2061/1000000, returns: -1.6, epsilon: 0.36\n",
      "regression loss:  670.6095\n",
      "regression loss:  441.18765\n",
      "episode: 2062/1000000, returns: 6.9, epsilon: 0.36\n",
      "regression loss:  1345.2428\n",
      "regression loss:  4907.5923\n",
      "episode: 2063/1000000, returns: -1.0, epsilon: 0.36\n",
      "regression loss:  6451.0903\n",
      "regression loss:  7176.866\n",
      "episode: 2064/1000000, returns: 3.2, epsilon: 0.36\n",
      "regression loss:  5364.8604\n",
      "regression loss:  2718.3352\n",
      "episode: 2065/1000000, returns: 1.5, epsilon: 0.36\n",
      "regression loss:  3478.7168\n",
      "regression loss:  9471.858\n",
      "episode: 2066/1000000, returns: 3.2, epsilon: 0.36\n",
      "regression loss:  12202.128\n",
      "regression loss:  4059.692\n",
      "episode: 2067/1000000, returns: 4.6, epsilon: 0.36\n",
      "regression loss:  9246.167\n",
      "regression loss:  1930.212\n",
      "episode: 2068/1000000, returns: 3.8, epsilon: 0.36\n",
      "regression loss:  3348.0137\n",
      "regression loss:  3822.9883\n",
      "episode: 2069/1000000, returns: -0.96, epsilon: 0.36\n",
      "regression loss:  542.6673\n",
      "regression loss:  2761.0034\n",
      "episode: 2070/1000000, returns: 0.18, epsilon: 0.36\n",
      "regression loss:  3257.348\n",
      "regression loss:  4061.9119\n",
      "episode: 2071/1000000, returns: -0.78, epsilon: 0.35\n",
      "regression loss:  7804.0356\n",
      "regression loss:  3666.5334\n",
      "episode: 2072/1000000, returns: -1.5, epsilon: 0.35\n",
      "regression loss:  426.1309\n",
      "regression loss:  7684.59\n",
      "episode: 2073/1000000, returns: 0.82, epsilon: 0.35\n",
      "regression loss:  5218.4487\n",
      "regression loss:  5737.4443\n",
      "episode: 2074/1000000, returns: 4.5, epsilon: 0.35\n",
      "regression loss:  2642.961\n",
      "regression loss:  3444.2969\n",
      "episode: 2075/1000000, returns: 1.9, epsilon: 0.35\n",
      "regression loss:  5007.148\n",
      "regression loss:  6793.8394\n",
      "episode: 2076/1000000, returns: 2.9, epsilon: 0.35\n",
      "regression loss:  3895.5264\n",
      "regression loss:  4275.959\n",
      "episode: 2077/1000000, returns: 5.7, epsilon: 0.35\n",
      "regression loss:  2129.1077\n",
      "regression loss:  12989.359\n",
      "episode: 2078/1000000, returns: -1.1, epsilon: 0.35\n",
      "regression loss:  9901.347\n",
      "regression loss:  2389.132\n",
      "episode: 2079/1000000, returns: -0.16, epsilon: 0.35\n",
      "regression loss:  339.386\n",
      "regression loss:  4709.677\n",
      "episode: 2080/1000000, returns: -2.3, epsilon: 0.35\n",
      "regression loss:  6376.857\n",
      "regression loss:  6246.074\n",
      "episode: 2081/1000000, returns: 0.91, epsilon: 0.35\n",
      "regression loss:  2053.1382\n",
      "regression loss:  2845.0906\n",
      "episode: 2082/1000000, returns: 1.0, epsilon: 0.35\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "regression loss:  4201.8506\n",
      "regression loss:  12589.827\n",
      "episode: 2083/1000000, returns: 2.0, epsilon: 0.35\n",
      "regression loss:  10667.921\n",
      "regression loss:  10168.009\n",
      "episode: 2084/1000000, returns: 0.29, epsilon: 0.35\n",
      "regression loss:  1277.4874\n",
      "regression loss:  8767.227\n",
      "episode: 2085/1000000, returns: -1.1, epsilon: 0.35\n",
      "regression loss:  13661.056\n",
      "regression loss:  20600.338\n",
      "episode: 2086/1000000, returns: 7.6, epsilon: 0.35\n",
      "regression loss:  19600.623\n",
      "regression loss:  22868.016\n",
      "episode: 2087/1000000, returns: 0.37, epsilon: 0.35\n",
      "regression loss:  3409.3262\n",
      "regression loss:  5783.622\n",
      "episode: 2088/1000000, returns: 4.0, epsilon: 0.35\n",
      "regression loss:  8504.782\n",
      "regression loss:  25570.207\n",
      "episode: 2089/1000000, returns: -4.0, epsilon: 0.35\n",
      "regression loss:  94337.23\n",
      "regression loss:  20792.086\n",
      "episode: 2090/1000000, returns: -0.56, epsilon: 0.35\n",
      "regression loss:  6178.973\n",
      "regression loss:  6135.5986\n",
      "episode: 2091/1000000, returns: -3.8, epsilon: 0.35\n",
      "regression loss:  3082.8298\n",
      "regression loss:  9969.624\n",
      "episode: 2092/1000000, returns: -2.4, epsilon: 0.35\n",
      "regression loss:  6165.4795\n",
      "regression loss:  6565.565\n",
      "episode: 2093/1000000, returns: 0.38, epsilon: 0.35\n",
      "regression loss:  867.4325\n",
      "regression loss:  9676.03\n",
      "episode: 2094/1000000, returns: 0.69, epsilon: 0.35\n",
      "regression loss:  4518.1367\n",
      "regression loss:  13254.295\n",
      "episode: 2095/1000000, returns: -3.1, epsilon: 0.35\n",
      "regression loss:  2546.6855\n",
      "regression loss:  3608.7905\n",
      "episode: 2096/1000000, returns: -1e+01, epsilon: 0.35\n",
      "regression loss:  11475.238\n",
      "regression loss:  15120.6\n",
      "episode: 2097/1000000, returns: 2.5, epsilon: 0.35\n",
      "regression loss:  13891.404\n",
      "regression loss:  8801.391\n",
      "episode: 2098/1000000, returns: -0.7, epsilon: 0.35\n",
      "regression loss:  2146.849\n",
      "regression loss:  9718.419\n",
      "episode: 2099/1000000, returns: -1.1, epsilon: 0.35\n",
      "regression loss:  2555.7207\n",
      "regression loss:  8737.431\n",
      "episode: 2100/1000000, returns: 0.9, epsilon: 0.35\n",
      "regression loss:  3151.6597\n",
      "regression loss:  3385.8042\n",
      "episode: 2101/1000000, returns: 1.4e+01, epsilon: 0.35\n",
      "regression loss:  3969.234\n",
      "regression loss:  6873.424\n",
      "episode: 2102/1000000, returns: 0.54, epsilon: 0.35\n",
      "regression loss:  863.9704\n",
      "regression loss:  5982.998\n",
      "episode: 2103/1000000, returns: -0.96, epsilon: 0.35\n",
      "regression loss:  1611.4146\n",
      "regression loss:  5424.736\n",
      "episode: 2104/1000000, returns: 1.2, epsilon: 0.35\n",
      "regression loss:  738.384\n",
      "regression loss:  3423.3457\n",
      "episode: 2105/1000000, returns: 2.3, epsilon: 0.35\n",
      "regression loss:  10045.182\n",
      "regression loss:  5235.379\n",
      "episode: 2106/1000000, returns: -1.5, epsilon: 0.35\n",
      "regression loss:  2441.8416\n",
      "regression loss:  6609.7666\n",
      "episode: 2107/1000000, returns: -2.0, epsilon: 0.35\n",
      "regression loss:  6159.5815\n",
      "regression loss:  12608.213\n",
      "episode: 2108/1000000, returns: 0.15, epsilon: 0.35\n",
      "regression loss:  3676.1882\n",
      "regression loss:  6326.173\n",
      "episode: 2109/1000000, returns: 1.7, epsilon: 0.35\n",
      "regression loss:  3791.093\n",
      "regression loss:  11695.556\n",
      "episode: 2110/1000000, returns: 2.7, epsilon: 0.35\n",
      "regression loss:  11992.796\n",
      "regression loss:  8604.275\n",
      "episode: 2111/1000000, returns: -0.21, epsilon: 0.35\n",
      "regression loss:  1241.8501\n",
      "regression loss:  8332.47\n",
      "episode: 2112/1000000, returns: 1.4, epsilon: 0.35\n",
      "regression loss:  4474.072\n",
      "regression loss:  9035.702\n",
      "episode: 2113/1000000, returns: -0.47, epsilon: 0.35\n",
      "regression loss:  1640.9098\n",
      "regression loss:  4927.1143\n",
      "episode: 2114/1000000, returns: 0.26, epsilon: 0.35\n",
      "regression loss:  17948.314\n",
      "regression loss:  19191.586\n",
      "episode: 2115/1000000, returns: 1.3, epsilon: 0.35\n",
      "regression loss:  19390.902\n",
      "regression loss:  7908.2783\n",
      "episode: 2116/1000000, returns: 2.4, epsilon: 0.35\n",
      "regression loss:  1830.1635\n",
      "regression loss:  6807.757\n",
      "episode: 2117/1000000, returns: 8.0, epsilon: 0.35\n",
      "regression loss:  4977.744\n",
      "regression loss:  7589.045\n",
      "episode: 2118/1000000, returns: -0.43, epsilon: 0.35\n",
      "regression loss:  2257.6753\n",
      "regression loss:  4974.3066\n",
      "episode: 2119/1000000, returns: 2.2, epsilon: 0.35\n",
      "regression loss:  1881.5349\n",
      "regression loss:  3966.5898\n",
      "episode: 2120/1000000, returns: 0.14, epsilon: 0.35\n",
      "regression loss:  539.4151\n",
      "regression loss:  4217.13\n",
      "episode: 2121/1000000, returns: -0.93, epsilon: 0.35\n",
      "regression loss:  379.40454\n",
      "regression loss:  6093.6533\n",
      "episode: 2122/1000000, returns: 0.3, epsilon: 0.35\n",
      "regression loss:  1084.428\n",
      "regression loss:  4700.871\n",
      "episode: 2123/1000000, returns: 4.4, epsilon: 0.35\n",
      "regression loss:  207.80063\n",
      "regression loss:  3448.0007\n",
      "episode: 2124/1000000, returns: 0.021, epsilon: 0.35\n",
      "regression loss:  327.7788\n",
      "regression loss:  1524.0715\n",
      "episode: 2125/1000000, returns: -0.55, epsilon: 0.35\n",
      "regression loss:  90.542145\n",
      "regression loss:  2341.4048\n",
      "episode: 2126/1000000, returns: 1.3, epsilon: 0.35\n",
      "regression loss:  178.11096\n",
      "regression loss:  915.1703\n",
      "episode: 2127/1000000, returns: -0.043, epsilon: 0.35\n",
      "regression loss:  1115.1377\n",
      "regression loss:  1523.4496\n",
      "episode: 2128/1000000, returns: -3.7, epsilon: 0.34\n",
      "regression loss:  765.6509\n",
      "regression loss:  1187.7322\n",
      "episode: 2129/1000000, returns: 1.6, epsilon: 0.34\n",
      "regression loss:  154.92848\n",
      "regression loss:  1488.6091\n",
      "episode: 2130/1000000, returns: -3.1, epsilon: 0.34\n",
      "regression loss:  2582.416\n",
      "regression loss:  2654.161\n",
      "episode: 2131/1000000, returns: 4.3, epsilon: 0.34\n",
      "regression loss:  462.82248\n",
      "regression loss:  830.8512\n",
      "episode: 2132/1000000, returns: -0.3, epsilon: 0.34\n",
      "regression loss:  388.9613\n",
      "regression loss:  2033.6711\n",
      "episode: 2133/1000000, returns: 3.8, epsilon: 0.34\n",
      "regression loss:  694.4727\n",
      "regression loss:  1689.1389\n",
      "episode: 2134/1000000, returns: 2.8, epsilon: 0.34\n",
      "regression loss:  988.0096\n",
      "regression loss:  3335.942\n",
      "episode: 2135/1000000, returns: -1.6, epsilon: 0.34\n",
      "regression loss:  809.8227\n",
      "regression loss:  3777.2046\n",
      "episode: 2136/1000000, returns: 4.0, epsilon: 0.34\n",
      "regression loss:  497.18173\n",
      "regression loss:  1659.4985\n",
      "episode: 2137/1000000, returns: 0.3, epsilon: 0.34\n",
      "regression loss:  3402.877\n",
      "regression loss:  3350.0964\n",
      "episode: 2138/1000000, returns: 0.24, epsilon: 0.34\n",
      "regression loss:  158.41998\n",
      "regression loss:  2742.425\n",
      "episode: 2139/1000000, returns: 4.4, epsilon: 0.34\n",
      "regression loss:  1920.9246\n",
      "regression loss:  1179.1978\n",
      "episode: 2140/1000000, returns: 7.7, epsilon: 0.34\n",
      "regression loss:  575.2874\n",
      "regression loss:  2160.1357\n",
      "episode: 2141/1000000, returns: 1.1, epsilon: 0.34\n",
      "regression loss:  126.0958\n",
      "regression loss:  2496.294\n",
      "episode: 2142/1000000, returns: -4.1, epsilon: 0.34\n",
      "regression loss:  697.0021\n",
      "regression loss:  910.72485\n",
      "episode: 2143/1000000, returns: -0.27, epsilon: 0.34\n",
      "regression loss:  3296.2217\n",
      "regression loss:  3187.56\n",
      "episode: 2144/1000000, returns: 1.8, epsilon: 0.34\n",
      "regression loss:  8091.1626\n",
      "regression loss:  3664.023\n",
      "episode: 2145/1000000, returns: -0.042, epsilon: 0.34\n",
      "regression loss:  2009.3563\n",
      "regression loss:  11444.332\n",
      "episode: 2146/1000000, returns: -0.94, epsilon: 0.34\n",
      "regression loss:  12858.951\n",
      "regression loss:  15325.433\n",
      "episode: 2147/1000000, returns: -0.059, epsilon: 0.34\n",
      "regression loss:  10530.341\n",
      "regression loss:  7436.549\n",
      "episode: 2148/1000000, returns: 2.0, epsilon: 0.34\n",
      "regression loss:  1278.7103\n",
      "regression loss:  9654.379\n",
      "episode: 2149/1000000, returns: 3.8, epsilon: 0.34\n",
      "regression loss:  45297.14\n",
      "regression loss:  29417.02\n",
      "episode: 2150/1000000, returns: -0.37, epsilon: 0.34\n",
      "regression loss:  37953.707\n",
      "regression loss:  11106.711\n",
      "episode: 2151/1000000, returns: 0.39, epsilon: 0.34\n",
      "regression loss:  1434.8987\n",
      "regression loss:  11300.732\n",
      "episode: 2152/1000000, returns: -2.4, epsilon: 0.34\n",
      "regression loss:  13040.965\n",
      "regression loss:  26128.613\n",
      "episode: 2153/1000000, returns: 0.27, epsilon: 0.34\n",
      "regression loss:  4816.0557\n",
      "regression loss:  10081.602\n",
      "episode: 2154/1000000, returns: 0.67, epsilon: 0.34\n",
      "regression loss:  6479.0503\n",
      "regression loss:  13327.986\n",
      "episode: 2155/1000000, returns: 8.7, epsilon: 0.34\n",
      "regression loss:  18925.055\n",
      "regression loss:  12415.293\n",
      "episode: 2156/1000000, returns: -3.2, epsilon: 0.34\n",
      "regression loss:  6560.747\n",
      "regression loss:  15016.291\n",
      "episode: 2157/1000000, returns: 2.2, epsilon: 0.34\n",
      "regression loss:  2836.4182\n",
      "regression loss:  19252.258\n",
      "episode: 2158/1000000, returns: -4.8, epsilon: 0.34\n",
      "regression loss:  10608.8955\n",
      "regression loss:  8864.711\n",
      "episode: 2159/1000000, returns: 0.92, epsilon: 0.34\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "regression loss:  2129.7622\n",
      "regression loss:  4268.8745\n",
      "episode: 2160/1000000, returns: 1.3, epsilon: 0.34\n",
      "regression loss:  2228.5742\n",
      "regression loss:  10433.674\n",
      "episode: 2161/1000000, returns: 0.92, epsilon: 0.34\n",
      "regression loss:  545.4079\n",
      "regression loss:  4087.096\n",
      "episode: 2162/1000000, returns: -0.6, epsilon: 0.34\n",
      "regression loss:  1143.5782\n",
      "regression loss:  4318.506\n",
      "episode: 2163/1000000, returns: 0.7, epsilon: 0.34\n",
      "regression loss:  279.77643\n",
      "regression loss:  2242.7236\n",
      "episode: 2164/1000000, returns: -0.62, epsilon: 0.34\n",
      "regression loss:  672.9029\n",
      "regression loss:  2254.1003\n",
      "episode: 2165/1000000, returns: 1.9, epsilon: 0.34\n",
      "regression loss:  1822.6581\n",
      "regression loss:  1736.1187\n",
      "episode: 2166/1000000, returns: -3.7, epsilon: 0.34\n",
      "regression loss:  2211.9395\n",
      "regression loss:  2511.0867\n",
      "episode: 2167/1000000, returns: -4.6, epsilon: 0.34\n",
      "regression loss:  2286.1487\n",
      "regression loss:  5803.051\n",
      "episode: 2168/1000000, returns: 1.6, epsilon: 0.34\n",
      "regression loss:  1291.5665\n",
      "regression loss:  2049.0874\n",
      "episode: 2169/1000000, returns: 5.9, epsilon: 0.34\n",
      "regression loss:  4596.456\n",
      "regression loss:  9085.41\n",
      "episode: 2170/1000000, returns: -4.6, epsilon: 0.34\n",
      "regression loss:  4672.142\n",
      "regression loss:  6913.2534\n",
      "episode: 2171/1000000, returns: 3.2, epsilon: 0.34\n",
      "regression loss:  7981.3813\n",
      "regression loss:  3340.5757\n",
      "episode: 2172/1000000, returns: 1.6, epsilon: 0.34\n",
      "regression loss:  2382.4048\n",
      "regression loss:  8196.503\n",
      "episode: 2173/1000000, returns: 5.8, epsilon: 0.34\n",
      "regression loss:  2888.5508\n",
      "regression loss:  2293.755\n",
      "episode: 2174/1000000, returns: 0.19, epsilon: 0.34\n",
      "regression loss:  1260.2847\n",
      "regression loss:  7418.05\n",
      "episode: 2175/1000000, returns: 0.61, epsilon: 0.34\n",
      "regression loss:  4163.1655\n",
      "regression loss:  4580.552\n",
      "episode: 2176/1000000, returns: 4.7, epsilon: 0.34\n",
      "regression loss:  2353.7825\n",
      "regression loss:  7084.9434\n",
      "episode: 2177/1000000, returns: -8.1, epsilon: 0.34\n",
      "regression loss:  7151.6987\n",
      "regression loss:  4612.256\n",
      "episode: 2178/1000000, returns: 1.7, epsilon: 0.34\n",
      "regression loss:  617.32904\n",
      "regression loss:  3614.86\n",
      "episode: 2179/1000000, returns: -0.96, epsilon: 0.34\n",
      "regression loss:  548.0847\n",
      "regression loss:  5395.8364\n",
      "episode: 2180/1000000, returns: -1.5, epsilon: 0.34\n",
      "regression loss:  803.34656\n",
      "regression loss:  3197.8777\n",
      "episode: 2181/1000000, returns: -8.2, epsilon: 0.34\n",
      "regression loss:  306.84387\n",
      "regression loss:  1354.3171\n",
      "episode: 2182/1000000, returns: 0.68, epsilon: 0.34\n",
      "regression loss:  1456.7566\n",
      "regression loss:  1619.2971\n",
      "episode: 2183/1000000, returns: 1.5, epsilon: 0.34\n",
      "regression loss:  1731.221\n",
      "regression loss:  2100.914\n",
      "episode: 2184/1000000, returns: -0.32, epsilon: 0.34\n",
      "regression loss:  744.35486\n",
      "regression loss:  283.329\n",
      "episode: 2185/1000000, returns: -0.27, epsilon: 0.34\n",
      "regression loss:  2730.3022\n",
      "regression loss:  2568.9268\n",
      "episode: 2186/1000000, returns: -2.1, epsilon: 0.34\n",
      "regression loss:  1138.2483\n",
      "regression loss:  3076.1384\n",
      "episode: 2187/1000000, returns: 8.2, epsilon: 0.33\n",
      "regression loss:  2883.6313\n",
      "regression loss:  1546.6743\n",
      "episode: 2188/1000000, returns: -3.4, epsilon: 0.33\n",
      "regression loss:  6457.4917\n",
      "regression loss:  213.84563\n",
      "episode: 2189/1000000, returns: -1.4, epsilon: 0.33\n",
      "regression loss:  1106.2687\n",
      "regression loss:  956.218\n",
      "episode: 2190/1000000, returns: 0.84, epsilon: 0.33\n",
      "regression loss:  80.89274\n",
      "regression loss:  216.63075\n",
      "episode: 2191/1000000, returns: 1.0, epsilon: 0.33\n",
      "regression loss:  128.34804\n",
      "regression loss:  38.214077\n",
      "episode: 2192/1000000, returns: 3.9, epsilon: 0.33\n",
      "regression loss:  1147.5599\n",
      "regression loss:  439.78528\n",
      "episode: 2193/1000000, returns: 7.6, epsilon: 0.33\n",
      "regression loss:  777.2704\n",
      "regression loss:  95.36814\n",
      "episode: 2194/1000000, returns: 2.2, epsilon: 0.33\n",
      "regression loss:  121.8562\n",
      "regression loss:  98.26043\n",
      "episode: 2195/1000000, returns: 1.3e+01, epsilon: 0.33\n",
      "regression loss:  284.14627\n",
      "regression loss:  277.4965\n",
      "episode: 2196/1000000, returns: -0.075, epsilon: 0.33\n",
      "regression loss:  52.427372\n",
      "regression loss:  47.702484\n",
      "episode: 2197/1000000, returns: 3.6, epsilon: 0.33\n",
      "regression loss:  11.8155985\n",
      "regression loss:  293.29715\n",
      "episode: 2198/1000000, returns: -2.7, epsilon: 0.33\n",
      "regression loss:  485.67184\n",
      "regression loss:  303.79642\n",
      "episode: 2199/1000000, returns: -2.7, epsilon: 0.33\n",
      "regression loss:  1445.4442\n",
      "regression loss:  284.62286\n",
      "episode: 2200/1000000, returns: -2.0, epsilon: 0.33\n",
      "regression loss:  94.22737\n",
      "regression loss:  7020.134\n",
      "episode: 2201/1000000, returns: 0.33, epsilon: 0.33\n",
      "regression loss:  3444.157\n",
      "regression loss:  1914.6504\n",
      "episode: 2202/1000000, returns: -9.8, epsilon: 0.33\n",
      "regression loss:  4051.3845\n",
      "regression loss:  6787.2373\n",
      "episode: 2203/1000000, returns: 0.35, epsilon: 0.33\n",
      "regression loss:  2094.0154\n",
      "regression loss:  3565.98\n",
      "episode: 2204/1000000, returns: -1.0, epsilon: 0.33\n",
      "regression loss:  103.933\n",
      "regression loss:  1345.9673\n",
      "episode: 2205/1000000, returns: -4.1, epsilon: 0.33\n",
      "regression loss:  1424.6807\n",
      "regression loss:  952.2773\n",
      "episode: 2206/1000000, returns: 0.6, epsilon: 0.33\n",
      "regression loss:  388.88004\n",
      "regression loss:  2308.8645\n",
      "episode: 2207/1000000, returns: 3.0, epsilon: 0.33\n",
      "regression loss:  3804.3972\n",
      "regression loss:  2103.0337\n",
      "episode: 2208/1000000, returns: -1e+01, epsilon: 0.33\n",
      "regression loss:  547.7549\n",
      "regression loss:  1635.3887\n",
      "episode: 2209/1000000, returns: 1.8, epsilon: 0.33\n",
      "regression loss:  5432.9194\n",
      "regression loss:  4355.817\n",
      "episode: 2210/1000000, returns: 6.0, epsilon: 0.33\n",
      "regression loss:  628.38354\n",
      "regression loss:  1696.593\n",
      "episode: 2211/1000000, returns: -3.1, epsilon: 0.33\n",
      "regression loss:  6461.2974\n",
      "regression loss:  7093.884\n",
      "episode: 2212/1000000, returns: 3.7, epsilon: 0.33\n",
      "regression loss:  1576.1698\n",
      "regression loss:  4141.349\n",
      "episode: 2213/1000000, returns: 5.0, epsilon: 0.33\n",
      "regression loss:  2991.7888\n",
      "regression loss:  4563.054\n",
      "episode: 2214/1000000, returns: 7.8, epsilon: 0.33\n",
      "regression loss:  4951.7764\n",
      "regression loss:  6480.207\n",
      "episode: 2215/1000000, returns: 1.2, epsilon: 0.33\n",
      "regression loss:  1744.7245\n",
      "regression loss:  6645.1143\n",
      "episode: 2216/1000000, returns: -1.3, epsilon: 0.33\n",
      "regression loss:  8685.942\n",
      "regression loss:  25260.32\n",
      "episode: 2217/1000000, returns: -0.089, epsilon: 0.33\n",
      "regression loss:  4520.3877\n",
      "regression loss:  14871.211\n",
      "episode: 2218/1000000, returns: 2.1, epsilon: 0.33\n",
      "regression loss:  4018.132\n",
      "regression loss:  2707.8652\n",
      "episode: 2219/1000000, returns: -1.4, epsilon: 0.33\n",
      "regression loss:  3884.769\n",
      "regression loss:  14886.987\n",
      "episode: 2220/1000000, returns: 3.8, epsilon: 0.33\n",
      "regression loss:  34070.4\n",
      "regression loss:  13830.752\n",
      "episode: 2221/1000000, returns: 0.46, epsilon: 0.33\n",
      "regression loss:  2265.3174\n",
      "regression loss:  6209.677\n",
      "episode: 2222/1000000, returns: 0.61, epsilon: 0.33\n",
      "regression loss:  24426.637\n",
      "regression loss:  23291.068\n",
      "episode: 2223/1000000, returns: 1.8, epsilon: 0.33\n",
      "regression loss:  7706.441\n",
      "regression loss:  22349.533\n",
      "episode: 2224/1000000, returns: -0.04, epsilon: 0.33\n",
      "regression loss:  6049.5273\n",
      "regression loss:  3305.725\n",
      "episode: 2225/1000000, returns: -6.0, epsilon: 0.33\n",
      "regression loss:  6539.095\n",
      "regression loss:  17561.207\n",
      "episode: 2226/1000000, returns: 1.7, epsilon: 0.33\n",
      "regression loss:  6169.3643\n",
      "regression loss:  7777.4736\n",
      "episode: 2227/1000000, returns: -3.8, epsilon: 0.33\n",
      "regression loss:  4820.288\n",
      "regression loss:  7168.9536\n",
      "episode: 2228/1000000, returns: 1.6, epsilon: 0.33\n",
      "regression loss:  1660.7467\n",
      "regression loss:  5443.8647\n",
      "episode: 2229/1000000, returns: 0.66, epsilon: 0.33\n",
      "regression loss:  813.983\n",
      "regression loss:  6467.998\n",
      "episode: 2230/1000000, returns: 0.53, epsilon: 0.33\n",
      "regression loss:  3242.9905\n",
      "regression loss:  5579.9487\n",
      "episode: 2231/1000000, returns: -3.5, epsilon: 0.33\n",
      "regression loss:  928.9393\n",
      "regression loss:  4464.074\n",
      "episode: 2232/1000000, returns: 1.5, epsilon: 0.33\n",
      "regression loss:  634.0444\n",
      "regression loss:  9385.819\n",
      "episode: 2233/1000000, returns: 2.2, epsilon: 0.33\n",
      "regression loss:  892.1028\n",
      "regression loss:  3005.438\n",
      "episode: 2234/1000000, returns: 0.85, epsilon: 0.33\n",
      "regression loss:  309.29407\n",
      "regression loss:  2206.8728\n",
      "episode: 2235/1000000, returns: 0.46, epsilon: 0.33\n",
      "regression loss:  316.05118\n",
      "regression loss:  2444.0356\n",
      "episode: 2236/1000000, returns: -0.98, epsilon: 0.33\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "regression loss:  900.45306\n",
      "regression loss:  1123.3339\n",
      "episode: 2237/1000000, returns: -5.6, epsilon: 0.33\n",
      "regression loss:  1825.0592\n",
      "regression loss:  2827.4822\n",
      "episode: 2238/1000000, returns: 0.036, epsilon: 0.33\n",
      "regression loss:  1207.6346\n",
      "regression loss:  2933.1414\n",
      "episode: 2239/1000000, returns: 5.0, epsilon: 0.33\n",
      "regression loss:  12289.1875\n",
      "regression loss:  6672.075\n",
      "episode: 2240/1000000, returns: 3.1, epsilon: 0.33\n",
      "regression loss:  5445.7617\n",
      "regression loss:  6572.7627\n",
      "episode: 2241/1000000, returns: -1.3, epsilon: 0.33\n",
      "regression loss:  691.2355\n",
      "regression loss:  543.8123\n",
      "episode: 2242/1000000, returns: -9.7, epsilon: 0.33\n",
      "regression loss:  1367.2213\n",
      "regression loss:  1108.5256\n",
      "episode: 2243/1000000, returns: -1.4, epsilon: 0.33\n",
      "regression loss:  2885.205\n",
      "regression loss:  1655.5046\n",
      "episode: 2244/1000000, returns: 2.0, epsilon: 0.33\n",
      "regression loss:  192.22214\n",
      "regression loss:  118.53584\n",
      "episode: 2245/1000000, returns: 0.49, epsilon: 0.33\n",
      "regression loss:  130.19437\n",
      "regression loss:  221.13138\n",
      "episode: 2246/1000000, returns: -4.8, epsilon: 0.33\n",
      "regression loss:  168.64836\n",
      "regression loss:  162.61555\n",
      "episode: 2247/1000000, returns: 0.086, epsilon: 0.33\n",
      "regression loss:  525.1393\n",
      "regression loss:  239.04774\n",
      "episode: 2248/1000000, returns: 6.2, epsilon: 0.32\n",
      "regression loss:  52.35627\n",
      "regression loss:  170.26266\n",
      "episode: 2249/1000000, returns: 5.8, epsilon: 0.32\n",
      "regression loss:  171.2076\n",
      "regression loss:  38.745266\n",
      "episode: 2250/1000000, returns: 0.54, epsilon: 0.32\n",
      "regression loss:  15.272914\n",
      "regression loss:  326.79318\n",
      "episode: 2251/1000000, returns: 0.95, epsilon: 0.32\n",
      "regression loss:  7.6899276\n",
      "regression loss:  52.30517\n",
      "episode: 2252/1000000, returns: 1.1, epsilon: 0.32\n",
      "regression loss:  94.3219\n",
      "regression loss:  341.2843\n",
      "episode: 2253/1000000, returns: 0.46, epsilon: 0.32\n",
      "regression loss:  115.682045\n",
      "regression loss:  205.4052\n",
      "episode: 2254/1000000, returns: -1.7, epsilon: 0.32\n",
      "regression loss:  62.485264\n",
      "regression loss:  63.11982\n",
      "episode: 2255/1000000, returns: 0.014, epsilon: 0.32\n",
      "regression loss:  28.555084\n",
      "regression loss:  369.91455\n",
      "episode: 2256/1000000, returns: 0.32, epsilon: 0.32\n",
      "regression loss:  4.9276958\n",
      "regression loss:  146.66586\n",
      "episode: 2257/1000000, returns: 2.7, epsilon: 0.32\n",
      "regression loss:  117.316765\n",
      "regression loss:  63.193058\n",
      "episode: 2258/1000000, returns: 1.4, epsilon: 0.32\n",
      "regression loss:  90.884315\n",
      "regression loss:  31.648882\n",
      "episode: 2259/1000000, returns: 1.4, epsilon: 0.32\n",
      "regression loss:  27.7693\n",
      "regression loss:  555.69434\n",
      "episode: 2260/1000000, returns: -2.6, epsilon: 0.32\n",
      "regression loss:  1916.0764\n",
      "regression loss:  1576.5455\n",
      "episode: 2261/1000000, returns: 3.2, epsilon: 0.32\n",
      "regression loss:  826.25507\n",
      "regression loss:  1218.1649\n",
      "episode: 2262/1000000, returns: -2.9, epsilon: 0.32\n",
      "regression loss:  17637.13\n",
      "regression loss:  9505.121\n",
      "episode: 2263/1000000, returns: -0.069, epsilon: 0.32\n",
      "regression loss:  5179.2183\n",
      "regression loss:  2315.6838\n",
      "episode: 2264/1000000, returns: 1.7, epsilon: 0.32\n",
      "regression loss:  2657.9011\n",
      "regression loss:  3015.2378\n",
      "episode: 2265/1000000, returns: -2.4, epsilon: 0.32\n",
      "regression loss:  12631.984\n",
      "regression loss:  6241.957\n",
      "episode: 2266/1000000, returns: -0.16, epsilon: 0.32\n",
      "regression loss:  715.9063\n",
      "regression loss:  4783.0303\n",
      "episode: 2267/1000000, returns: -0.087, epsilon: 0.32\n",
      "regression loss:  2338.716\n",
      "regression loss:  2767.1643\n",
      "episode: 2268/1000000, returns: -4.6, epsilon: 0.32\n",
      "regression loss:  391.48846\n",
      "regression loss:  5197.7188\n",
      "episode: 2269/1000000, returns: -1.3, epsilon: 0.32\n",
      "regression loss:  580.8611\n",
      "regression loss:  2873.317\n",
      "episode: 2270/1000000, returns: -2.7, epsilon: 0.32\n",
      "regression loss:  477.9753\n",
      "regression loss:  1511.229\n",
      "episode: 2271/1000000, returns: 4.1, epsilon: 0.32\n",
      "regression loss:  357.6852\n",
      "regression loss:  2363.856\n",
      "episode: 2272/1000000, returns: -2.8, epsilon: 0.32\n",
      "regression loss:  306.2124\n",
      "regression loss:  1829.9224\n",
      "episode: 2273/1000000, returns: -1.7, epsilon: 0.32\n",
      "regression loss:  108.61689\n",
      "regression loss:  692.3163\n",
      "episode: 2274/1000000, returns: -2.6, epsilon: 0.32\n",
      "regression loss:  66.64098\n",
      "regression loss:  3116.0288\n",
      "episode: 2275/1000000, returns: 4.1, epsilon: 0.32\n",
      "regression loss:  317.27487\n",
      "regression loss:  648.78076\n",
      "episode: 2276/1000000, returns: 2.4, epsilon: 0.32\n",
      "regression loss:  1561.9421\n",
      "regression loss:  3632.7278\n",
      "episode: 2277/1000000, returns: 1.2, epsilon: 0.32\n",
      "regression loss:  751.1457\n",
      "regression loss:  6466.6216\n",
      "episode: 2278/1000000, returns: 1.0, epsilon: 0.32\n",
      "regression loss:  960.8942\n",
      "regression loss:  946.52356\n",
      "episode: 2279/1000000, returns: 3.3, epsilon: 0.32\n",
      "regression loss:  521.4522\n",
      "regression loss:  3328.0132\n",
      "episode: 2280/1000000, returns: 9.1, epsilon: 0.32\n",
      "regression loss:  3901.7402\n",
      "regression loss:  1381.8909\n",
      "episode: 2281/1000000, returns: 2.1, epsilon: 0.32\n",
      "regression loss:  865.2303\n",
      "regression loss:  3800.829\n",
      "episode: 2282/1000000, returns: 1.9, epsilon: 0.32\n",
      "regression loss:  9936.761\n",
      "regression loss:  1548.5564\n",
      "episode: 2283/1000000, returns: 0.3, epsilon: 0.32\n",
      "regression loss:  258.88208\n",
      "regression loss:  3485.189\n",
      "episode: 2284/1000000, returns: -0.85, epsilon: 0.32\n",
      "regression loss:  272.86594\n",
      "regression loss:  2360.38\n",
      "episode: 2285/1000000, returns: 1.5, epsilon: 0.32\n",
      "regression loss:  1622.4873\n",
      "regression loss:  2544.0112\n",
      "episode: 2286/1000000, returns: -2.5, epsilon: 0.32\n",
      "regression loss:  496.15506\n",
      "regression loss:  1710.8561\n",
      "episode: 2287/1000000, returns: 1.4e+01, epsilon: 0.32\n",
      "regression loss:  863.9941\n",
      "regression loss:  1927.9456\n",
      "episode: 2288/1000000, returns: -5.7, epsilon: 0.32\n",
      "regression loss:  845.23016\n",
      "regression loss:  2125.0386\n",
      "episode: 2289/1000000, returns: 9.8, epsilon: 0.32\n",
      "regression loss:  146.8799\n",
      "regression loss:  685.0755\n",
      "episode: 2290/1000000, returns: -2.6, epsilon: 0.32\n",
      "regression loss:  409.67645\n",
      "regression loss:  726.2828\n",
      "episode: 2291/1000000, returns: -1.9, epsilon: 0.32\n",
      "regression loss:  1137.2449\n",
      "regression loss:  2869.7642\n",
      "episode: 2292/1000000, returns: -2.6, epsilon: 0.32\n",
      "regression loss:  749.658\n",
      "regression loss:  1470.5706\n",
      "episode: 2293/1000000, returns: 7.8, epsilon: 0.32\n",
      "regression loss:  1059.1567\n",
      "regression loss:  2009.1731\n",
      "episode: 2294/1000000, returns: 3.8, epsilon: 0.32\n",
      "regression loss:  861.21826\n",
      "regression loss:  2054.57\n",
      "episode: 2295/1000000, returns: -3.2, epsilon: 0.32\n",
      "regression loss:  1027.8649\n",
      "regression loss:  1930.2574\n",
      "episode: 2296/1000000, returns: -6.4, epsilon: 0.32\n",
      "regression loss:  2727.9268\n",
      "regression loss:  4458.383\n",
      "episode: 2297/1000000, returns: 7.9, epsilon: 0.32\n",
      "regression loss:  706.286\n",
      "regression loss:  1149.4813\n",
      "episode: 2298/1000000, returns: 4.2, epsilon: 0.32\n",
      "regression loss:  1613.9688\n",
      "regression loss:  1585.251\n",
      "episode: 2299/1000000, returns: 2.6, epsilon: 0.32\n",
      "regression loss:  166.05087\n",
      "regression loss:  2351.3533\n",
      "episode: 2300/1000000, returns: 0.28, epsilon: 0.32\n",
      "regression loss:  4051.2256\n",
      "regression loss:  3451.1885\n",
      "episode: 2301/1000000, returns: 0.63, epsilon: 0.32\n",
      "regression loss:  3153.3892\n",
      "regression loss:  6041.368\n",
      "episode: 2302/1000000, returns: -1.6, epsilon: 0.32\n",
      "regression loss:  447.1593\n",
      "regression loss:  4995.034\n",
      "episode: 2303/1000000, returns: 0.37, epsilon: 0.32\n",
      "regression loss:  1500.5194\n",
      "regression loss:  2741.3484\n",
      "episode: 2304/1000000, returns: 3.8, epsilon: 0.32\n",
      "regression loss:  1484.1049\n",
      "regression loss:  2672.4595\n",
      "episode: 2305/1000000, returns: -2.0, epsilon: 0.32\n",
      "regression loss:  472.9521\n",
      "regression loss:  1316.7017\n",
      "episode: 2306/1000000, returns: 8.2, epsilon: 0.32\n",
      "regression loss:  2252.1426\n",
      "regression loss:  2546.442\n",
      "episode: 2307/1000000, returns: 0.22, epsilon: 0.32\n",
      "regression loss:  383.00693\n",
      "regression loss:  5069.1987\n",
      "episode: 2308/1000000, returns: 1.4, epsilon: 0.32\n",
      "regression loss:  1952.7738\n",
      "regression loss:  3819.8303\n",
      "episode: 2309/1000000, returns: 4.5, epsilon: 0.32\n",
      "regression loss:  413.95374\n",
      "regression loss:  1782.7405\n",
      "episode: 2310/1000000, returns: -1.5, epsilon: 0.31\n",
      "regression loss:  631.8941\n",
      "regression loss:  3821.69\n",
      "episode: 2311/1000000, returns: -0.37, epsilon: 0.31\n",
      "regression loss:  525.4537\n",
      "regression loss:  2351.6077\n",
      "episode: 2312/1000000, returns: 2.9, epsilon: 0.31\n",
      "regression loss:  172.24042\n",
      "regression loss:  832.9375\n",
      "episode: 2313/1000000, returns: 2.9, epsilon: 0.31\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "regression loss:  909.92566\n",
      "regression loss:  1570.0437\n",
      "episode: 2314/1000000, returns: 3.5, epsilon: 0.31\n",
      "regression loss:  677.2933\n",
      "regression loss:  1507.6116\n",
      "episode: 2315/1000000, returns: -0.19, epsilon: 0.31\n",
      "regression loss:  701.15875\n",
      "regression loss:  1812.9933\n",
      "episode: 2316/1000000, returns: 2.6, epsilon: 0.31\n",
      "regression loss:  2689.164\n",
      "regression loss:  4633.9585\n",
      "episode: 2317/1000000, returns: -0.12, epsilon: 0.31\n",
      "regression loss:  280.56848\n",
      "regression loss:  3633.8877\n",
      "episode: 2318/1000000, returns: -2.7, epsilon: 0.31\n",
      "regression loss:  5965.149\n",
      "regression loss:  4409.5234\n",
      "episode: 2319/1000000, returns: 6.9, epsilon: 0.31\n",
      "regression loss:  279.5891\n",
      "regression loss:  4556.1064\n",
      "episode: 2320/1000000, returns: 0.069, epsilon: 0.31\n",
      "regression loss:  717.3779\n",
      "regression loss:  1087.0958\n",
      "episode: 2321/1000000, returns: -0.084, epsilon: 0.31\n",
      "regression loss:  536.41675\n",
      "regression loss:  4322.485\n",
      "episode: 2322/1000000, returns: 9.7, epsilon: 0.31\n",
      "regression loss:  223.91066\n",
      "regression loss:  367.9799\n",
      "episode: 2323/1000000, returns: 0.18, epsilon: 0.31\n",
      "regression loss:  22.185476\n",
      "regression loss:  1283.2859\n",
      "episode: 2324/1000000, returns: -0.31, epsilon: 0.31\n",
      "regression loss:  611.47577\n",
      "regression loss:  441.05505\n",
      "episode: 2325/1000000, returns: 1.6, epsilon: 0.31\n",
      "regression loss:  576.8661\n",
      "regression loss:  373.53204\n",
      "episode: 2326/1000000, returns: -0.53, epsilon: 0.31\n",
      "regression loss:  20.87103\n",
      "regression loss:  1723.3369\n",
      "episode: 2327/1000000, returns: 3.7, epsilon: 0.31\n",
      "regression loss:  1397.0453\n",
      "regression loss:  4997.8735\n",
      "episode: 2328/1000000, returns: 0.85, epsilon: 0.31\n",
      "regression loss:  1800.8358\n",
      "regression loss:  1941.114\n",
      "episode: 2329/1000000, returns: 3.3, epsilon: 0.31\n",
      "regression loss:  1498.9945\n",
      "regression loss:  987.60846\n",
      "episode: 2330/1000000, returns: 2.5, epsilon: 0.31\n",
      "regression loss:  798.7146\n",
      "regression loss:  1343.1157\n",
      "episode: 2331/1000000, returns: 0.2, epsilon: 0.31\n",
      "regression loss:  1788.5299\n",
      "regression loss:  1495.406\n",
      "episode: 2332/1000000, returns: -0.022, epsilon: 0.31\n",
      "regression loss:  1233.5735\n",
      "regression loss:  1578.5652\n",
      "episode: 2333/1000000, returns: 9.1, epsilon: 0.31\n",
      "regression loss:  557.5519\n",
      "regression loss:  1279.1466\n",
      "episode: 2334/1000000, returns: -0.37, epsilon: 0.31\n",
      "regression loss:  1065.984\n",
      "regression loss:  1953.732\n",
      "episode: 2335/1000000, returns: 0.42, epsilon: 0.31\n",
      "regression loss:  327.90967\n",
      "regression loss:  314.01666\n",
      "episode: 2336/1000000, returns: -4.9, epsilon: 0.31\n",
      "regression loss:  160.88087\n",
      "regression loss:  2483.927\n",
      "episode: 2337/1000000, returns: 3.4, epsilon: 0.31\n",
      "regression loss:  627.82715\n",
      "regression loss:  191.00656\n",
      "episode: 2338/1000000, returns: 3.3, epsilon: 0.31\n",
      "regression loss:  45.32746\n",
      "regression loss:  209.23775\n",
      "episode: 2339/1000000, returns: -0.24, epsilon: 0.31\n",
      "regression loss:  105.99408\n",
      "regression loss:  63.881683\n",
      "episode: 2340/1000000, returns: -0.31, epsilon: 0.31\n",
      "regression loss:  11.811344\n",
      "regression loss:  6024.651\n",
      "episode: 2341/1000000, returns: 0.55, epsilon: 0.31\n",
      "regression loss:  1060.5579\n",
      "regression loss:  408.59476\n",
      "episode: 2342/1000000, returns: 2.9, epsilon: 0.31\n",
      "regression loss:  1244.9388\n",
      "regression loss:  3901.9167\n",
      "episode: 2343/1000000, returns: -0.18, epsilon: 0.31\n",
      "regression loss:  1172.4595\n",
      "regression loss:  1244.836\n",
      "episode: 2344/1000000, returns: -4.6, epsilon: 0.31\n",
      "regression loss:  5073.145\n",
      "regression loss:  6496.6787\n",
      "episode: 2345/1000000, returns: 0.83, epsilon: 0.31\n",
      "regression loss:  3626.4778\n",
      "regression loss:  2804.871\n",
      "episode: 2346/1000000, returns: 0.84, epsilon: 0.31\n",
      "regression loss:  877.99774\n",
      "regression loss:  3874.205\n",
      "episode: 2347/1000000, returns: -6.1, epsilon: 0.31\n",
      "regression loss:  1006.9274\n",
      "regression loss:  2071.1768\n",
      "episode: 2348/1000000, returns: 1.5, epsilon: 0.31\n",
      "regression loss:  251.77641\n",
      "regression loss:  929.799\n",
      "episode: 2349/1000000, returns: 1.7, epsilon: 0.31\n",
      "regression loss:  2263.7239\n",
      "regression loss:  3567.951\n",
      "episode: 2350/1000000, returns: 6.6, epsilon: 0.31\n",
      "regression loss:  1278.1725\n",
      "regression loss:  2936.0303\n",
      "episode: 2351/1000000, returns: -2.3, epsilon: 0.31\n",
      "regression loss:  308.29425\n",
      "regression loss:  252.99335\n",
      "episode: 2352/1000000, returns: -6.3, epsilon: 0.31\n",
      "regression loss:  646.73865\n",
      "regression loss:  1979.8909\n",
      "episode: 2353/1000000, returns: 1.7, epsilon: 0.31\n",
      "regression loss:  726.4677\n",
      "regression loss:  1081.6204\n",
      "episode: 2354/1000000, returns: 2.5, epsilon: 0.31\n",
      "regression loss:  2097.7686\n",
      "regression loss:  2145.9834\n",
      "episode: 2355/1000000, returns: -4.0, epsilon: 0.31\n",
      "regression loss:  102.755424\n",
      "regression loss:  5225.382\n",
      "episode: 2356/1000000, returns: -3.1, epsilon: 0.31\n",
      "regression loss:  376.9657\n",
      "regression loss:  4445.479\n",
      "episode: 2357/1000000, returns: -4.6, epsilon: 0.31\n",
      "regression loss:  2120.3528\n",
      "regression loss:  613.5767\n",
      "episode: 2358/1000000, returns: -0.014, epsilon: 0.31\n",
      "regression loss:  717.26746\n",
      "regression loss:  683.28845\n",
      "episode: 2359/1000000, returns: 0.32, epsilon: 0.31\n",
      "regression loss:  297.50623\n",
      "regression loss:  183.94022\n",
      "episode: 2360/1000000, returns: 1e+01, epsilon: 0.31\n",
      "regression loss:  249.49892\n",
      "regression loss:  1787.2465\n",
      "episode: 2361/1000000, returns: -9.6, epsilon: 0.31\n",
      "regression loss:  1463.3059\n",
      "regression loss:  654.49\n",
      "episode: 2362/1000000, returns: 1.7, epsilon: 0.31\n",
      "regression loss:  42.01706\n",
      "regression loss:  89.82526\n",
      "episode: 2363/1000000, returns: -1.8, epsilon: 0.31\n",
      "regression loss:  5.842218\n",
      "regression loss:  676.4161\n",
      "episode: 2364/1000000, returns: 1.9, epsilon: 0.31\n",
      "regression loss:  188.69037\n",
      "regression loss:  871.8417\n",
      "episode: 2365/1000000, returns: 0.47, epsilon: 0.31\n",
      "regression loss:  372.1878\n",
      "regression loss:  907.51904\n",
      "episode: 2366/1000000, returns: 0.85, epsilon: 0.31\n",
      "regression loss:  2260.4014\n",
      "regression loss:  253.01149\n",
      "episode: 2367/1000000, returns: 0.24, epsilon: 0.31\n",
      "regression loss:  1095.51\n",
      "regression loss:  629.9479\n",
      "episode: 2368/1000000, returns: -0.12, epsilon: 0.31\n",
      "regression loss:  1277.6139\n",
      "regression loss:  305.46616\n",
      "episode: 2369/1000000, returns: -6.8, epsilon: 0.31\n",
      "regression loss:  444.99548\n",
      "regression loss:  233.23688\n",
      "episode: 2370/1000000, returns: 0.47, epsilon: 0.31\n",
      "regression loss:  34.668545\n",
      "regression loss:  163.21819\n",
      "episode: 2371/1000000, returns: 4.8, epsilon: 0.31\n",
      "regression loss:  840.69965\n",
      "regression loss:  1474.789\n",
      "episode: 2372/1000000, returns: 0.81, epsilon: 0.31\n",
      "regression loss:  965.2321\n",
      "regression loss:  112.00572\n",
      "episode: 2373/1000000, returns: 1.3, epsilon: 0.31\n",
      "regression loss:  338.23358\n",
      "regression loss:  2800.7239\n",
      "episode: 2374/1000000, returns: -0.43, epsilon: 0.31\n",
      "regression loss:  4351.2876\n",
      "regression loss:  2886.52\n",
      "episode: 2375/1000000, returns: 0.93, epsilon: 0.3\n",
      "regression loss:  427.07233\n",
      "regression loss:  350.86917\n",
      "episode: 2376/1000000, returns: -6.5, epsilon: 0.3\n",
      "regression loss:  6891.313\n",
      "regression loss:  15155.13\n",
      "episode: 2377/1000000, returns: -0.77, epsilon: 0.3\n",
      "regression loss:  5817.9795\n",
      "regression loss:  6171.318\n",
      "episode: 2378/1000000, returns: 1.3, epsilon: 0.3\n",
      "regression loss:  2432.1416\n",
      "regression loss:  7210.042\n",
      "episode: 2379/1000000, returns: 1.4, epsilon: 0.3\n",
      "regression loss:  8246.386\n",
      "regression loss:  3840.3713\n",
      "episode: 2380/1000000, returns: -1.3, epsilon: 0.3\n",
      "regression loss:  1181.9843\n",
      "regression loss:  7322.649\n",
      "episode: 2381/1000000, returns: 0.56, epsilon: 0.3\n",
      "regression loss:  6501.964\n",
      "regression loss:  12262.297\n",
      "episode: 2382/1000000, returns: -1.4, epsilon: 0.3\n",
      "regression loss:  4634.7334\n",
      "regression loss:  5971.5054\n",
      "episode: 2383/1000000, returns: 1.9, epsilon: 0.3\n",
      "regression loss:  1654.1743\n",
      "regression loss:  15194.842\n",
      "episode: 2384/1000000, returns: -0.53, epsilon: 0.3\n",
      "regression loss:  6805.603\n",
      "regression loss:  8458.324\n",
      "episode: 2385/1000000, returns: -2.8, epsilon: 0.3\n",
      "regression loss:  3064.0876\n",
      "regression loss:  6924.237\n",
      "episode: 2386/1000000, returns: 1.4e+01, epsilon: 0.3\n",
      "regression loss:  5054.498\n",
      "regression loss:  12548.982\n",
      "episode: 2387/1000000, returns: 0.81, epsilon: 0.3\n",
      "regression loss:  2862.6558\n",
      "regression loss:  8934.566\n",
      "episode: 2388/1000000, returns: -0.62, epsilon: 0.3\n",
      "regression loss:  1081.5557\n",
      "regression loss:  4721.443\n",
      "episode: 2389/1000000, returns: -1.2e+01, epsilon: 0.3\n",
      "regression loss:  610.7478\n",
      "regression loss:  4194.0703\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 2390/1000000, returns: -0.16, epsilon: 0.3\n",
      "regression loss:  1678.7751\n",
      "regression loss:  4216.2017\n",
      "episode: 2391/1000000, returns: 3.5, epsilon: 0.3\n",
      "regression loss:  330.98132\n",
      "regression loss:  3701.9712\n",
      "episode: 2392/1000000, returns: -2.5, epsilon: 0.3\n",
      "regression loss:  288.67117\n",
      "regression loss:  2741.044\n",
      "episode: 2393/1000000, returns: -4.3, epsilon: 0.3\n",
      "regression loss:  409.6195\n",
      "regression loss:  2045.6774\n",
      "episode: 2394/1000000, returns: -6.4, epsilon: 0.3\n",
      "regression loss:  137.60379\n",
      "regression loss:  1748.3079\n",
      "episode: 2395/1000000, returns: -3.3, epsilon: 0.3\n",
      "regression loss:  442.97562\n",
      "regression loss:  1199.3964\n",
      "episode: 2396/1000000, returns: 0.63, epsilon: 0.3\n",
      "regression loss:  188.22717\n",
      "regression loss:  998.4763\n",
      "episode: 2397/1000000, returns: -0.0083, epsilon: 0.3\n",
      "regression loss:  175.32928\n",
      "regression loss:  1425.5417\n",
      "episode: 2398/1000000, returns: 0.71, epsilon: 0.3\n",
      "regression loss:  1369.0986\n",
      "regression loss:  2032.8031\n",
      "episode: 2399/1000000, returns: 3.0, epsilon: 0.3\n",
      "regression loss:  3282.6174\n",
      "regression loss:  4677.9365\n",
      "episode: 2400/1000000, returns: -0.74, epsilon: 0.3\n",
      "regression loss:  997.9928\n",
      "regression loss:  3141.7056\n",
      "episode: 2401/1000000, returns: -4.9, epsilon: 0.3\n",
      "regression loss:  8157.7363\n",
      "regression loss:  7712.3096\n",
      "episode: 2402/1000000, returns: -0.086, epsilon: 0.3\n",
      "regression loss:  5276.6494\n",
      "regression loss:  4545.7925\n",
      "episode: 2403/1000000, returns: 0.56, epsilon: 0.3\n",
      "regression loss:  1336.7814\n",
      "regression loss:  3176.3865\n",
      "episode: 2404/1000000, returns: 1.3, epsilon: 0.3\n",
      "regression loss:  382.46576\n",
      "regression loss:  3629.7388\n",
      "episode: 2405/1000000, returns: 0.056, epsilon: 0.3\n",
      "regression loss:  569.7109\n",
      "regression loss:  929.66437\n",
      "episode: 2406/1000000, returns: -0.56, epsilon: 0.3\n",
      "regression loss:  1282.0925\n",
      "regression loss:  1892.7418\n",
      "episode: 2407/1000000, returns: 0.63, epsilon: 0.3\n",
      "regression loss:  133.83945\n",
      "regression loss:  2050.381\n",
      "episode: 2408/1000000, returns: 0.11, epsilon: 0.3\n",
      "regression loss:  1404.5767\n",
      "regression loss:  3199.1372\n",
      "episode: 2409/1000000, returns: -2.2, epsilon: 0.3\n",
      "regression loss:  4591.203\n",
      "regression loss:  7323.4844\n",
      "episode: 2410/1000000, returns: -0.074, epsilon: 0.3\n",
      "regression loss:  1694.1283\n",
      "regression loss:  4849.044\n",
      "episode: 2411/1000000, returns: 0.021, epsilon: 0.3\n",
      "regression loss:  618.94635\n",
      "regression loss:  12449.371\n",
      "episode: 2412/1000000, returns: 0.98, epsilon: 0.3\n",
      "regression loss:  16652.018\n",
      "regression loss:  10758.56\n",
      "episode: 2413/1000000, returns: -1.8, epsilon: 0.3\n",
      "regression loss:  4186.627\n",
      "regression loss:  1354.7322\n",
      "episode: 2414/1000000, returns: 9.6, epsilon: 0.3\n",
      "regression loss:  664.97076\n",
      "regression loss:  9600.838\n",
      "episode: 2415/1000000, returns: 0.61, epsilon: 0.3\n",
      "regression loss:  9069.68\n",
      "regression loss:  8675.075\n",
      "episode: 2416/1000000, returns: 0.51, epsilon: 0.3\n",
      "regression loss:  1111.0135\n",
      "regression loss:  8490.029\n",
      "episode: 2417/1000000, returns: 3.5, epsilon: 0.3\n",
      "regression loss:  4763.927\n",
      "regression loss:  6915.0835\n",
      "episode: 2418/1000000, returns: -0.42, epsilon: 0.3\n",
      "regression loss:  13187.655\n",
      "regression loss:  7089.6245\n",
      "episode: 2419/1000000, returns: -0.6, epsilon: 0.3\n",
      "regression loss:  9559.289\n",
      "regression loss:  6165.766\n",
      "episode: 2420/1000000, returns: 1.8, epsilon: 0.3\n",
      "regression loss:  10994.502\n",
      "regression loss:  8690.042\n",
      "episode: 2421/1000000, returns: -1.1, epsilon: 0.3\n",
      "regression loss:  3166.9302\n",
      "regression loss:  1931.0913\n",
      "episode: 2422/1000000, returns: -0.2, epsilon: 0.3\n",
      "regression loss:  1368.6089\n",
      "regression loss:  10374.525\n",
      "episode: 2423/1000000, returns: 1.5, epsilon: 0.3\n",
      "regression loss:  2996.313\n",
      "regression loss:  4064.0142\n",
      "episode: 2424/1000000, returns: -0.92, epsilon: 0.3\n",
      "regression loss:  3519.382\n",
      "regression loss:  5954.9424\n",
      "episode: 2425/1000000, returns: 0.45, epsilon: 0.3\n",
      "regression loss:  1468.7147\n",
      "regression loss:  3591.9487\n",
      "episode: 2426/1000000, returns: -3.7, epsilon: 0.3\n",
      "regression loss:  4241.462\n",
      "regression loss:  8842.976\n",
      "episode: 2427/1000000, returns: 2.6, epsilon: 0.3\n",
      "regression loss:  11659.235\n",
      "regression loss:  7866.216\n",
      "episode: 2428/1000000, returns: 2.6, epsilon: 0.3\n",
      "regression loss:  3693.6157\n",
      "regression loss:  1342.6746\n",
      "episode: 2429/1000000, returns: 1.3e+01, epsilon: 0.3\n",
      "regression loss:  1252.9648\n",
      "regression loss:  1761.2706\n",
      "episode: 2430/1000000, returns: 2.1, epsilon: 0.3\n",
      "regression loss:  1570.3125\n",
      "regression loss:  3734.5217\n",
      "episode: 2431/1000000, returns: -4.7, epsilon: 0.3\n",
      "regression loss:  1117.4321\n",
      "regression loss:  4537.3237\n",
      "episode: 2432/1000000, returns: 0.55, epsilon: 0.3\n",
      "regression loss:  215.58723\n",
      "regression loss:  2847.2153\n",
      "episode: 2433/1000000, returns: -0.098, epsilon: 0.3\n",
      "regression loss:  591.6621\n",
      "regression loss:  1787.7517\n",
      "episode: 2434/1000000, returns: 0.96, epsilon: 0.3\n",
      "regression loss:  204.34033\n",
      "regression loss:  2355.229\n",
      "episode: 2435/1000000, returns: -2.9, epsilon: 0.3\n",
      "regression loss:  152.11806\n",
      "regression loss:  1433.6512\n",
      "episode: 2436/1000000, returns: 0.27, epsilon: 0.3\n",
      "regression loss:  85.151024\n",
      "regression loss:  1652.9761\n",
      "episode: 2437/1000000, returns: 2.9, epsilon: 0.3\n",
      "regression loss:  211.38245\n",
      "regression loss:  1538.4866\n",
      "episode: 2438/1000000, returns: 0.51, epsilon: 0.3\n",
      "regression loss:  119.6565\n",
      "regression loss:  1383.3756\n",
      "episode: 2439/1000000, returns: 1.8, epsilon: 0.3\n",
      "regression loss:  744.39746\n",
      "regression loss:  782.19885\n",
      "episode: 2440/1000000, returns: -1.4, epsilon: 0.3\n",
      "regression loss:  309.0807\n",
      "regression loss:  2459.4822\n",
      "episode: 2441/1000000, returns: -2.6, epsilon: 0.29\n",
      "regression loss:  1826.0815\n",
      "regression loss:  1713.9575\n",
      "episode: 2442/1000000, returns: 0.8, epsilon: 0.29\n",
      "regression loss:  649.0838\n",
      "regression loss:  8062.248\n",
      "episode: 2443/1000000, returns: -2.4, epsilon: 0.29\n",
      "regression loss:  2340.556\n",
      "regression loss:  1642.0043\n",
      "episode: 2444/1000000, returns: -1.6, epsilon: 0.29\n",
      "regression loss:  650.98553\n",
      "regression loss:  1317.6113\n",
      "episode: 2445/1000000, returns: -6.2, epsilon: 0.29\n",
      "regression loss:  181.45355\n",
      "regression loss:  5766.45\n",
      "episode: 2446/1000000, returns: 0.15, epsilon: 0.29\n",
      "regression loss:  2285.3313\n",
      "regression loss:  1759.6667\n",
      "episode: 2447/1000000, returns: -2.6, epsilon: 0.29\n",
      "regression loss:  382.8683\n",
      "regression loss:  1590.8115\n",
      "episode: 2448/1000000, returns: 2.5, epsilon: 0.29\n",
      "regression loss:  358.3175\n",
      "regression loss:  2608.3552\n",
      "episode: 2449/1000000, returns: 0.95, epsilon: 0.29\n",
      "regression loss:  1212.6442\n",
      "regression loss:  656.01935\n",
      "episode: 2450/1000000, returns: 7.5, epsilon: 0.29\n",
      "regression loss:  934.377\n",
      "regression loss:  1742.1437\n",
      "episode: 2451/1000000, returns: 1.2, epsilon: 0.29\n",
      "regression loss:  848.8838\n",
      "regression loss:  3319.218\n",
      "episode: 2452/1000000, returns: 0.68, epsilon: 0.29\n",
      "regression loss:  308.40857\n",
      "regression loss:  4474.0205\n",
      "episode: 2453/1000000, returns: 5.0, epsilon: 0.29\n",
      "regression loss:  447.09238\n",
      "regression loss:  430.78156\n",
      "episode: 2454/1000000, returns: -1.9, epsilon: 0.29\n",
      "regression loss:  2014.1483\n",
      "regression loss:  1224.9375\n",
      "episode: 2455/1000000, returns: -4.8, epsilon: 0.29\n",
      "regression loss:  248.25555\n",
      "regression loss:  1582.9541\n",
      "episode: 2456/1000000, returns: 5.2, epsilon: 0.29\n",
      "regression loss:  2024.8052\n",
      "regression loss:  1219.656\n",
      "episode: 2457/1000000, returns: -0.97, epsilon: 0.29\n",
      "regression loss:  4128.1523\n",
      "regression loss:  2060.724\n",
      "episode: 2458/1000000, returns: 2.8, epsilon: 0.29\n",
      "regression loss:  2918.1855\n",
      "regression loss:  1327.2563\n",
      "episode: 2459/1000000, returns: -1.5, epsilon: 0.29\n",
      "regression loss:  737.07916\n",
      "regression loss:  3883.8765\n",
      "episode: 2460/1000000, returns: 0.58, epsilon: 0.29\n",
      "regression loss:  10918.484\n",
      "regression loss:  10031.184\n",
      "episode: 2461/1000000, returns: 0.98, epsilon: 0.29\n",
      "regression loss:  3120.75\n",
      "regression loss:  1996.5378\n",
      "episode: 2462/1000000, returns: -1.0, epsilon: 0.29\n",
      "regression loss:  3450.718\n",
      "regression loss:  1216.314\n",
      "episode: 2463/1000000, returns: -5.5, epsilon: 0.29\n",
      "regression loss:  3576.995\n",
      "regression loss:  3614.9568\n",
      "episode: 2464/1000000, returns: -0.98, epsilon: 0.29\n",
      "regression loss:  410.33112\n",
      "regression loss:  4293.3687\n",
      "episode: 2465/1000000, returns: 0.069, epsilon: 0.29\n",
      "regression loss:  2028.0282\n",
      "regression loss:  11071.575\n",
      "episode: 2466/1000000, returns: 4.4, epsilon: 0.29\n",
      "regression loss:  3383.4993\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "regression loss:  4349.736\n",
      "episode: 2467/1000000, returns: 3.4, epsilon: 0.29\n",
      "regression loss:  200.85695\n",
      "regression loss:  1329.2698\n",
      "episode: 2468/1000000, returns: -0.0095, epsilon: 0.29\n",
      "regression loss:  1467.9684\n",
      "regression loss:  4979.157\n",
      "episode: 2469/1000000, returns: 1.5, epsilon: 0.29\n",
      "regression loss:  3396.956\n",
      "regression loss:  3222.9492\n",
      "episode: 2470/1000000, returns: -1.0, epsilon: 0.29\n",
      "regression loss:  1434.2152\n",
      "regression loss:  4926.868\n",
      "episode: 2471/1000000, returns: 0.42, epsilon: 0.29\n",
      "regression loss:  782.7874\n",
      "regression loss:  1371.0533\n",
      "episode: 2472/1000000, returns: 3.2, epsilon: 0.29\n",
      "regression loss:  270.43192\n",
      "regression loss:  4740.7886\n",
      "episode: 2473/1000000, returns: -1.5, epsilon: 0.29\n",
      "regression loss:  1615.9747\n",
      "regression loss:  2367.6338\n",
      "episode: 2474/1000000, returns: -0.19, epsilon: 0.29\n",
      "regression loss:  1788.6962\n",
      "regression loss:  6573.151\n",
      "episode: 2475/1000000, returns: -5.3, epsilon: 0.29\n",
      "regression loss:  1022.4291\n",
      "regression loss:  2001.7996\n",
      "episode: 2476/1000000, returns: 1e+01, epsilon: 0.29\n",
      "regression loss:  2151.2412\n",
      "regression loss:  2746.7578\n",
      "episode: 2477/1000000, returns: 0.18, epsilon: 0.29\n",
      "regression loss:  522.32324\n",
      "regression loss:  9019.534\n",
      "episode: 2478/1000000, returns: 0.89, epsilon: 0.29\n",
      "regression loss:  2370.4832\n",
      "regression loss:  3442.8184\n",
      "episode: 2479/1000000, returns: -0.33, epsilon: 0.29\n",
      "regression loss:  2278.7346\n",
      "regression loss:  954.7266\n",
      "episode: 2480/1000000, returns: 4.7, epsilon: 0.29\n",
      "regression loss:  355.41583\n",
      "regression loss:  1898.8213\n",
      "episode: 2481/1000000, returns: 3.1, epsilon: 0.29\n",
      "regression loss:  3296.9402\n",
      "regression loss:  3582.3298\n",
      "episode: 2482/1000000, returns: -2.0, epsilon: 0.29\n",
      "regression loss:  1314.5149\n",
      "regression loss:  2302.5518\n",
      "episode: 2483/1000000, returns: -3.0, epsilon: 0.29\n",
      "regression loss:  1090.1729\n",
      "regression loss:  3056.4756\n",
      "episode: 2484/1000000, returns: 1.5, epsilon: 0.29\n",
      "regression loss:  1182.9937\n",
      "regression loss:  3111.4844\n",
      "episode: 2485/1000000, returns: -4.7, epsilon: 0.29\n",
      "regression loss:  366.30896\n",
      "regression loss:  3215.7659\n",
      "episode: 2486/1000000, returns: 0.47, epsilon: 0.29\n",
      "regression loss:  1066.6704\n",
      "regression loss:  2286.7095\n",
      "episode: 2487/1000000, returns: 3.1, epsilon: 0.29\n",
      "regression loss:  333.39502\n",
      "regression loss:  1835.6534\n",
      "episode: 2488/1000000, returns: 3.1, epsilon: 0.29\n",
      "regression loss:  688.97534\n",
      "regression loss:  2124.8594\n",
      "episode: 2489/1000000, returns: -4.8, epsilon: 0.29\n",
      "regression loss:  757.1936\n",
      "regression loss:  1711.3582\n",
      "episode: 2490/1000000, returns: -1.6, epsilon: 0.29\n",
      "regression loss:  184.2738\n",
      "regression loss:  1842.1794\n",
      "episode: 2491/1000000, returns: -3.5, epsilon: 0.29\n",
      "regression loss:  1453.3973\n",
      "regression loss:  1075.9851\n",
      "episode: 2492/1000000, returns: 0.9, epsilon: 0.29\n",
      "regression loss:  1354.2509\n",
      "regression loss:  2072.6226\n",
      "episode: 2493/1000000, returns: 2.2, epsilon: 0.29\n",
      "regression loss:  227.55048\n",
      "regression loss:  1448.8293\n",
      "episode: 2494/1000000, returns: -6.1, epsilon: 0.29\n",
      "regression loss:  3891.56\n",
      "regression loss:  2159.2388\n",
      "episode: 2495/1000000, returns: 3.0, epsilon: 0.29\n",
      "regression loss:  919.54926\n",
      "regression loss:  3680.4023\n",
      "episode: 2496/1000000, returns: 0.32, epsilon: 0.29\n",
      "regression loss:  738.06366\n",
      "regression loss:  1223.9554\n",
      "episode: 2497/1000000, returns: -5.6, epsilon: 0.29\n",
      "regression loss:  782.6352\n",
      "regression loss:  3116.3376\n",
      "episode: 2498/1000000, returns: 0.083, epsilon: 0.29\n",
      "regression loss:  2863.6316\n",
      "regression loss:  3170.218\n",
      "episode: 2499/1000000, returns: 0.51, epsilon: 0.29\n",
      "regression loss:  678.9727\n",
      "regression loss:  7856.8447\n",
      "episode: 2500/1000000, returns: -5.1, epsilon: 0.29\n",
      "regression loss:  367.55563\n",
      "regression loss:  910.78314\n",
      "episode: 2501/1000000, returns: -1.4, epsilon: 0.29\n",
      "regression loss:  1619.1509\n",
      "regression loss:  6989.5566\n",
      "episode: 2502/1000000, returns: 0.42, epsilon: 0.29\n",
      "regression loss:  2047.5006\n",
      "regression loss:  8899.862\n",
      "episode: 2503/1000000, returns: 1.3, epsilon: 0.29\n",
      "regression loss:  7474.706\n",
      "regression loss:  12700.915\n",
      "episode: 2504/1000000, returns: -0.25, epsilon: 0.29\n",
      "regression loss:  5853.6294\n",
      "regression loss:  4787.431\n",
      "episode: 2505/1000000, returns: -0.37, epsilon: 0.29\n",
      "regression loss:  297.05734\n",
      "regression loss:  3407.5117\n",
      "episode: 2506/1000000, returns: 0.57, epsilon: 0.29\n",
      "regression loss:  840.2979\n",
      "regression loss:  2001.1619\n",
      "episode: 2507/1000000, returns: -0.8, epsilon: 0.29\n",
      "regression loss:  2314.9841\n",
      "regression loss:  3280.6936\n",
      "episode: 2508/1000000, returns: 1.1, epsilon: 0.29\n",
      "regression loss:  835.9395\n",
      "regression loss:  2623.2842\n",
      "episode: 2509/1000000, returns: 2.3, epsilon: 0.29\n",
      "regression loss:  1099.4991\n",
      "regression loss:  1871.1143\n",
      "episode: 2510/1000000, returns: -1.9, epsilon: 0.28\n",
      "regression loss:  703.7448\n",
      "regression loss:  518.71796\n",
      "episode: 2511/1000000, returns: 3.8, epsilon: 0.28\n",
      "regression loss:  812.4495\n",
      "regression loss:  355.6374\n",
      "episode: 2512/1000000, returns: -2.6, epsilon: 0.28\n",
      "regression loss:  2210.0925\n",
      "regression loss:  1987.0773\n",
      "episode: 2513/1000000, returns: 6.9, epsilon: 0.28\n",
      "regression loss:  1341.3773\n",
      "regression loss:  2327.1443\n",
      "episode: 2514/1000000, returns: -2.8, epsilon: 0.28\n",
      "regression loss:  831.48956\n",
      "regression loss:  1651.3364\n",
      "episode: 2515/1000000, returns: -0.53, epsilon: 0.28\n",
      "regression loss:  6007.447\n",
      "regression loss:  2403.6396\n",
      "episode: 2516/1000000, returns: 0.22, epsilon: 0.28\n",
      "regression loss:  621.20795\n",
      "regression loss:  4677.1577\n",
      "episode: 2517/1000000, returns: 4.1, epsilon: 0.28\n",
      "regression loss:  5605.698\n",
      "regression loss:  8752.658\n",
      "episode: 2518/1000000, returns: -0.29, epsilon: 0.28\n",
      "regression loss:  1696.3989\n",
      "regression loss:  1015.8662\n",
      "episode: 2519/1000000, returns: -3.7, epsilon: 0.28\n",
      "regression loss:  150.4418\n",
      "regression loss:  5109.1514\n",
      "episode: 2520/1000000, returns: -0.35, epsilon: 0.28\n",
      "regression loss:  3145.119\n",
      "regression loss:  4039.1892\n",
      "episode: 2521/1000000, returns: -0.21, epsilon: 0.28\n",
      "regression loss:  1149.1401\n",
      "regression loss:  2339.2146\n",
      "episode: 2522/1000000, returns: 0.097, epsilon: 0.28\n",
      "regression loss:  501.31058\n",
      "regression loss:  3927.4236\n",
      "episode: 2523/1000000, returns: 2.5, epsilon: 0.28\n",
      "regression loss:  7094.55\n",
      "regression loss:  6867.882\n",
      "episode: 2524/1000000, returns: 3.7, epsilon: 0.28\n",
      "regression loss:  2313.2449\n",
      "regression loss:  2909.6533\n",
      "episode: 2525/1000000, returns: 2.6, epsilon: 0.28\n",
      "regression loss:  4888.191\n",
      "regression loss:  2846.22\n",
      "episode: 2526/1000000, returns: 2.3, epsilon: 0.28\n",
      "regression loss:  5056.9507\n",
      "regression loss:  2770.3154\n",
      "episode: 2527/1000000, returns: -6.3, epsilon: 0.28\n",
      "regression loss:  1816.0874\n",
      "regression loss:  3830.775\n",
      "episode: 2528/1000000, returns: -3.0, epsilon: 0.28\n",
      "regression loss:  1553.8136\n",
      "regression loss:  717.55536\n",
      "episode: 2529/1000000, returns: -0.14, epsilon: 0.28\n",
      "regression loss:  253.39523\n",
      "regression loss:  1816.2224\n",
      "episode: 2530/1000000, returns: 7.8, epsilon: 0.28\n",
      "regression loss:  431.2632\n",
      "regression loss:  1083.196\n",
      "episode: 2531/1000000, returns: 5.0, epsilon: 0.28\n",
      "regression loss:  824.0825\n",
      "regression loss:  3622.8633\n",
      "episode: 2532/1000000, returns: 8.9, epsilon: 0.28\n",
      "regression loss:  309.05215\n",
      "regression loss:  1141.9069\n",
      "episode: 2533/1000000, returns: -2.8, epsilon: 0.28\n",
      "regression loss:  333.5911\n",
      "regression loss:  614.99927\n",
      "episode: 2534/1000000, returns: -4.5, epsilon: 0.28\n",
      "regression loss:  238.41669\n",
      "regression loss:  1078.2477\n",
      "episode: 2535/1000000, returns: 2.4, epsilon: 0.28\n",
      "regression loss:  817.17847\n",
      "regression loss:  1161.4053\n",
      "episode: 2536/1000000, returns: -0.4, epsilon: 0.28\n",
      "regression loss:  1614.5267\n",
      "regression loss:  3437.3804\n",
      "episode: 2537/1000000, returns: 1.1e+01, epsilon: 0.28\n",
      "regression loss:  561.94086\n",
      "regression loss:  1288.7122\n",
      "episode: 2538/1000000, returns: -1.6e+01, epsilon: 0.28\n",
      "regression loss:  240.26384\n",
      "regression loss:  3237.5955\n",
      "episode: 2539/1000000, returns: 3.3, epsilon: 0.28\n",
      "regression loss:  2799.0188\n",
      "regression loss:  4526.8975\n",
      "episode: 2540/1000000, returns: -4.7, epsilon: 0.28\n",
      "regression loss:  1298.5024\n",
      "regression loss:  5170.43\n",
      "episode: 2541/1000000, returns: 4.9, epsilon: 0.28\n",
      "regression loss:  340.39117\n",
      "regression loss:  501.1369\n",
      "episode: 2542/1000000, returns: 0.59, epsilon: 0.28\n",
      "regression loss:  229.65857\n",
      "regression loss:  789.63617\n",
      "episode: 2543/1000000, returns: 1.1, epsilon: 0.28\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "regression loss:  651.5874\n",
      "regression loss:  3238.9353\n",
      "episode: 2544/1000000, returns: -0.62, epsilon: 0.28\n",
      "regression loss:  1964.5763\n",
      "regression loss:  1002.83765\n",
      "episode: 2545/1000000, returns: -7.8, epsilon: 0.28\n",
      "regression loss:  2578.1216\n",
      "regression loss:  3223.4314\n",
      "episode: 2546/1000000, returns: -3.2, epsilon: 0.28\n",
      "regression loss:  1593.124\n",
      "regression loss:  2826.149\n",
      "episode: 2547/1000000, returns: 1.5e+01, epsilon: 0.28\n",
      "regression loss:  727.43243\n",
      "regression loss:  2252.2983\n",
      "episode: 2548/1000000, returns: -5.7, epsilon: 0.28\n",
      "regression loss:  422.74792\n",
      "regression loss:  2210.893\n",
      "episode: 2549/1000000, returns: 2.1, epsilon: 0.28\n",
      "regression loss:  1734.6676\n",
      "regression loss:  1460.3387\n",
      "episode: 2550/1000000, returns: -6.4, epsilon: 0.28\n",
      "regression loss:  378.39185\n",
      "regression loss:  1868.6952\n",
      "episode: 2551/1000000, returns: 1.6, epsilon: 0.28\n",
      "regression loss:  508.2365\n",
      "regression loss:  1900.0623\n",
      "episode: 2552/1000000, returns: 1.5, epsilon: 0.28\n",
      "regression loss:  1494.36\n",
      "regression loss:  1063.9722\n",
      "episode: 2553/1000000, returns: 2.3, epsilon: 0.28\n",
      "regression loss:  331.47897\n",
      "regression loss:  2822.394\n",
      "episode: 2554/1000000, returns: 1.4, epsilon: 0.28\n",
      "regression loss:  722.8931\n",
      "regression loss:  2184.1777\n",
      "episode: 2555/1000000, returns: 1.4, epsilon: 0.28\n",
      "regression loss:  161.80003\n",
      "regression loss:  692.50696\n",
      "episode: 2556/1000000, returns: -1.3e+01, epsilon: 0.28\n",
      "regression loss:  1199.3306\n",
      "regression loss:  1218.3191\n",
      "episode: 2557/1000000, returns: -7.8, epsilon: 0.28\n",
      "regression loss:  1659.0883\n",
      "regression loss:  1419.148\n",
      "episode: 2558/1000000, returns: -0.56, epsilon: 0.28\n",
      "regression loss:  2757.4229\n",
      "regression loss:  3301.5532\n",
      "episode: 2559/1000000, returns: 0.73, epsilon: 0.28\n",
      "regression loss:  2493.4163\n",
      "regression loss:  3847.2056\n",
      "episode: 2560/1000000, returns: 1.1e+01, epsilon: 0.28\n",
      "regression loss:  1489.4814\n",
      "regression loss:  4131.2725\n",
      "episode: 2561/1000000, returns: 3.9, epsilon: 0.28\n",
      "regression loss:  1668.6125\n",
      "regression loss:  3088.2378\n",
      "episode: 2562/1000000, returns: -6.9, epsilon: 0.28\n",
      "regression loss:  2806.3875\n",
      "regression loss:  9475.34\n",
      "episode: 2563/1000000, returns: 0.41, epsilon: 0.28\n",
      "regression loss:  5655.009\n",
      "regression loss:  5372.9404\n",
      "episode: 2564/1000000, returns: -1.0, epsilon: 0.28\n",
      "regression loss:  2251.7463\n",
      "regression loss:  2001.6456\n",
      "episode: 2565/1000000, returns: 2.2, epsilon: 0.28\n",
      "regression loss:  10124.231\n",
      "regression loss:  5050.0493\n",
      "episode: 2566/1000000, returns: -5.0, epsilon: 0.28\n",
      "regression loss:  901.1532\n",
      "regression loss:  5245.1187\n",
      "episode: 2567/1000000, returns: 0.92, epsilon: 0.28\n",
      "regression loss:  1151.1056\n",
      "regression loss:  3459.0505\n",
      "episode: 2568/1000000, returns: -1.3, epsilon: 0.28\n",
      "regression loss:  1349.2115\n",
      "regression loss:  1158.5674\n",
      "episode: 2569/1000000, returns: 1.9, epsilon: 0.28\n",
      "regression loss:  1184.6937\n",
      "regression loss:  2771.9407\n",
      "episode: 2570/1000000, returns: 3.1, epsilon: 0.28\n",
      "regression loss:  156.67427\n",
      "regression loss:  492.23343\n",
      "episode: 2571/1000000, returns: 0.37, epsilon: 0.28\n",
      "regression loss:  769.5204\n",
      "regression loss:  2068.7495\n",
      "episode: 2572/1000000, returns: -0.31, epsilon: 0.28\n",
      "regression loss:  725.7782\n",
      "regression loss:  1347.149\n",
      "episode: 2573/1000000, returns: 0.44, epsilon: 0.28\n",
      "regression loss:  323.14288\n",
      "regression loss:  2658.4646\n",
      "episode: 2574/1000000, returns: 8.0, epsilon: 0.28\n",
      "regression loss:  266.0898\n",
      "regression loss:  902.125\n",
      "episode: 2575/1000000, returns: -5.2, epsilon: 0.28\n",
      "regression loss:  620.88354\n",
      "regression loss:  1841.736\n",
      "episode: 2576/1000000, returns: 1.7, epsilon: 0.28\n",
      "regression loss:  437.54373\n",
      "regression loss:  839.81213\n",
      "episode: 2577/1000000, returns: 1.2, epsilon: 0.28\n",
      "regression loss:  369.1482\n",
      "regression loss:  626.14996\n",
      "episode: 2578/1000000, returns: 2.6, epsilon: 0.28\n",
      "regression loss:  122.44933\n",
      "regression loss:  495.9851\n",
      "episode: 2579/1000000, returns: 1.8e+01, epsilon: 0.28\n",
      "regression loss:  36.394524\n",
      "regression loss:  1747.1635\n",
      "episode: 2580/1000000, returns: -5.2, epsilon: 0.28\n",
      "regression loss:  759.6731\n",
      "regression loss:  1171.7545\n",
      "episode: 2581/1000000, returns: 4.7, epsilon: 0.28\n",
      "regression loss:  787.8331\n",
      "regression loss:  1361.189\n",
      "episode: 2582/1000000, returns: -7.1, epsilon: 0.27\n",
      "regression loss:  93.688034\n",
      "regression loss:  2202.9773\n",
      "episode: 2583/1000000, returns: 0.11, epsilon: 0.27\n",
      "regression loss:  36.821667\n",
      "regression loss:  60.165485\n",
      "episode: 2584/1000000, returns: 1.7, epsilon: 0.27\n",
      "regression loss:  121.17955\n",
      "regression loss:  1730.8302\n",
      "episode: 2585/1000000, returns: 2.7, epsilon: 0.27\n",
      "regression loss:  218.21695\n",
      "regression loss:  236.05719\n",
      "episode: 2586/1000000, returns: 1.1e+01, epsilon: 0.27\n",
      "regression loss:  95.717606\n",
      "regression loss:  27.15759\n",
      "episode: 2587/1000000, returns: 0.99, epsilon: 0.27\n",
      "regression loss:  5.9475746\n",
      "regression loss:  1946.8202\n",
      "episode: 2588/1000000, returns: -0.95, epsilon: 0.27\n",
      "regression loss:  549.1483\n",
      "regression loss:  1126.3644\n",
      "episode: 2589/1000000, returns: -0.12, epsilon: 0.27\n",
      "regression loss:  1538.0345\n",
      "regression loss:  814.72833\n",
      "episode: 2590/1000000, returns: -3.8, epsilon: 0.27\n",
      "regression loss:  16026.534\n",
      "regression loss:  1626.9797\n",
      "episode: 2591/1000000, returns: 0.63, epsilon: 0.27\n",
      "regression loss:  323.80902\n",
      "regression loss:  2678.9695\n",
      "episode: 2592/1000000, returns: 1.5, epsilon: 0.27\n",
      "regression loss:  6022.7324\n",
      "regression loss:  5564.3066\n",
      "episode: 2593/1000000, returns: 0.82, epsilon: 0.27\n",
      "regression loss:  3804.6704\n",
      "regression loss:  2361.3816\n",
      "episode: 2594/1000000, returns: -5.2, epsilon: 0.27\n",
      "regression loss:  1068.7893\n",
      "regression loss:  5512.765\n",
      "episode: 2595/1000000, returns: 9.8, epsilon: 0.27\n",
      "regression loss:  2552.5613\n",
      "regression loss:  6625.005\n",
      "episode: 2596/1000000, returns: 1.6, epsilon: 0.27\n",
      "regression loss:  25039.521\n",
      "regression loss:  8060.866\n",
      "episode: 2597/1000000, returns: -0.14, epsilon: 0.27\n",
      "regression loss:  10035.553\n",
      "regression loss:  8398.555\n",
      "episode: 2598/1000000, returns: 1.4, epsilon: 0.27\n",
      "regression loss:  1010.79724\n",
      "regression loss:  1520.8024\n",
      "episode: 2599/1000000, returns: 1.9, epsilon: 0.27\n",
      "regression loss:  167.45615\n",
      "regression loss:  2688.911\n",
      "episode: 2600/1000000, returns: 2.6, epsilon: 0.27\n",
      "regression loss:  385.56815\n",
      "regression loss:  341.8277\n",
      "episode: 2601/1000000, returns: 0.5, epsilon: 0.27\n",
      "regression loss:  171.04573\n",
      "regression loss:  517.2652\n",
      "episode: 2602/1000000, returns: -1.4, epsilon: 0.27\n",
      "regression loss:  982.60657\n",
      "regression loss:  1917.537\n",
      "episode: 2603/1000000, returns: 5.8, epsilon: 0.27\n",
      "regression loss:  2019.2043\n",
      "regression loss:  1502.1104\n",
      "episode: 2604/1000000, returns: 3.2, epsilon: 0.27\n",
      "regression loss:  9933.694\n",
      "regression loss:  8535.664\n",
      "episode: 2605/1000000, returns: -1.8, epsilon: 0.27\n",
      "regression loss:  12136.918\n",
      "regression loss:  5488.9116\n",
      "episode: 2606/1000000, returns: -2e+01, epsilon: 0.27\n",
      "regression loss:  8198.742\n",
      "regression loss:  1522.7488\n",
      "episode: 2607/1000000, returns: -1.3e+01, epsilon: 0.27\n",
      "regression loss:  411.7393\n",
      "regression loss:  4081.8384\n",
      "episode: 2608/1000000, returns: -2.4, epsilon: 0.27\n",
      "regression loss:  466.30023\n",
      "regression loss:  550.0187\n",
      "episode: 2609/1000000, returns: -1.0, epsilon: 0.27\n",
      "regression loss:  396.58572\n",
      "regression loss:  4856.3247\n",
      "episode: 2610/1000000, returns: 4.8, epsilon: 0.27\n",
      "regression loss:  3641.3704\n",
      "regression loss:  1689.8066\n",
      "episode: 2611/1000000, returns: -8.5, epsilon: 0.27\n",
      "regression loss:  308.19687\n",
      "regression loss:  2338.652\n",
      "episode: 2612/1000000, returns: 3.4, epsilon: 0.27\n",
      "regression loss:  198.41902\n",
      "regression loss:  3631.502\n",
      "episode: 2613/1000000, returns: 1.9, epsilon: 0.27\n",
      "regression loss:  732.7132\n",
      "regression loss:  847.4091\n",
      "episode: 2614/1000000, returns: -0.66, epsilon: 0.27\n",
      "regression loss:  523.89355\n",
      "regression loss:  2177.3887\n",
      "episode: 2615/1000000, returns: 4.1, epsilon: 0.27\n",
      "regression loss:  2068.8486\n",
      "regression loss:  6967.646\n",
      "episode: 2616/1000000, returns: -0.71, epsilon: 0.27\n",
      "regression loss:  5292.644\n",
      "regression loss:  2095.949\n",
      "episode: 2617/1000000, returns: 0.19, epsilon: 0.27\n",
      "regression loss:  1443.7345\n",
      "regression loss:  2384.155\n",
      "episode: 2618/1000000, returns: -3.1, epsilon: 0.27\n",
      "regression loss:  2969.5908\n",
      "regression loss:  4010.6067\n",
      "episode: 2619/1000000, returns: 1.3e+01, epsilon: 0.27\n",
      "regression loss:  469.50958\n",
      "regression loss:  569.5737\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 2620/1000000, returns: 0.52, epsilon: 0.27\n",
      "regression loss:  347.13626\n",
      "regression loss:  3421.1775\n",
      "episode: 2621/1000000, returns: -0.58, epsilon: 0.27\n",
      "regression loss:  261.42612\n",
      "regression loss:  1730.2107\n",
      "episode: 2622/1000000, returns: 8.7, epsilon: 0.27\n",
      "regression loss:  6317.313\n",
      "regression loss:  4703.8047\n",
      "episode: 2623/1000000, returns: -0.74, epsilon: 0.27\n",
      "regression loss:  3912.8206\n",
      "regression loss:  4760.0405\n",
      "episode: 2624/1000000, returns: -0.62, epsilon: 0.27\n",
      "regression loss:  262.05005\n",
      "regression loss:  1718.213\n",
      "episode: 2625/1000000, returns: 5.7, epsilon: 0.27\n",
      "regression loss:  2456.543\n",
      "regression loss:  2873.3418\n",
      "episode: 2626/1000000, returns: 1e+01, epsilon: 0.27\n",
      "regression loss:  3056.7756\n",
      "regression loss:  3512.9614\n",
      "episode: 2627/1000000, returns: 5.1, epsilon: 0.27\n",
      "regression loss:  8300.518\n",
      "regression loss:  953.149\n",
      "episode: 2628/1000000, returns: 4.1, epsilon: 0.27\n",
      "regression loss:  1960.3958\n",
      "regression loss:  2870.9048\n",
      "episode: 2629/1000000, returns: 1.5e+01, epsilon: 0.27\n",
      "regression loss:  1862.4849\n",
      "regression loss:  945.04395\n",
      "episode: 2630/1000000, returns: 1.5, epsilon: 0.27\n",
      "regression loss:  503.83368\n",
      "regression loss:  3815.7827\n",
      "episode: 2631/1000000, returns: -3.9, epsilon: 0.27\n",
      "regression loss:  389.79794\n",
      "regression loss:  2467.6797\n",
      "episode: 2632/1000000, returns: 5.0, epsilon: 0.27\n",
      "regression loss:  431.5935\n",
      "regression loss:  1022.2661\n",
      "episode: 2633/1000000, returns: 1.8, epsilon: 0.27\n",
      "regression loss:  480.75415\n",
      "regression loss:  5012.3765\n",
      "episode: 2634/1000000, returns: 1.3, epsilon: 0.27\n",
      "regression loss:  587.93225\n",
      "regression loss:  876.5171\n",
      "episode: 2635/1000000, returns: -0.7, epsilon: 0.27\n",
      "regression loss:  344.69662\n",
      "regression loss:  630.49805\n",
      "episode: 2636/1000000, returns: -0.59, epsilon: 0.27\n",
      "regression loss:  596.1127\n",
      "regression loss:  1532.9858\n",
      "episode: 2637/1000000, returns: 1.7, epsilon: 0.27\n",
      "regression loss:  2848.1338\n",
      "regression loss:  1088.0813\n",
      "episode: 2638/1000000, returns: -0.51, epsilon: 0.27\n",
      "regression loss:  162.11125\n",
      "regression loss:  1752.0359\n",
      "episode: 2639/1000000, returns: 2.5, epsilon: 0.27\n",
      "regression loss:  125.009094\n",
      "regression loss:  1624.565\n",
      "episode: 2640/1000000, returns: 4.4, epsilon: 0.27\n",
      "regression loss:  2660.8306\n",
      "regression loss:  1413.6707\n",
      "episode: 2641/1000000, returns: 0.15, epsilon: 0.27\n",
      "regression loss:  1747.3707\n",
      "regression loss:  2098.4712\n",
      "episode: 2642/1000000, returns: 2.5, epsilon: 0.27\n",
      "regression loss:  943.63116\n",
      "regression loss:  479.95905\n",
      "episode: 2643/1000000, returns: 5.7, epsilon: 0.27\n",
      "regression loss:  815.1302\n",
      "regression loss:  4439.8936\n",
      "episode: 2644/1000000, returns: -0.091, epsilon: 0.27\n",
      "regression loss:  330.13586\n",
      "regression loss:  1055.1271\n",
      "episode: 2645/1000000, returns: -0.26, epsilon: 0.27\n",
      "regression loss:  1325.3258\n",
      "regression loss:  3879.7712\n",
      "episode: 2646/1000000, returns: 0.32, epsilon: 0.27\n",
      "regression loss:  1248.0793\n",
      "regression loss:  5711.114\n",
      "episode: 2647/1000000, returns: -0.14, epsilon: 0.27\n",
      "regression loss:  2020.759\n",
      "regression loss:  8677.044\n",
      "episode: 2648/1000000, returns: 5.9, epsilon: 0.27\n",
      "regression loss:  7621.6836\n",
      "regression loss:  3050.9329\n",
      "episode: 2649/1000000, returns: 3.3, epsilon: 0.27\n",
      "regression loss:  1259.4329\n",
      "regression loss:  762.69025\n",
      "episode: 2650/1000000, returns: -6.0, epsilon: 0.27\n",
      "regression loss:  1416.3268\n",
      "regression loss:  2828.731\n",
      "episode: 2651/1000000, returns: -3.0, epsilon: 0.27\n",
      "regression loss:  3559.6592\n",
      "regression loss:  7723.643\n",
      "episode: 2652/1000000, returns: -0.57, epsilon: 0.27\n",
      "regression loss:  4515.4624\n",
      "regression loss:  8730.75\n",
      "episode: 2653/1000000, returns: -1.8, epsilon: 0.27\n",
      "regression loss:  2537.4229\n",
      "regression loss:  5450.812\n",
      "episode: 2654/1000000, returns: 1.5, epsilon: 0.27\n",
      "regression loss:  7123.6484\n",
      "regression loss:  1789.7407\n",
      "episode: 2655/1000000, returns: 1.5, epsilon: 0.27\n",
      "regression loss:  1573.9211\n",
      "regression loss:  5021.2085\n",
      "episode: 2656/1000000, returns: 7.1, epsilon: 0.26\n",
      "regression loss:  855.46497\n",
      "regression loss:  1859.5168\n",
      "episode: 2657/1000000, returns: 6.4, epsilon: 0.26\n",
      "regression loss:  5695.7114\n",
      "regression loss:  2610.4194\n",
      "episode: 2658/1000000, returns: 4.2, epsilon: 0.26\n",
      "regression loss:  2847.4614\n",
      "regression loss:  1425.4071\n",
      "episode: 2659/1000000, returns: 4.3, epsilon: 0.26\n",
      "regression loss:  1080.2294\n",
      "regression loss:  820.5497\n",
      "episode: 2660/1000000, returns: -0.85, epsilon: 0.26\n",
      "regression loss:  447.42603\n",
      "regression loss:  372.38742\n",
      "episode: 2661/1000000, returns: -3.7, epsilon: 0.26\n",
      "regression loss:  1113.5337\n",
      "regression loss:  1088.1537\n",
      "episode: 2662/1000000, returns: 8.1, epsilon: 0.26\n",
      "regression loss:  270.4013\n",
      "regression loss:  134.95789\n",
      "episode: 2663/1000000, returns: 0.49, epsilon: 0.26\n",
      "regression loss:  26.881098\n",
      "regression loss:  271.10934\n",
      "episode: 2664/1000000, returns: 5.6, epsilon: 0.26\n",
      "regression loss:  830.28046\n",
      "regression loss:  2786.6506\n",
      "episode: 2665/1000000, returns: 3.2, epsilon: 0.26\n",
      "regression loss:  557.3767\n",
      "regression loss:  1662.9706\n",
      "episode: 2666/1000000, returns: 2.4, epsilon: 0.26\n",
      "regression loss:  914.35266\n",
      "regression loss:  1319.9225\n",
      "episode: 2667/1000000, returns: 2.6, epsilon: 0.26\n",
      "regression loss:  2334.001\n",
      "regression loss:  3438.5547\n",
      "episode: 2668/1000000, returns: 0.41, epsilon: 0.26\n",
      "regression loss:  2625.1575\n",
      "regression loss:  3193.1157\n",
      "episode: 2669/1000000, returns: 2.5, epsilon: 0.26\n",
      "regression loss:  2081.573\n",
      "regression loss:  4865.1123\n",
      "episode: 2670/1000000, returns: 0.89, epsilon: 0.26\n",
      "regression loss:  10762.213\n",
      "regression loss:  10683.118\n",
      "episode: 2671/1000000, returns: 0.8, epsilon: 0.26\n",
      "regression loss:  47399.445\n",
      "regression loss:  4891.5635\n",
      "episode: 2672/1000000, returns: -1.2, epsilon: 0.26\n",
      "regression loss:  591.3165\n",
      "regression loss:  1702.0967\n",
      "episode: 2673/1000000, returns: -1.0, epsilon: 0.26\n",
      "regression loss:  12184.854\n",
      "regression loss:  7169.5845\n",
      "episode: 2674/1000000, returns: -0.14, epsilon: 0.26\n",
      "regression loss:  1294.2075\n",
      "regression loss:  2169.8987\n",
      "episode: 2675/1000000, returns: 2.5, epsilon: 0.26\n",
      "regression loss:  3656.301\n",
      "regression loss:  6863.1226\n",
      "episode: 2676/1000000, returns: -2.4, epsilon: 0.26\n",
      "regression loss:  2275.8674\n",
      "regression loss:  5216.2437\n",
      "episode: 2677/1000000, returns: -1.8, epsilon: 0.26\n",
      "regression loss:  185.16708\n",
      "regression loss:  2148.0413\n",
      "episode: 2678/1000000, returns: 1.8, epsilon: 0.26\n",
      "regression loss:  136.81055\n",
      "regression loss:  1285.0065\n",
      "episode: 2679/1000000, returns: -0.25, epsilon: 0.26\n",
      "regression loss:  255.09631\n",
      "regression loss:  3440.7153\n",
      "episode: 2680/1000000, returns: -1.1, epsilon: 0.26\n",
      "regression loss:  717.8086\n",
      "regression loss:  2290.1465\n",
      "episode: 2681/1000000, returns: -3.7, epsilon: 0.26\n",
      "regression loss:  3818.0024\n",
      "regression loss:  3979.8628\n",
      "episode: 2682/1000000, returns: -1.5, epsilon: 0.26\n",
      "regression loss:  1711.4762\n",
      "regression loss:  1938.7917\n",
      "episode: 2683/1000000, returns: 1.5, epsilon: 0.26\n",
      "regression loss:  5633.833\n",
      "regression loss:  2763.03\n",
      "episode: 2684/1000000, returns: 1e+01, epsilon: 0.26\n",
      "regression loss:  2384.268\n",
      "regression loss:  7213.2188\n",
      "episode: 2685/1000000, returns: 3.3, epsilon: 0.26\n",
      "regression loss:  6888.9395\n",
      "regression loss:  1949.4216\n",
      "episode: 2686/1000000, returns: -1.5, epsilon: 0.26\n",
      "regression loss:  158.5699\n",
      "regression loss:  2325.76\n",
      "episode: 2687/1000000, returns: -0.79, epsilon: 0.26\n",
      "regression loss:  1563.1946\n",
      "regression loss:  1403.5687\n",
      "episode: 2688/1000000, returns: -2.1, epsilon: 0.26\n",
      "regression loss:  529.4675\n",
      "regression loss:  1844.1882\n",
      "episode: 2689/1000000, returns: -3.4, epsilon: 0.26\n",
      "regression loss:  230.55557\n",
      "regression loss:  2840.9956\n",
      "episode: 2690/1000000, returns: 2.4, epsilon: 0.26\n",
      "regression loss:  1497.2738\n",
      "regression loss:  1263.4751\n",
      "episode: 2691/1000000, returns: 4.3, epsilon: 0.26\n",
      "regression loss:  873.65125\n",
      "regression loss:  1338.2211\n",
      "episode: 2692/1000000, returns: 2.6, epsilon: 0.26\n",
      "regression loss:  588.8547\n",
      "regression loss:  2533.3389\n",
      "episode: 2693/1000000, returns: 1.8, epsilon: 0.26\n",
      "regression loss:  309.68973\n",
      "regression loss:  2529.0376\n",
      "episode: 2694/1000000, returns: -0.0013, epsilon: 0.26\n",
      "regression loss:  896.61475\n",
      "regression loss:  4359.3423\n",
      "episode: 2695/1000000, returns: 1.6, epsilon: 0.26\n",
      "regression loss:  497.2137\n",
      "regression loss:  2576.63\n",
      "episode: 2696/1000000, returns: 3.4, epsilon: 0.26\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "regression loss:  312.1771\n",
      "regression loss:  1573.7389\n",
      "episode: 2697/1000000, returns: -2.1, epsilon: 0.26\n",
      "regression loss:  1315.6274\n",
      "regression loss:  2214.2046\n",
      "episode: 2698/1000000, returns: 1.2, epsilon: 0.26\n",
      "regression loss:  256.16757\n",
      "regression loss:  2368.1367\n",
      "episode: 2699/1000000, returns: -2.4, epsilon: 0.26\n",
      "regression loss:  27621.152\n",
      "regression loss:  9292.66\n",
      "episode: 2700/1000000, returns: 0.1, epsilon: 0.26\n",
      "regression loss:  1864.1187\n",
      "regression loss:  3358.755\n",
      "episode: 2701/1000000, returns: -0.23, epsilon: 0.26\n",
      "regression loss:  11242.187\n",
      "regression loss:  7403.254\n",
      "episode: 2702/1000000, returns: -2.8, epsilon: 0.26\n",
      "regression loss:  4305.868\n",
      "regression loss:  5552.9946\n",
      "episode: 2703/1000000, returns: 1.1, epsilon: 0.26\n",
      "regression loss:  3125.929\n",
      "regression loss:  4146.6426\n",
      "episode: 2704/1000000, returns: -0.81, epsilon: 0.26\n",
      "regression loss:  1481.4644\n",
      "regression loss:  5041.076\n",
      "episode: 2705/1000000, returns: -0.79, epsilon: 0.26\n",
      "regression loss:  375.10318\n",
      "regression loss:  2365.7993\n",
      "episode: 2706/1000000, returns: -1.8, epsilon: 0.26\n",
      "regression loss:  1671.936\n",
      "regression loss:  3071.314\n",
      "episode: 2707/1000000, returns: 4.5, epsilon: 0.26\n",
      "regression loss:  686.27966\n",
      "regression loss:  2409.217\n",
      "episode: 2708/1000000, returns: -9.5, epsilon: 0.26\n",
      "regression loss:  4869.285\n",
      "regression loss:  7200.4194\n",
      "episode: 2709/1000000, returns: 0.029, epsilon: 0.26\n",
      "regression loss:  548.68304\n",
      "regression loss:  2851.5723\n",
      "episode: 2710/1000000, returns: 0.21, epsilon: 0.26\n",
      "regression loss:  586.836\n",
      "regression loss:  3935.964\n",
      "episode: 2711/1000000, returns: 2.4, epsilon: 0.26\n",
      "regression loss:  1163.4133\n",
      "regression loss:  3865.9805\n",
      "episode: 2712/1000000, returns: 1.3, epsilon: 0.26\n",
      "regression loss:  316.9628\n",
      "regression loss:  1639.2582\n",
      "episode: 2713/1000000, returns: -3.5, epsilon: 0.26\n",
      "regression loss:  553.0682\n",
      "regression loss:  622.5177\n",
      "episode: 2714/1000000, returns: -0.38, epsilon: 0.26\n",
      "regression loss:  1664.1736\n",
      "regression loss:  834.2401\n",
      "episode: 2715/1000000, returns: 0.3, epsilon: 0.26\n",
      "regression loss:  2507.2524\n",
      "regression loss:  3597.646\n",
      "episode: 2716/1000000, returns: 1.5, epsilon: 0.26\n",
      "regression loss:  877.6631\n",
      "regression loss:  1138.4915\n",
      "episode: 2717/1000000, returns: -1.5, epsilon: 0.26\n",
      "regression loss:  1235.4238\n",
      "regression loss:  1937.1259\n",
      "episode: 2718/1000000, returns: -5.1, epsilon: 0.26\n",
      "regression loss:  257.40195\n",
      "regression loss:  1288.0608\n",
      "episode: 2719/1000000, returns: 0.5, epsilon: 0.26\n",
      "regression loss:  515.0845\n",
      "regression loss:  1188.8147\n",
      "episode: 2720/1000000, returns: 5.5, epsilon: 0.26\n",
      "regression loss:  294.89212\n",
      "regression loss:  1801.3245\n",
      "episode: 2721/1000000, returns: -5.2, epsilon: 0.26\n",
      "regression loss:  1118.498\n",
      "regression loss:  2510.0215\n",
      "episode: 2722/1000000, returns: 0.7, epsilon: 0.26\n",
      "regression loss:  79.79675\n",
      "regression loss:  577.65906\n",
      "episode: 2723/1000000, returns: -3.5, epsilon: 0.26\n",
      "regression loss:  285.86838\n",
      "regression loss:  1119.063\n",
      "episode: 2724/1000000, returns: 1.6, epsilon: 0.26\n",
      "regression loss:  669.7014\n",
      "regression loss:  2315.5884\n",
      "episode: 2725/1000000, returns: 1.2, epsilon: 0.26\n",
      "regression loss:  1817.208\n",
      "regression loss:  840.131\n",
      "episode: 2726/1000000, returns: -0.48, epsilon: 0.26\n",
      "regression loss:  1663.6265\n",
      "regression loss:  3157.6858\n",
      "episode: 2727/1000000, returns: -1.8, epsilon: 0.26\n",
      "regression loss:  446.48114\n",
      "regression loss:  4332.081\n",
      "episode: 2728/1000000, returns: -2.1, epsilon: 0.26\n",
      "regression loss:  1182.4368\n",
      "regression loss:  1865.287\n",
      "episode: 2729/1000000, returns: -3.2, epsilon: 0.26\n",
      "regression loss:  78.74725\n",
      "regression loss:  1632.6212\n",
      "episode: 2730/1000000, returns: -0.91, epsilon: 0.26\n",
      "regression loss:  83.91161\n",
      "regression loss:  4101.148\n",
      "episode: 2731/1000000, returns: 2.8, epsilon: 0.26\n",
      "regression loss:  2069.256\n",
      "regression loss:  1722.1594\n",
      "episode: 2732/1000000, returns: -0.78, epsilon: 0.26\n",
      "regression loss:  91.981\n",
      "regression loss:  3994.7104\n",
      "episode: 2733/1000000, returns: 0.21, epsilon: 0.25\n",
      "regression loss:  3050.314\n",
      "regression loss:  1936.2489\n",
      "episode: 2734/1000000, returns: -0.81, epsilon: 0.25\n",
      "regression loss:  1105.639\n",
      "regression loss:  1859.2369\n",
      "episode: 2735/1000000, returns: 0.3, epsilon: 0.25\n",
      "regression loss:  1925.2716\n",
      "regression loss:  3261.4436\n",
      "episode: 2736/1000000, returns: 0.16, epsilon: 0.25\n",
      "regression loss:  1560.8135\n",
      "regression loss:  2442.3271\n",
      "episode: 2737/1000000, returns: 3.9, epsilon: 0.25\n",
      "regression loss:  1290.6077\n",
      "regression loss:  1809.5564\n",
      "episode: 2738/1000000, returns: -0.31, epsilon: 0.25\n",
      "regression loss:  3210.5852\n",
      "regression loss:  3864.4114\n",
      "episode: 2739/1000000, returns: -3.2, epsilon: 0.25\n",
      "regression loss:  307.72702\n",
      "regression loss:  8035.5483\n",
      "episode: 2740/1000000, returns: -0.23, epsilon: 0.25\n",
      "regression loss:  787.8581\n",
      "regression loss:  936.5949\n",
      "episode: 2741/1000000, returns: 1.4, epsilon: 0.25\n",
      "regression loss:  299.495\n",
      "regression loss:  3881.1304\n",
      "episode: 2742/1000000, returns: -6.4, epsilon: 0.25\n",
      "regression loss:  196.50056\n",
      "regression loss:  880.221\n",
      "episode: 2743/1000000, returns: 2.5, epsilon: 0.25\n",
      "regression loss:  104.08198\n",
      "regression loss:  3230.1248\n",
      "episode: 2744/1000000, returns: 1.8, epsilon: 0.25\n",
      "regression loss:  1658.2308\n",
      "regression loss:  727.5783\n",
      "episode: 2745/1000000, returns: -4.8, epsilon: 0.25\n",
      "regression loss:  1033.8208\n",
      "regression loss:  2338.649\n",
      "episode: 2746/1000000, returns: 0.8, epsilon: 0.25\n",
      "regression loss:  1523.835\n",
      "regression loss:  1731.2545\n",
      "episode: 2747/1000000, returns: 0.19, epsilon: 0.25\n",
      "regression loss:  1878.5596\n",
      "regression loss:  1366.5165\n",
      "episode: 2748/1000000, returns: -9.6, epsilon: 0.25\n",
      "regression loss:  773.3417\n",
      "regression loss:  1381.2313\n",
      "episode: 2749/1000000, returns: -0.6, epsilon: 0.25\n",
      "regression loss:  1068.6163\n",
      "regression loss:  578.3968\n",
      "episode: 2750/1000000, returns: 1.3e+01, epsilon: 0.25\n",
      "regression loss:  4991.9883\n",
      "regression loss:  9013.849\n",
      "episode: 2751/1000000, returns: 1.2, epsilon: 0.25\n",
      "regression loss:  18740.54\n",
      "regression loss:  6620.609\n",
      "episode: 2752/1000000, returns: 2.8, epsilon: 0.25\n",
      "regression loss:  896.4292\n",
      "regression loss:  3400.8682\n",
      "episode: 2753/1000000, returns: 1.7, epsilon: 0.25\n",
      "regression loss:  5830.961\n",
      "regression loss:  10913.023\n",
      "episode: 2754/1000000, returns: -0.14, epsilon: 0.25\n",
      "regression loss:  1786.5957\n",
      "regression loss:  6349.445\n",
      "episode: 2755/1000000, returns: 1.8, epsilon: 0.25\n",
      "regression loss:  458.2326\n",
      "regression loss:  2926.2842\n",
      "episode: 2756/1000000, returns: 1.1, epsilon: 0.25\n",
      "regression loss:  399.11444\n",
      "regression loss:  3615.652\n",
      "episode: 2757/1000000, returns: 1.0, epsilon: 0.25\n",
      "regression loss:  334.69482\n",
      "regression loss:  1897.7379\n",
      "episode: 2758/1000000, returns: 0.6, epsilon: 0.25\n",
      "regression loss:  545.9142\n",
      "regression loss:  3238.8901\n",
      "episode: 2759/1000000, returns: 0.5, epsilon: 0.25\n",
      "regression loss:  1095.2334\n",
      "regression loss:  2539.3862\n",
      "episode: 2760/1000000, returns: -2.6, epsilon: 0.25\n",
      "regression loss:  147.89\n",
      "regression loss:  1114.4514\n",
      "episode: 2761/1000000, returns: 0.58, epsilon: 0.25\n",
      "regression loss:  1294.3113\n",
      "regression loss:  3458.4314\n",
      "episode: 2762/1000000, returns: 2.2, epsilon: 0.25\n",
      "regression loss:  6354.9165\n",
      "regression loss:  6362.1665\n",
      "episode: 2763/1000000, returns: -0.6, epsilon: 0.25\n",
      "regression loss:  3829.9387\n",
      "regression loss:  1112.2023\n",
      "episode: 2764/1000000, returns: 0.27, epsilon: 0.25\n",
      "regression loss:  543.23566\n",
      "regression loss:  1195.9794\n",
      "episode: 2765/1000000, returns: -0.28, epsilon: 0.25\n",
      "regression loss:  2834.2324\n",
      "regression loss:  1723.4685\n",
      "episode: 2766/1000000, returns: 1.7, epsilon: 0.25\n",
      "regression loss:  369.26822\n",
      "regression loss:  2420.0815\n",
      "episode: 2767/1000000, returns: 2.3, epsilon: 0.25\n",
      "regression loss:  322.12137\n",
      "regression loss:  2262.2634\n",
      "episode: 2768/1000000, returns: 4.3, epsilon: 0.25\n",
      "regression loss:  270.45895\n",
      "regression loss:  1676.8613\n",
      "episode: 2769/1000000, returns: 0.25, epsilon: 0.25\n",
      "regression loss:  231.89835\n",
      "regression loss:  2258.9307\n",
      "episode: 2770/1000000, returns: -0.4, epsilon: 0.25\n",
      "regression loss:  495.6494\n",
      "regression loss:  1894.4362\n",
      "episode: 2771/1000000, returns: -3.1, epsilon: 0.25\n",
      "regression loss:  242.04167\n",
      "regression loss:  1904.4009\n",
      "episode: 2772/1000000, returns: -1.2, epsilon: 0.25\n",
      "regression loss:  2372.0063\n",
      "regression loss:  1994.3141\n",
      "episode: 2773/1000000, returns: -0.098, epsilon: 0.25\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "regression loss:  425.09283\n",
      "regression loss:  2637.392\n",
      "episode: 2774/1000000, returns: -0.3, epsilon: 0.25\n",
      "regression loss:  996.9375\n",
      "regression loss:  4741.049\n",
      "episode: 2775/1000000, returns: -0.5, epsilon: 0.25\n",
      "regression loss:  117.6569\n",
      "regression loss:  1647.5\n",
      "episode: 2776/1000000, returns: 0.52, epsilon: 0.25\n",
      "regression loss:  1529.6711\n",
      "regression loss:  4385.741\n",
      "episode: 2777/1000000, returns: 0.97, epsilon: 0.25\n",
      "regression loss:  9112.601\n",
      "regression loss:  6840.002\n",
      "episode: 2778/1000000, returns: -0.88, epsilon: 0.25\n",
      "regression loss:  3987.4375\n",
      "regression loss:  1999.7708\n",
      "episode: 2779/1000000, returns: -0.26, epsilon: 0.25\n",
      "regression loss:  3160.6506\n",
      "regression loss:  4629.4595\n",
      "episode: 2780/1000000, returns: 1.6, epsilon: 0.25\n",
      "regression loss:  3794.9622\n",
      "regression loss:  1831.4587\n",
      "episode: 2781/1000000, returns: 5.0, epsilon: 0.25\n",
      "regression loss:  10673.237\n",
      "regression loss:  5991.656\n",
      "episode: 2782/1000000, returns: 4.9, epsilon: 0.25\n",
      "regression loss:  1312.5835\n",
      "regression loss:  869.7994\n",
      "episode: 2783/1000000, returns: 4.9, epsilon: 0.25\n",
      "regression loss:  2434.411\n",
      "regression loss:  2067.912\n",
      "episode: 2784/1000000, returns: -5.2, epsilon: 0.25\n",
      "regression loss:  362.5659\n",
      "regression loss:  176.60085\n",
      "episode: 2785/1000000, returns: -2.0, epsilon: 0.25\n",
      "regression loss:  431.663\n",
      "regression loss:  2079.7998\n",
      "episode: 2786/1000000, returns: 0.18, epsilon: 0.25\n",
      "regression loss:  4429.7515\n",
      "regression loss:  5094.0586\n",
      "episode: 2787/1000000, returns: -1.2, epsilon: 0.25\n",
      "regression loss:  5956.7817\n",
      "regression loss:  8376.671\n",
      "episode: 2788/1000000, returns: 0.85, epsilon: 0.25\n",
      "regression loss:  492.9127\n",
      "regression loss:  801.5963\n",
      "episode: 2789/1000000, returns: -1.6, epsilon: 0.25\n",
      "regression loss:  112.07767\n",
      "regression loss:  2575.4739\n",
      "episode: 2790/1000000, returns: -0.22, epsilon: 0.25\n",
      "regression loss:  410.93292\n",
      "regression loss:  253.20123\n",
      "episode: 2791/1000000, returns: 0.094, epsilon: 0.25\n",
      "regression loss:  1244.9589\n",
      "regression loss:  2537.7358\n",
      "episode: 2792/1000000, returns: 1.3, epsilon: 0.25\n",
      "regression loss:  2148.7222\n",
      "regression loss:  2324.6565\n",
      "episode: 2793/1000000, returns: -2.8, epsilon: 0.25\n",
      "regression loss:  958.78687\n",
      "regression loss:  1480.6212\n",
      "episode: 2794/1000000, returns: -2.6, epsilon: 0.25\n",
      "regression loss:  1300.6309\n",
      "regression loss:  1150.9893\n",
      "episode: 2795/1000000, returns: 2.2, epsilon: 0.25\n",
      "regression loss:  897.4458\n",
      "regression loss:  2933.6677\n",
      "episode: 2796/1000000, returns: 0.44, epsilon: 0.25\n",
      "regression loss:  1455.7332\n",
      "regression loss:  458.66556\n",
      "episode: 2797/1000000, returns: 2.1, epsilon: 0.25\n",
      "regression loss:  483.78226\n",
      "regression loss:  1670.6504\n",
      "episode: 2798/1000000, returns: -0.32, epsilon: 0.25\n",
      "regression loss:  3107.251\n",
      "regression loss:  2184.3\n",
      "episode: 2799/1000000, returns: 0.15, epsilon: 0.25\n",
      "regression loss:  2035.7701\n",
      "regression loss:  3489.5767\n",
      "episode: 2800/1000000, returns: -3.2, epsilon: 0.25\n",
      "regression loss:  10015.635\n",
      "regression loss:  9439.76\n",
      "episode: 2801/1000000, returns: 0.44, epsilon: 0.25\n",
      "regression loss:  3647.1816\n",
      "regression loss:  4529.7847\n",
      "episode: 2802/1000000, returns: 0.49, epsilon: 0.25\n",
      "regression loss:  3200.5034\n",
      "regression loss:  2075.179\n",
      "episode: 2803/1000000, returns: -1.3, epsilon: 0.25\n",
      "regression loss:  1170.6644\n",
      "regression loss:  2130.5925\n",
      "episode: 2804/1000000, returns: -2.0, epsilon: 0.25\n",
      "regression loss:  709.1019\n",
      "regression loss:  3317.5833\n",
      "episode: 2805/1000000, returns: -0.19, epsilon: 0.25\n",
      "regression loss:  3810.5496\n",
      "regression loss:  2768.5317\n",
      "episode: 2806/1000000, returns: -2.5, epsilon: 0.25\n",
      "regression loss:  190.00415\n",
      "regression loss:  2866.0833\n",
      "episode: 2807/1000000, returns: -1.8, epsilon: 0.25\n",
      "regression loss:  1076.3534\n",
      "regression loss:  3026.5957\n",
      "episode: 2808/1000000, returns: 1.8, epsilon: 0.25\n",
      "regression loss:  377.68884\n",
      "regression loss:  1556.3148\n",
      "episode: 2809/1000000, returns: -2.1, epsilon: 0.25\n",
      "regression loss:  247.89218\n",
      "regression loss:  1150.9589\n",
      "episode: 2810/1000000, returns: -1.8, epsilon: 0.25\n",
      "regression loss:  1462.2521\n",
      "regression loss:  1496.5956\n",
      "episode: 2811/1000000, returns: -0.26, epsilon: 0.25\n",
      "regression loss:  160.96959\n",
      "regression loss:  766.893\n",
      "episode: 2812/1000000, returns: 5.5, epsilon: 0.25\n",
      "regression loss:  165.22859\n",
      "regression loss:  1466.7837\n",
      "episode: 2813/1000000, returns: 0.32, epsilon: 0.24\n",
      "regression loss:  216.10869\n",
      "regression loss:  114.77079\n",
      "episode: 2814/1000000, returns: -0.18, epsilon: 0.24\n",
      "regression loss:  14.134744\n",
      "regression loss:  277.30142\n",
      "episode: 2815/1000000, returns: 2.4, epsilon: 0.24\n",
      "regression loss:  151.69206\n",
      "regression loss:  38.739037\n",
      "episode: 2816/1000000, returns: 6.5, epsilon: 0.24\n",
      "regression loss:  50.615623\n",
      "regression loss:  36.245743\n",
      "episode: 2817/1000000, returns: 3.3, epsilon: 0.24\n",
      "regression loss:  11.993699\n",
      "regression loss:  51.10252\n",
      "episode: 2818/1000000, returns: 3.3, epsilon: 0.24\n",
      "regression loss:  32.105488\n",
      "regression loss:  36.15569\n",
      "episode: 2819/1000000, returns: 0.24, epsilon: 0.24\n",
      "regression loss:  32.147625\n",
      "regression loss:  39.46993\n",
      "episode: 2820/1000000, returns: 1.9, epsilon: 0.24\n",
      "regression loss:  122.061485\n",
      "regression loss:  51.535305\n",
      "episode: 2821/1000000, returns: -4.2, epsilon: 0.24\n",
      "regression loss:  53.101143\n",
      "regression loss:  19.040688\n",
      "episode: 2822/1000000, returns: -1.4, epsilon: 0.24\n",
      "regression loss:  9.1697\n",
      "regression loss:  63.306446\n",
      "episode: 2823/1000000, returns: -0.44, epsilon: 0.24\n",
      "regression loss:  75.23839\n",
      "regression loss:  112.30766\n",
      "episode: 2824/1000000, returns: 1.6e+01, epsilon: 0.24\n",
      "regression loss:  168.28348\n",
      "regression loss:  117.220894\n",
      "episode: 2825/1000000, returns: 5.4, epsilon: 0.24\n",
      "regression loss:  9.290488\n",
      "regression loss:  2501.4226\n",
      "episode: 2826/1000000, returns: -2.1, epsilon: 0.24\n",
      "regression loss:  2079.063\n",
      "regression loss:  2313.713\n",
      "episode: 2827/1000000, returns: 1.9, epsilon: 0.24\n",
      "regression loss:  3462.0771\n",
      "regression loss:  823.6358\n",
      "episode: 2828/1000000, returns: 1.1e+01, epsilon: 0.24\n",
      "regression loss:  2737.7222\n",
      "regression loss:  2644.4514\n",
      "episode: 2829/1000000, returns: 0.12, epsilon: 0.24\n",
      "regression loss:  523.05634\n",
      "regression loss:  2237.251\n",
      "episode: 2830/1000000, returns: 1.6, epsilon: 0.24\n",
      "regression loss:  188.87775\n",
      "regression loss:  1116.88\n",
      "episode: 2831/1000000, returns: 3.9, epsilon: 0.24\n",
      "regression loss:  3014.5205\n",
      "regression loss:  2948.2695\n",
      "episode: 2832/1000000, returns: 0.74, epsilon: 0.24\n",
      "regression loss:  136.31992\n",
      "regression loss:  3883.6738\n",
      "episode: 2833/1000000, returns: 0.91, epsilon: 0.24\n",
      "regression loss:  1987.3655\n",
      "regression loss:  2858.7913\n",
      "episode: 2834/1000000, returns: -1.2, epsilon: 0.24\n",
      "regression loss:  303.63034\n",
      "regression loss:  2582.8823\n",
      "episode: 2835/1000000, returns: -3.5, epsilon: 0.24\n",
      "regression loss:  435.74744\n",
      "regression loss:  2893.1125\n",
      "episode: 2836/1000000, returns: 3.3, epsilon: 0.24\n",
      "regression loss:  3764.9011\n",
      "regression loss:  2797.246\n",
      "episode: 2837/1000000, returns: -0.048, epsilon: 0.24\n",
      "regression loss:  209.54971\n",
      "regression loss:  1823.4646\n",
      "episode: 2838/1000000, returns: 1.6, epsilon: 0.24\n",
      "regression loss:  226.50969\n",
      "regression loss:  3099.3442\n",
      "episode: 2839/1000000, returns: 1.4, epsilon: 0.24\n",
      "regression loss:  2915.8044\n",
      "regression loss:  1251.4518\n",
      "episode: 2840/1000000, returns: 0.6, epsilon: 0.24\n",
      "regression loss:  1138.5573\n",
      "regression loss:  2760.4841\n",
      "episode: 2841/1000000, returns: -4.5, epsilon: 0.24\n",
      "regression loss:  153.94476\n",
      "regression loss:  271.70764\n",
      "episode: 2842/1000000, returns: 0.55, epsilon: 0.24\n",
      "regression loss:  613.7633\n",
      "regression loss:  292.42844\n",
      "episode: 2843/1000000, returns: 3.9, epsilon: 0.24\n",
      "regression loss:  316.49277\n",
      "regression loss:  618.3091\n",
      "episode: 2844/1000000, returns: 3.6, epsilon: 0.24\n",
      "regression loss:  787.6314\n",
      "regression loss:  1337.7849\n",
      "episode: 2845/1000000, returns: 0.25, epsilon: 0.24\n",
      "regression loss:  1452.4178\n",
      "regression loss:  699.1181\n",
      "episode: 2846/1000000, returns: 3.3, epsilon: 0.24\n",
      "regression loss:  252.53955\n",
      "regression loss:  1536.5015\n",
      "episode: 2847/1000000, returns: 3.0, epsilon: 0.24\n",
      "regression loss:  428.4798\n",
      "regression loss:  2948.877\n",
      "episode: 2848/1000000, returns: -0.45, epsilon: 0.24\n",
      "regression loss:  1840.5634\n",
      "regression loss:  969.46326\n",
      "episode: 2849/1000000, returns: 3.1, epsilon: 0.24\n",
      "regression loss:  879.0216\n",
      "regression loss:  1236.0488\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 2850/1000000, returns: -7.9, epsilon: 0.24\n",
      "regression loss:  552.8904\n",
      "regression loss:  4510.443\n",
      "episode: 2851/1000000, returns: 5.2, epsilon: 0.24\n",
      "regression loss:  987.6566\n",
      "regression loss:  1180.2107\n",
      "episode: 2852/1000000, returns: 0.52, epsilon: 0.24\n",
      "regression loss:  2139.6333\n",
      "regression loss:  4151.3823\n",
      "episode: 2853/1000000, returns: 0.27, epsilon: 0.24\n",
      "regression loss:  906.4733\n",
      "regression loss:  2869.396\n",
      "episode: 2854/1000000, returns: 0.42, epsilon: 0.24\n",
      "regression loss:  2266.337\n",
      "regression loss:  3016.607\n",
      "episode: 2855/1000000, returns: 0.42, epsilon: 0.24\n",
      "regression loss:  1067.9541\n",
      "regression loss:  2638.8594\n",
      "episode: 2856/1000000, returns: 0.21, epsilon: 0.24\n",
      "regression loss:  356.15387\n",
      "regression loss:  10615.738\n",
      "episode: 2857/1000000, returns: 2.3, epsilon: 0.24\n",
      "regression loss:  1581.2542\n",
      "regression loss:  508.41904\n",
      "episode: 2858/1000000, returns: 2.4, epsilon: 0.24\n",
      "regression loss:  700.039\n",
      "regression loss:  1878.1741\n",
      "episode: 2859/1000000, returns: 0.53, epsilon: 0.24\n",
      "regression loss:  751.4424\n",
      "regression loss:  3389.2295\n",
      "episode: 2860/1000000, returns: 2.5, epsilon: 0.24\n",
      "regression loss:  1064.0433\n",
      "regression loss:  4188.72\n",
      "episode: 2861/1000000, returns: -0.03, epsilon: 0.24\n",
      "regression loss:  6210.478\n",
      "regression loss:  4446.8203\n",
      "episode: 2862/1000000, returns: -0.11, epsilon: 0.24\n",
      "regression loss:  2274.7778\n",
      "regression loss:  1731.6611\n",
      "episode: 2863/1000000, returns: -4.6, epsilon: 0.24\n",
      "regression loss:  2124.6157\n",
      "regression loss:  6106.071\n",
      "episode: 2864/1000000, returns: 3.4, epsilon: 0.24\n",
      "regression loss:  4222.7817\n",
      "regression loss:  4018.5413\n",
      "episode: 2865/1000000, returns: 1.2, epsilon: 0.24\n",
      "regression loss:  1187.8208\n",
      "regression loss:  3301.8\n",
      "episode: 2866/1000000, returns: -4.6, epsilon: 0.24\n",
      "regression loss:  1609.9023\n",
      "regression loss:  2392.281\n",
      "episode: 2867/1000000, returns: 2.9, epsilon: 0.24\n",
      "regression loss:  967.3142\n",
      "regression loss:  575.08026\n",
      "episode: 2868/1000000, returns: 0.41, epsilon: 0.24\n",
      "regression loss:  26.065025\n",
      "regression loss:  125.166595\n",
      "episode: 2869/1000000, returns: -0.56, epsilon: 0.24\n",
      "regression loss:  1125.5487\n",
      "regression loss:  4855.5103\n",
      "episode: 2870/1000000, returns: 1.5, epsilon: 0.24\n",
      "regression loss:  110.862854\n",
      "regression loss:  3987.2688\n",
      "episode: 2871/1000000, returns: 0.43, epsilon: 0.24\n",
      "regression loss:  1886.211\n",
      "regression loss:  3391.7605\n",
      "episode: 2872/1000000, returns: 3.3, epsilon: 0.24\n",
      "regression loss:  238.48798\n",
      "regression loss:  1389.7721\n",
      "episode: 2873/1000000, returns: -1.2e+01, epsilon: 0.24\n",
      "regression loss:  3719.9668\n",
      "regression loss:  3393.5713\n",
      "episode: 2874/1000000, returns: 2.3, epsilon: 0.24\n",
      "regression loss:  6227.1914\n",
      "regression loss:  4206.6104\n",
      "episode: 2875/1000000, returns: 0.82, epsilon: 0.24\n",
      "regression loss:  2314.975\n",
      "regression loss:  2114.7354\n",
      "episode: 2876/1000000, returns: -1.4, epsilon: 0.24\n",
      "regression loss:  1224.5519\n",
      "regression loss:  2522.1658\n",
      "episode: 2877/1000000, returns: -4.7, epsilon: 0.24\n",
      "regression loss:  6635.716\n",
      "regression loss:  9410.939\n",
      "episode: 2878/1000000, returns: 8.1, epsilon: 0.24\n",
      "regression loss:  2623.2534\n",
      "regression loss:  7321.139\n",
      "episode: 2879/1000000, returns: -2.1, epsilon: 0.24\n",
      "regression loss:  308.483\n",
      "regression loss:  1127.8293\n",
      "episode: 2880/1000000, returns: -2.7, epsilon: 0.24\n",
      "regression loss:  1537.4667\n",
      "regression loss:  2046.321\n",
      "episode: 2881/1000000, returns: 5.2, epsilon: 0.24\n",
      "regression loss:  335.1754\n",
      "regression loss:  864.76776\n",
      "episode: 2882/1000000, returns: 5.4, epsilon: 0.24\n",
      "regression loss:  306.48587\n",
      "regression loss:  1163.216\n",
      "episode: 2883/1000000, returns: 1.8, epsilon: 0.24\n",
      "regression loss:  30641.65\n",
      "regression loss:  6126.8926\n",
      "episode: 2884/1000000, returns: 0.11, epsilon: 0.24\n",
      "regression loss:  4528.308\n",
      "regression loss:  2602.3853\n",
      "episode: 2885/1000000, returns: -3.2, epsilon: 0.24\n",
      "regression loss:  156.22218\n",
      "regression loss:  779.5049\n",
      "episode: 2886/1000000, returns: 1.3, epsilon: 0.24\n",
      "regression loss:  3782.2654\n",
      "regression loss:  3036.6091\n",
      "episode: 2887/1000000, returns: 0.47, epsilon: 0.24\n",
      "regression loss:  49.170567\n",
      "regression loss:  2690.9734\n",
      "episode: 2888/1000000, returns: -0.045, epsilon: 0.24\n",
      "regression loss:  1423.5876\n",
      "regression loss:  1080.101\n",
      "episode: 2889/1000000, returns: 3.4, epsilon: 0.24\n",
      "regression loss:  1398.2039\n",
      "regression loss:  1569.6306\n",
      "episode: 2890/1000000, returns: 9.9, epsilon: 0.24\n",
      "regression loss:  292.42557\n",
      "regression loss:  1001.3538\n",
      "episode: 2891/1000000, returns: -2.6, epsilon: 0.24\n",
      "regression loss:  57.30345\n",
      "regression loss:  1056.3751\n",
      "episode: 2892/1000000, returns: 0.64, epsilon: 0.24\n",
      "regression loss:  428.93613\n",
      "regression loss:  966.1462\n",
      "episode: 2893/1000000, returns: 3.3, epsilon: 0.24\n",
      "regression loss:  1294.8607\n",
      "regression loss:  2767.2112\n",
      "episode: 2894/1000000, returns: 1.3e+01, epsilon: 0.24\n",
      "regression loss:  1414.1628\n",
      "regression loss:  3163.164\n",
      "episode: 2895/1000000, returns: 0.75, epsilon: 0.24\n",
      "regression loss:  2144.893\n",
      "regression loss:  1605.6351\n",
      "episode: 2896/1000000, returns: 1.3e+01, epsilon: 0.23\n",
      "regression loss:  876.2296\n",
      "regression loss:  633.846\n",
      "episode: 2897/1000000, returns: 6.2, epsilon: 0.23\n",
      "regression loss:  540.2726\n",
      "regression loss:  4524.4907\n",
      "episode: 2898/1000000, returns: -6.8, epsilon: 0.23\n",
      "regression loss:  1471.2571\n",
      "regression loss:  548.4177\n",
      "episode: 2899/1000000, returns: 2.3, epsilon: 0.23\n",
      "regression loss:  2139.1226\n",
      "regression loss:  2529.1392\n",
      "episode: 2900/1000000, returns: 0.45, epsilon: 0.23\n",
      "regression loss:  385.93478\n",
      "regression loss:  2229.9583\n",
      "episode: 2901/1000000, returns: 9.6, epsilon: 0.23\n",
      "regression loss:  118.56804\n",
      "regression loss:  732.485\n",
      "episode: 2902/1000000, returns: -1e+01, epsilon: 0.23\n",
      "regression loss:  101.93933\n",
      "regression loss:  982.25653\n",
      "episode: 2903/1000000, returns: -3.3, epsilon: 0.23\n",
      "regression loss:  2235.6462\n",
      "regression loss:  1096.9447\n",
      "episode: 2904/1000000, returns: 0.6, epsilon: 0.23\n",
      "regression loss:  3393.4158\n",
      "regression loss:  254.54323\n",
      "episode: 2905/1000000, returns: -0.37, epsilon: 0.23\n",
      "regression loss:  29.647963\n",
      "regression loss:  36891.0\n",
      "episode: 2906/1000000, returns: -0.33, epsilon: 0.23\n",
      "regression loss:  11983.832\n",
      "regression loss:  21498.424\n",
      "episode: 2907/1000000, returns: -3.7, epsilon: 0.23\n",
      "regression loss:  11543.306\n",
      "regression loss:  13419.848\n",
      "episode: 2908/1000000, returns: 0.24, epsilon: 0.23\n",
      "regression loss:  5889.5425\n",
      "regression loss:  11479.049\n",
      "episode: 2909/1000000, returns: -0.4, epsilon: 0.23\n",
      "regression loss:  16016.797\n",
      "regression loss:  12252.107\n",
      "episode: 2910/1000000, returns: 3.2e+01, epsilon: 0.23\n",
      "regression loss:  3663.526\n",
      "regression loss:  4973.507\n",
      "episode: 2911/1000000, returns: 4.9, epsilon: 0.23\n",
      "regression loss:  177608.2\n",
      "regression loss:  29914.652\n",
      "episode: 2912/1000000, returns: -2.1, epsilon: 0.23\n",
      "regression loss:  129565.33\n",
      "regression loss:  18331.418\n",
      "episode: 2913/1000000, returns: 0.72, epsilon: 0.23\n",
      "regression loss:  2037.5051\n",
      "regression loss:  8930.158\n",
      "episode: 2914/1000000, returns: 0.12, epsilon: 0.23\n",
      "regression loss:  8755.014\n",
      "regression loss:  24897.945\n",
      "episode: 2915/1000000, returns: 0.45, epsilon: 0.23\n",
      "regression loss:  11206.502\n",
      "regression loss:  20338.324\n",
      "episode: 2916/1000000, returns: 1.8, epsilon: 0.23\n",
      "regression loss:  5695.381\n",
      "regression loss:  5773.484\n",
      "episode: 2917/1000000, returns: 1.1e+01, epsilon: 0.23\n",
      "regression loss:  5288.223\n",
      "regression loss:  16465.322\n",
      "episode: 2918/1000000, returns: 2.0, epsilon: 0.23\n",
      "regression loss:  5040.6074\n",
      "regression loss:  8247.096\n",
      "episode: 2919/1000000, returns: -1.0, epsilon: 0.23\n",
      "regression loss:  1083.9547\n",
      "regression loss:  6103.3486\n",
      "episode: 2920/1000000, returns: -0.48, epsilon: 0.23\n",
      "regression loss:  4019.7312\n",
      "regression loss:  11349.472\n",
      "episode: 2921/1000000, returns: 1.2, epsilon: 0.23\n",
      "regression loss:  2743.137\n",
      "regression loss:  17202.168\n",
      "episode: 2922/1000000, returns: 7.6, epsilon: 0.23\n",
      "regression loss:  754.348\n",
      "regression loss:  2839.9343\n",
      "episode: 2923/1000000, returns: -0.43, epsilon: 0.23\n",
      "regression loss:  1990.6583\n",
      "regression loss:  4244.0854\n",
      "episode: 2924/1000000, returns: -7.6, epsilon: 0.23\n",
      "regression loss:  1438.8031\n",
      "regression loss:  1043.452\n",
      "episode: 2925/1000000, returns: -0.91, epsilon: 0.23\n",
      "regression loss:  584.28656\n",
      "regression loss:  4636.9927\n",
      "episode: 2926/1000000, returns: 0.85, epsilon: 0.23\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "regression loss:  674.93506\n",
      "regression loss:  2266.8901\n",
      "episode: 2927/1000000, returns: -2.4, epsilon: 0.23\n",
      "regression loss:  884.5117\n",
      "regression loss:  1784.3137\n",
      "episode: 2928/1000000, returns: 0.092, epsilon: 0.23\n",
      "regression loss:  958.01886\n",
      "regression loss:  1889.3649\n",
      "episode: 2929/1000000, returns: 4.4, epsilon: 0.23\n",
      "regression loss:  1122.5975\n",
      "regression loss:  2673.7703\n",
      "episode: 2930/1000000, returns: 0.25, epsilon: 0.23\n",
      "regression loss:  1021.3313\n",
      "regression loss:  2899.9282\n",
      "episode: 2931/1000000, returns: -9.9, epsilon: 0.23\n",
      "regression loss:  2072.2705\n",
      "regression loss:  2892.9402\n",
      "episode: 2932/1000000, returns: 2.7, epsilon: 0.23\n",
      "regression loss:  21.711111\n",
      "regression loss:  116.93664\n",
      "episode: 2933/1000000, returns: -2.7, epsilon: 0.23\n",
      "regression loss:  59.4807\n",
      "regression loss:  21.686024\n",
      "episode: 2934/1000000, returns: -3.8, epsilon: 0.23\n",
      "regression loss:  62.722816\n",
      "regression loss:  195.49188\n",
      "episode: 2935/1000000, returns: -0.79, epsilon: 0.23\n",
      "regression loss:  119.831825\n",
      "regression loss:  107.57898\n",
      "episode: 2936/1000000, returns: -6.1, epsilon: 0.23\n",
      "regression loss:  160.44058\n",
      "regression loss:  131.46178\n",
      "episode: 2937/1000000, returns: 3.9, epsilon: 0.23\n",
      "regression loss:  16.746088\n",
      "regression loss:  79.764565\n",
      "episode: 2938/1000000, returns: 4.4, epsilon: 0.23\n",
      "regression loss:  28.37446\n",
      "regression loss:  1623.8474\n",
      "episode: 2939/1000000, returns: -5.9, epsilon: 0.23\n",
      "regression loss:  112.90354\n",
      "regression loss:  228.75652\n",
      "episode: 2940/1000000, returns: 0.7, epsilon: 0.23\n",
      "regression loss:  165.85715\n",
      "regression loss:  1299.9436\n",
      "episode: 2941/1000000, returns: -0.093, epsilon: 0.23\n",
      "regression loss:  330.73886\n",
      "regression loss:  1146.8204\n",
      "episode: 2942/1000000, returns: 8.7, epsilon: 0.23\n",
      "regression loss:  170.81711\n",
      "regression loss:  227.38176\n",
      "episode: 2943/1000000, returns: 3.5, epsilon: 0.23\n",
      "regression loss:  1660.2781\n",
      "regression loss:  940.7383\n",
      "episode: 2944/1000000, returns: 9.7, epsilon: 0.23\n",
      "regression loss:  151.48361\n",
      "regression loss:  326.8229\n",
      "episode: 2945/1000000, returns: 1.6e+01, epsilon: 0.23\n",
      "regression loss:  84.179756\n",
      "regression loss:  1179.2521\n",
      "episode: 2946/1000000, returns: 6.1, epsilon: 0.23\n",
      "regression loss:  8577.478\n",
      "regression loss:  7932.7983\n",
      "episode: 2947/1000000, returns: 1.2, epsilon: 0.23\n",
      "regression loss:  3689.3684\n",
      "regression loss:  6770.505\n",
      "episode: 2948/1000000, returns: 3.1, epsilon: 0.23\n",
      "regression loss:  556.3515\n",
      "regression loss:  1387.8289\n",
      "episode: 2949/1000000, returns: -0.42, epsilon: 0.23\n",
      "regression loss:  1262.136\n",
      "regression loss:  7797.672\n",
      "episode: 2950/1000000, returns: -0.35, epsilon: 0.23\n",
      "regression loss:  1971.0471\n",
      "regression loss:  6722.586\n",
      "episode: 2951/1000000, returns: -1.4, epsilon: 0.23\n",
      "regression loss:  4455.7505\n",
      "regression loss:  3874.0781\n",
      "episode: 2952/1000000, returns: 1.2, epsilon: 0.23\n",
      "regression loss:  3971.1377\n",
      "regression loss:  1670.217\n",
      "episode: 2953/1000000, returns: 0.25, epsilon: 0.23\n",
      "regression loss:  2427.7493\n",
      "regression loss:  4744.2783\n",
      "episode: 2954/1000000, returns: 0.25, epsilon: 0.23\n",
      "regression loss:  1479.3588\n",
      "regression loss:  4412.579\n",
      "episode: 2955/1000000, returns: -3.4, epsilon: 0.23\n",
      "regression loss:  1755.2771\n",
      "regression loss:  14092.284\n",
      "episode: 2956/1000000, returns: 0.22, epsilon: 0.23\n",
      "regression loss:  714.85187\n",
      "regression loss:  1369.2451\n",
      "episode: 2957/1000000, returns: -3.2, epsilon: 0.23\n",
      "regression loss:  780.5362\n",
      "regression loss:  3297.204\n",
      "episode: 2958/1000000, returns: -7.6, epsilon: 0.23\n",
      "regression loss:  879.4002\n",
      "regression loss:  1747.5702\n",
      "episode: 2959/1000000, returns: 1.8, epsilon: 0.23\n",
      "regression loss:  113.40963\n",
      "regression loss:  359.15573\n",
      "episode: 2960/1000000, returns: 8.2, epsilon: 0.23\n",
      "regression loss:  407.33176\n",
      "regression loss:  221.71948\n",
      "episode: 2961/1000000, returns: -0.73, epsilon: 0.23\n",
      "regression loss:  55.879642\n",
      "regression loss:  125.41785\n",
      "episode: 2962/1000000, returns: 0.43, epsilon: 0.23\n",
      "regression loss:  3277.129\n",
      "regression loss:  6003.923\n",
      "episode: 2963/1000000, returns: -1.7, epsilon: 0.23\n",
      "regression loss:  1791.1373\n",
      "regression loss:  2108.593\n",
      "episode: 2964/1000000, returns: 0.69, epsilon: 0.23\n",
      "regression loss:  390.88907\n",
      "regression loss:  1902.8835\n",
      "episode: 2965/1000000, returns: -1e+01, epsilon: 0.23\n",
      "regression loss:  78.86704\n",
      "regression loss:  169.14395\n",
      "episode: 2966/1000000, returns: -3.2, epsilon: 0.23\n",
      "regression loss:  57.061234\n",
      "regression loss:  45.063057\n",
      "episode: 2967/1000000, returns: 5.1, epsilon: 0.23\n",
      "regression loss:  11.035853\n",
      "regression loss:  2832.0696\n",
      "episode: 2968/1000000, returns: 2.7, epsilon: 0.23\n",
      "regression loss:  964.849\n",
      "regression loss:  2063.6194\n",
      "episode: 2969/1000000, returns: 0.45, epsilon: 0.23\n",
      "regression loss:  2212.9172\n",
      "regression loss:  1125.6816\n",
      "episode: 2970/1000000, returns: 2.1, epsilon: 0.23\n",
      "regression loss:  1423.6838\n",
      "regression loss:  1350.5042\n",
      "episode: 2971/1000000, returns: 1.1, epsilon: 0.23\n",
      "regression loss:  452.44562\n",
      "regression loss:  2405.8555\n",
      "episode: 2972/1000000, returns: -0.18, epsilon: 0.23\n",
      "regression loss:  455.1877\n",
      "regression loss:  1679.7977\n",
      "episode: 2973/1000000, returns: 2.5, epsilon: 0.23\n",
      "regression loss:  1031.722\n",
      "regression loss:  1029.6696\n",
      "episode: 2974/1000000, returns: 2.0, epsilon: 0.23\n",
      "regression loss:  100.71562\n",
      "regression loss:  1152.4749\n",
      "episode: 2975/1000000, returns: 0.2, epsilon: 0.23\n",
      "regression loss:  725.2946\n",
      "regression loss:  2021.9373\n",
      "episode: 2976/1000000, returns: -2.8, epsilon: 0.23\n",
      "regression loss:  180.0392\n",
      "regression loss:  2097.2031\n",
      "episode: 2977/1000000, returns: 0.46, epsilon: 0.23\n",
      "regression loss:  769.7198\n",
      "regression loss:  1806.6558\n",
      "episode: 2978/1000000, returns: -5.4, epsilon: 0.23\n",
      "regression loss:  939.4611\n",
      "regression loss:  5374.5938\n",
      "episode: 2979/1000000, returns: 2.8, epsilon: 0.23\n",
      "regression loss:  459.25153\n",
      "regression loss:  2947.6853\n",
      "episode: 2980/1000000, returns: 0.44, epsilon: 0.23\n",
      "regression loss:  316.2561\n",
      "regression loss:  3678.2947\n",
      "episode: 2981/1000000, returns: -0.16, epsilon: 0.23\n",
      "regression loss:  767.0898\n",
      "regression loss:  1486.3066\n",
      "episode: 2982/1000000, returns: 1.2, epsilon: 0.23\n",
      "regression loss:  934.32556\n",
      "regression loss:  3999.3877\n",
      "episode: 2983/1000000, returns: -0.16, epsilon: 0.22\n",
      "regression loss:  452.89624\n",
      "regression loss:  5233.3457\n",
      "episode: 2984/1000000, returns: 2.1, epsilon: 0.22\n",
      "regression loss:  369.62732\n",
      "regression loss:  1951.7904\n",
      "episode: 2985/1000000, returns: 0.24, epsilon: 0.22\n",
      "regression loss:  1586.5277\n",
      "regression loss:  4170.1177\n",
      "episode: 2986/1000000, returns: 0.72, epsilon: 0.22\n",
      "regression loss:  1023.3825\n",
      "regression loss:  2918.092\n",
      "episode: 2987/1000000, returns: -1.2, epsilon: 0.22\n",
      "regression loss:  1461.7257\n",
      "regression loss:  3760.5151\n",
      "episode: 2988/1000000, returns: 0.91, epsilon: 0.22\n",
      "regression loss:  1431.0018\n",
      "regression loss:  2452.3958\n",
      "episode: 2989/1000000, returns: -5.6, epsilon: 0.22\n",
      "regression loss:  434.87006\n",
      "regression loss:  1777.941\n",
      "episode: 2990/1000000, returns: 0.71, epsilon: 0.22\n",
      "regression loss:  436.4687\n",
      "regression loss:  879.50433\n",
      "episode: 2991/1000000, returns: 0.3, epsilon: 0.22\n",
      "regression loss:  456.17923\n",
      "regression loss:  4456.7627\n",
      "episode: 2992/1000000, returns: 0.45, epsilon: 0.22\n",
      "regression loss:  695.31445\n",
      "regression loss:  723.5657\n",
      "episode: 2993/1000000, returns: 1.0, epsilon: 0.22\n",
      "regression loss:  2538.5742\n",
      "regression loss:  3453.6545\n",
      "episode: 2994/1000000, returns: -0.83, epsilon: 0.22\n",
      "regression loss:  1308.6326\n",
      "regression loss:  2192.5144\n",
      "episode: 2995/1000000, returns: 2.2, epsilon: 0.22\n",
      "regression loss:  596.7485\n",
      "regression loss:  2539.0513\n",
      "episode: 2996/1000000, returns: -1e+01, epsilon: 0.22\n",
      "regression loss:  435.76712\n",
      "regression loss:  3794.206\n",
      "episode: 2997/1000000, returns: 2.6, epsilon: 0.22\n",
      "regression loss:  489.33496\n",
      "regression loss:  4408.946\n",
      "episode: 2998/1000000, returns: 2.5, epsilon: 0.22\n",
      "regression loss:  8.701696\n",
      "regression loss:  3318.957\n",
      "episode: 2999/1000000, returns: 1.1, epsilon: 0.22\n",
      "regression loss:  223.26715\n",
      "regression loss:  207.35397\n",
      "episode: 3000/1000000, returns: 0.85, epsilon: 0.22\n",
      "regression loss:  269.0098\n",
      "regression loss:  1988.2914\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-23f9f2070adc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0msess\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexperiment_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mexperiment_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_start\u001b[0m\u001b[0;34m,\u001b[0m             \u001b[0mend_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_end\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_episodes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1000000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mauto_save_and_load\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# SHOULD USE MORE EPOCHS!!!\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Desktop/CS341/Trading/StrawberryDQNAgent.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, experiment_name, session, start_time, end_time, num_episodes, verbose, auto_save_and_load)\u001b[0m\n\u001b[1;32m    340\u001b[0m \u001b[0;31m#                 self.__replay(session, self.batch_size)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    341\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__replay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 342\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__update_target_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    343\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m             \u001b[0mcum_return\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mportfolio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetReturnsPercent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetCurrentPrice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/CS341/Trading/StrawberryDQNAgent.py\u001b[0m in \u001b[0;36m__update_target_model\u001b[0;34m(self, sess)\u001b[0m\n\u001b[1;32m    161\u001b[0m             assignments = [v.assign(self.old_vars[v.name.split('model/')[-1]]) \\\n\u001b[1;32m    162\u001b[0m                            for v in tf.trainable_variables() if v.name.startswith(scope.name + \"/\")]\n\u001b[0;32m--> 163\u001b[0;31m             \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0massignments\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    164\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mprint_my_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    903\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    904\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 905\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    906\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    907\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1138\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1139\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1140\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1141\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1142\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1319\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1320\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1321\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1322\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1323\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1325\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1326\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1327\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1328\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1329\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1308\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_run_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1309\u001b[0m       \u001b[0;31m# Ensure any changes to the graph are reflected in the runtime.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1310\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1311\u001b[0m       return self._call_tf_sessionrun(\n\u001b[1;32m   1312\u001b[0m           options, feed_dict, fetch_list, target_list, run_metadata)\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_extend_graph\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1351\u001b[0m           \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1352\u001b[0m           graph_def, self._current_version = self._graph._as_graph_def(\n\u001b[0;32m-> 1353\u001b[0;31m               from_version=self._current_version, add_shapes=self._add_shapes)\n\u001b[0m\u001b[1;32m   1354\u001b[0m           \u001b[0;31m# pylint: enable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1355\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m_as_graph_def\u001b[0;34m(self, from_version, add_shapes)\u001b[0m\n\u001b[1;32m   3082\u001b[0m         \u001b[0mgraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mversions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCopyFrom\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_graph_def_versions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3083\u001b[0m         \u001b[0mbytesize\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3084\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mop_id\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nodes_by_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3085\u001b[0m           \u001b[0mop\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nodes_by_id\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mop_id\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3086\u001b[0m           \u001b[0;32mif\u001b[0m \u001b[0mfrom_version\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mop_id\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mfrom_version\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_start = datetime.datetime(2017,1,1,0)\n",
    "train_end = datetime.datetime(2018,1,1,0)\n",
    "\n",
    "sess = tf.Session()\n",
    "agent.train(experiment_name=experiment_name, session=sess, start_time = train_start, \\\n",
    "            end_time = train_end, num_episodes=1000000, verbose=False, auto_save_and_load=False) # SHOULD USE MORE EPOCHS!!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sanity check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.env.set_current_time(datetime.datetime(2017,12,1,0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.9532575377826319, 0.9267650050798705, 0.9401831129058176, 0.9304050337685525, 0.9406253897460588, 0.007633262057391906, 0.009181824126555549, 0.0, 2.0, 2.0]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[22.258205  , -0.13544695, 22.522032  ]], dtype=float32)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.env.step()\n",
    "state = agent.env.getStatesSequence() + agent.portfolio.getStates()\n",
    "agent.preprocess(state, agent.env.getCurrentPrice())\n",
    "# state -= agent.state_mean\n",
    "print(state[:10])\n",
    "\n",
    "agent.model.predict(sess, state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save Model (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./Strawberry/model.ckpt'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.saver.save(sess, \"./\"+experiment_name+\"/model.ckpt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test on (2017-09-01 ~ 2018,1,1,0) Cheating..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing from  2017-09-01 00:00:00  to 2018-01-01 00:00:00 :  ~ 122 days\n",
      "\n",
      "Current time: 2017-09-01 00:00:00\n",
      "Action start: Action.BUY , Total value before action: 5000.0\n",
      "Before buying: coin:0.000, cash:5000.000, buy price:4738.100\n",
      "After buying: coin bought:0.106, transaction fees:1.250, coin now:0.106, cash now:4498.750\n",
      "Action end:  Action.BUY , Total value now: 4998.750.  , Return since entry: -0.025 %\n",
      "\n",
      "Current time: 2017-09-02 00:00:00\n",
      "Action start: Action.BUY , Total value before action: 5043.521220695726\n",
      "Before buying: coin:0.309, cash:3526.057, buy price:4906.800\n",
      "After buying: coin bought:0.072, transaction fees:0.882, coin now:0.381, cash now:3172.570\n",
      "Action end:  Action.BUY , Total value now: 5042.640.  , Return since entry: 0.853 %\n",
      "\n",
      "Current time: 2017-09-03 00:00:00\n",
      "Action start: Action.BUY , Total value before action: 4924.619428819393\n",
      "Before buying: coin:0.323, cash:3442.661, buy price:4591.637\n",
      "After buying: coin bought:0.075, transaction fees:0.861, coin now:0.398, cash now:3097.534\n",
      "Action end:  Action.BUY , Total value now: 4923.759.  , Return since entry: -1.525 %\n",
      "\n",
      "Current time: 2017-09-04 00:00:00\n",
      "Action start: Action.SELL , Total value before action: 4905.37770142816\n",
      "Before selling: coin:0.354, cash:3298.932, sell price:4534.902\n",
      "After selling: coin sold:0.035, transaction fees:0.402, coin now:0.319, cash now:3459.175\n",
      "Action end:  Action.SELL , Total value now: 4904.976.  , Return since entry: -1.900 %\n",
      "\n",
      "Current time: 2017-09-05 00:00:00\n",
      "Action start: Action.SELL , Total value before action: 4736.992161527523\n",
      "Before selling: coin:0.372, cash:3201.303, sell price:4125.021\n",
      "After selling: coin sold:0.037, transaction fees:0.384, coin now:0.335, cash now:3354.488\n",
      "Action end:  Action.SELL , Total value now: 4736.608.  , Return since entry: -5.268 %\n",
      "\n",
      "Current time: 2017-09-06 00:00:00\n",
      "Action start: Action.SELL , Total value before action: 4842.539304304074\n",
      "Before selling: coin:0.390, cash:3115.632, sell price:4432.570\n",
      "After selling: coin sold:0.039, transaction fees:0.432, coin now:0.351, cash now:3287.891\n",
      "Action end:  Action.SELL , Total value now: 4842.108.  , Return since entry: -3.158 %\n",
      "\n",
      "Current time: 2017-09-07 00:00:00\n",
      "Action start: Action.SELL , Total value before action: 4857.401623139931\n",
      "Before selling: coin:0.395, cash:3072.973, sell price:4520.452\n",
      "After selling: coin sold:0.039, transaction fees:0.446, coin now:0.355, cash now:3250.970\n",
      "Action end:  Action.SELL , Total value now: 4856.956.  , Return since entry: -2.861 %\n",
      "\n",
      "Current time: 2017-09-08 00:00:00\n",
      "Action start: Action.BUY , Total value before action: 4868.947866802249\n",
      "Before buying: coin:0.304, cash:3468.635, buy price:4600.000\n",
      "After buying: coin bought:0.075, transaction fees:0.867, coin now:0.380, cash now:3120.905\n",
      "Action end:  Action.BUY , Total value now: 4868.081.  , Return since entry: -2.638 %\n",
      "\n",
      "Current time: 2017-09-09 00:00:00\n",
      "Action start: Action.BUY , Total value before action: 4726.927126814291\n",
      "Before buying: coin:0.324, cash:3338.927, buy price:4277.434\n",
      "After buying: coin bought:0.078, transaction fees:0.835, coin now:0.403, cash now:3004.199\n",
      "Action end:  Action.BUY , Total value now: 4726.092.  , Return since entry: -5.478 %\n",
      "\n",
      "Current time: 2017-09-10 00:00:00\n",
      "Action start: Action.BUY , Total value before action: 4721.588808418857\n",
      "Before buying: coin:0.330, cash:3308.374, buy price:4287.529\n",
      "After buying: coin bought:0.077, transaction fees:0.827, coin now:0.407, cash now:2976.709\n",
      "Action end:  Action.BUY , Total value now: 4720.762.  , Return since entry: -5.585 %\n",
      "\n",
      "Current time: 2017-09-11 00:00:00\n",
      "Action start: Action.SELL , Total value before action: 4700.639943378112\n",
      "Before selling: coin:0.351, cash:3220.108, sell price:4212.196\n",
      "After selling: coin sold:0.035, transaction fees:0.370, coin now:0.316, cash now:3367.791\n",
      "Action end:  Action.SELL , Total value now: 4700.270.  , Return since entry: -5.995 %\n",
      "\n",
      "Current time: 2017-09-12 00:00:00\n",
      "Action start: Action.SELL , Total value before action: 4686.33964087972\n",
      "Before selling: coin:0.367, cash:3129.010, sell price:4238.364\n",
      "After selling: coin sold:0.037, transaction fees:0.389, coin now:0.331, cash now:3284.354\n",
      "Action end:  Action.SELL , Total value now: 4685.950.  , Return since entry: -6.281 %\n",
      "\n",
      "Current time: 2017-09-13 00:00:00\n",
      "Action start: Action.SELL , Total value before action: 4636.34284152427\n",
      "Before selling: coin:0.405, cash:2964.507, sell price:4126.289\n",
      "After selling: coin sold:0.041, transaction fees:0.418, coin now:0.365, cash now:3131.273\n",
      "Action end:  Action.SELL , Total value now: 4635.925.  , Return since entry: -7.282 %\n",
      "\n",
      "Current time: 2017-09-14 00:00:00\n",
      "Action start: Action.SELL , Total value before action: 4558.625340966568\n",
      "Before selling: coin:0.431, cash:2874.303, sell price:3912.000\n",
      "After selling: coin sold:0.043, transaction fees:0.421, coin now:0.387, cash now:3042.314\n",
      "Action end:  Action.SELL , Total value now: 4558.204.  , Return since entry: -8.836 %\n",
      "\n",
      "Current time: 2017-09-15 00:00:00\n",
      "Action start: Action.SELL , Total value before action: 4324.639855977105\n",
      "Before selling: coin:0.467, cash:2768.607, sell price:3328.793\n",
      "After selling: coin sold:0.047, transaction fees:0.389, coin now:0.421, cash now:2923.821\n",
      "Action end:  Action.SELL , Total value now: 4324.251.  , Return since entry: -13.515 %\n",
      "\n",
      "Current time: 2017-09-16 00:00:00\n",
      "Action start: Action.BUY , Total value before action: 4469.40598262043\n",
      "Before buying: coin:0.358, cash:3134.544, buy price:3724.116\n",
      "After buying: coin bought:0.084, transaction fees:0.784, coin now:0.443, cash now:2820.306\n",
      "Action end:  Action.BUY , Total value now: 4468.622.  , Return since entry: -10.628 %\n",
      "\n",
      "Current time: 2017-09-17 00:00:00\n",
      "Action start: Action.SELL , Total value before action: 4445.9993043394825\n",
      "Before selling: coin:0.396, cash:2994.844, sell price:3664.674\n",
      "After selling: coin sold:0.040, transaction fees:0.363, coin now:0.356, cash now:3139.596\n",
      "Action end:  Action.SELL , Total value now: 4445.637.  , Return since entry: -11.087 %\n",
      "\n",
      "Current time: 2017-09-18 00:00:00\n",
      "Action start: Action.SELL , Total value before action: 4459.623180595482\n",
      "Before selling: coin:0.429, cash:2861.477, sell price:3729.573\n",
      "After selling: coin sold:0.043, transaction fees:0.400, coin now:0.386, cash now:3020.892\n",
      "Action end:  Action.SELL , Total value now: 4459.224.  , Return since entry: -10.816 %\n",
      "\n",
      "Current time: 2017-09-19 00:00:00\n",
      "Action start: Action.BUY , Total value before action: 4540.2794691186555\n",
      "Before buying: coin:0.333, cash:3209.475, buy price:3995.566\n",
      "After buying: coin bought:0.080, transaction fees:0.802, coin now:0.413, cash now:2887.725\n",
      "Action end:  Action.BUY , Total value now: 4539.477.  , Return since entry: -9.210 %\n",
      "\n",
      "Current time: 2017-09-20 00:00:00\n",
      "Action start: Action.BUY , Total value before action: 4479.10886109093\n",
      "Before buying: coin:0.340, cash:3153.151, buy price:3900.500\n",
      "After buying: coin bought:0.081, transaction fees:0.788, coin now:0.421, cash now:2837.048\n",
      "Action end:  Action.BUY , Total value now: 4478.321.  , Return since entry: -10.434 %\n",
      "\n",
      "Current time: 2017-09-21 00:00:00\n",
      "Action start: Action.SELL , Total value before action: 4440.63947678531\n",
      "Before selling: coin:0.362, cash:3049.682, sell price:3840.000\n",
      "After selling: coin sold:0.036, transaction fees:0.348, coin now:0.326, cash now:3188.430\n",
      "Action end:  Action.SELL , Total value now: 4440.292.  , Return since entry: -11.194 %\n",
      "\n",
      "Current time: 2017-09-22 00:00:00\n",
      "Action start: Action.SELL , Total value before action: 4345.923177512306\n",
      "Before selling: coin:0.389, cash:2928.149, sell price:3643.640\n",
      "After selling: coin sold:0.039, transaction fees:0.354, coin now:0.350, cash now:3069.572\n",
      "Action end:  Action.SELL , Total value now: 4345.569.  , Return since entry: -13.089 %\n",
      "\n",
      "Current time: 2017-09-23 00:00:00\n",
      "Action start: Action.SELL , Total value before action: 4316.9565585617165\n",
      "Before selling: coin:0.436, cash:2749.304, sell price:3592.559\n",
      "After selling: coin sold:0.044, transaction fees:0.392, coin now:0.393, cash now:2905.677\n",
      "Action end:  Action.SELL , Total value now: 4316.565.  , Return since entry: -13.669 %\n",
      "\n",
      "Current time: 2017-09-24 00:00:00\n",
      "Action start: Action.BUY , Total value before action: 4376.3193410131535\n",
      "Before buying: coin:0.345, cash:3081.922, buy price:3748.000\n",
      "After buying: coin bought:0.082, transaction fees:0.770, coin now:0.428, cash now:2772.959\n",
      "Action end:  Action.BUY , Total value now: 4375.549.  , Return since entry: -12.489 %\n",
      "\n",
      "Current time: 2017-09-25 00:00:00\n",
      "Action start: Action.BUY , Total value before action: 4336.156319282949\n",
      "Before buying: coin:0.352, cash:3042.549, buy price:3671.709\n",
      "After buying: coin bought:0.083, transaction fees:0.761, coin now:0.435, cash now:2737.534\n",
      "Action end:  Action.BUY , Total value now: 4335.396.  , Return since entry: -13.292 %\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current time: 2017-09-26 00:00:00\n",
      "Action start: Action.SELL , Total value before action: 4425.279011224328\n",
      "Before selling: coin:0.386, cash:2907.019, sell price:3935.100\n",
      "After selling: coin sold:0.039, transaction fees:0.380, coin now:0.347, cash now:3058.465\n",
      "Action end:  Action.SELL , Total value now: 4424.899.  , Return since entry: -11.502 %\n",
      "\n",
      "Current time: 2017-09-27 00:00:00\n",
      "Action start: Action.SELL , Total value before action: 4407.270658149797\n",
      "Before selling: coin:0.414, cash:2788.231, sell price:3914.700\n",
      "After selling: coin sold:0.041, transaction fees:0.405, coin now:0.372, cash now:2949.730\n",
      "Action end:  Action.SELL , Total value now: 4406.866.  , Return since entry: -11.863 %\n",
      "\n",
      "Current time: 2017-09-28 00:00:00\n",
      "Action start: Action.BUY , Total value before action: 4487.532265966885\n",
      "Before buying: coin:0.317, cash:3166.356, buy price:4174.160\n",
      "After buying: coin bought:0.076, transaction fees:0.792, coin now:0.392, cash now:2848.929\n",
      "Action end:  Action.BUY , Total value now: 4486.741.  , Return since entry: -10.265 %\n",
      "\n",
      "Current time: 2017-09-29 00:00:00\n",
      "Action start: Action.BUY , Total value before action: 4477.386123422218\n",
      "Before buying: coin:0.320, cash:3136.405, buy price:4194.000\n",
      "After buying: coin bought:0.075, transaction fees:0.784, coin now:0.395, cash now:2821.980\n",
      "Action end:  Action.BUY , Total value now: 4476.602.  , Return since entry: -10.468 %\n",
      "\n",
      "Current time: 2017-09-30 00:00:00\n",
      "Action start: Action.SELL , Total value before action: 4469.723364596679\n",
      "Before selling: coin:0.346, cash:3020.111, sell price:4191.000\n",
      "After selling: coin sold:0.035, transaction fees:0.362, coin now:0.311, cash now:3164.710\n",
      "Action end:  Action.SELL , Total value now: 4469.361.  , Return since entry: -10.613 %\n",
      "\n",
      "Current time: 2017-10-01 00:00:00\n",
      "Action start: Action.SELL , Total value before action: 4514.434851590581\n",
      "Before selling: coin:0.369, cash:2903.297, sell price:4371.592\n",
      "After selling: coin sold:0.037, transaction fees:0.403, coin now:0.332, cash now:3064.008\n",
      "Action end:  Action.SELL , Total value now: 4514.032.  , Return since entry: -9.719 %\n",
      "\n",
      "Current time: 2017-10-02 00:00:00\n",
      "Action start: Action.BUY , Total value before action: 4525.523121807841\n",
      "Before buying: coin:0.278, cash:3291.466, buy price:4434.000\n",
      "After buying: coin bought:0.074, transaction fees:0.823, coin now:0.353, cash now:2961.497\n",
      "Action end:  Action.BUY , Total value now: 4524.700.  , Return since entry: -9.506 %\n",
      "\n",
      "Current time: 2017-10-03 00:00:00\n",
      "Action start: Action.BUY , Total value before action: 4498.389049653184\n",
      "Before buying: coin:0.304, cash:3166.677, buy price:4382.277\n",
      "After buying: coin bought:0.072, transaction fees:0.792, coin now:0.376, cash now:2849.218\n",
      "Action end:  Action.BUY , Total value now: 4497.597.  , Return since entry: -10.048 %\n",
      "\n",
      "Current time: 2017-10-04 00:00:00\n",
      "Action start: Action.SELL , Total value before action: 4459.913895179201\n",
      "Before selling: coin:0.345, cash:2977.928, sell price:4300.594\n",
      "After selling: coin sold:0.034, transaction fees:0.370, coin now:0.310, cash now:3125.756\n",
      "Action end:  Action.SELL , Total value now: 4459.543.  , Return since entry: -10.809 %\n",
      "\n",
      "Current time: 2017-10-05 00:00:00\n",
      "Action start: Action.SELL , Total value before action: 4414.881069325193\n",
      "Before selling: coin:0.347, cash:2949.938, sell price:4222.410\n",
      "After selling: coin sold:0.035, transaction fees:0.366, coin now:0.312, cash now:3096.066\n",
      "Action end:  Action.SELL , Total value now: 4414.515.  , Return since entry: -11.710 %\n",
      "\n",
      "Current time: 2017-10-06 00:00:00\n",
      "Action start: Action.SELL , Total value before action: 4438.487513407236\n",
      "Before selling: coin:0.370, cash:2837.965, sell price:4322.800\n",
      "After selling: coin sold:0.037, transaction fees:0.400, coin now:0.333, cash now:2997.618\n",
      "Action end:  Action.SELL , Total value now: 4438.087.  , Return since entry: -11.238 %\n",
      "\n",
      "Current time: 2017-10-07 00:00:00\n",
      "Action start: Action.SELL , Total value before action: 4444.598260624232\n",
      "Before selling: coin:0.375, cash:2800.779, sell price:4382.728\n",
      "After selling: coin sold:0.038, transaction fees:0.411, coin now:0.338, cash now:2964.750\n",
      "Action end:  Action.SELL , Total value now: 4444.187.  , Return since entry: -11.116 %\n",
      "\n",
      "Current time: 2017-10-08 00:00:00\n",
      "Action start: Action.BUY , Total value before action: 4455.6617061272555\n",
      "Before buying: coin:0.274, cash:3237.138, buy price:4453.000\n",
      "After buying: coin bought:0.073, transaction fees:0.809, coin now:0.346, cash now:2912.615\n",
      "Action end:  Action.BUY , Total value now: 4454.852.  , Return since entry: -10.903 %\n",
      "\n",
      "Current time: 2017-10-09 00:00:00\n",
      "Action start: Action.BUY , Total value before action: 4491.03974391285\n",
      "Before buying: coin:0.293, cash:3145.788, buy price:4588.000\n",
      "After buying: coin bought:0.069, transaction fees:0.786, coin now:0.362, cash now:2830.423\n",
      "Action end:  Action.BUY , Total value now: 4490.253.  , Return since entry: -10.195 %\n",
      "\n",
      "Current time: 2017-10-10 00:00:00\n",
      "Action start: Action.SELL , Total value before action: 4579.249604827539\n",
      "Before selling: coin:0.317, cash:3034.769, sell price:4875.000\n",
      "After selling: coin sold:0.032, transaction fees:0.386, coin now:0.285, cash now:3188.831\n",
      "Action end:  Action.SELL , Total value now: 4578.863.  , Return since entry: -8.423 %\n",
      "\n",
      "Current time: 2017-10-11 00:00:00\n",
      "Action start: Action.SELL , Total value before action: 4526.2374433358345\n",
      "Before selling: coin:0.346, cash:2884.851, sell price:4747.221\n",
      "After selling: coin sold:0.035, transaction fees:0.410, coin now:0.311, cash now:3048.579\n",
      "Action end:  Action.SELL , Total value now: 4525.827.  , Return since entry: -9.483 %\n",
      "\n",
      "Current time: 2017-10-12 00:00:00\n",
      "Action start: Action.SELL , Total value before action: 4543.251453809677\n",
      "Before selling: coin:0.347, cash:2859.675, sell price:4846.000\n",
      "After selling: coin sold:0.035, transaction fees:0.421, coin now:0.313, cash now:3027.612\n",
      "Action end:  Action.SELL , Total value now: 4542.831.  , Return since entry: -9.143 %\n",
      "\n",
      "Current time: 2017-10-13 00:00:00\n",
      "Action start: Action.SELL , Total value before action: 4689.350876849402\n",
      "Before selling: coin:0.261, cash:3279.478, sell price:5403.690\n",
      "After selling: coin sold:0.026, transaction fees:0.352, coin now:0.235, cash now:3420.113\n",
      "Action end:  Action.SELL , Total value now: 4688.998.  , Return since entry: -6.220 %\n",
      "\n",
      "Current time: 2017-10-14 00:00:00\n",
      "Action start: Action.SELL , Total value before action: 4728.791772043059\n",
      "Before selling: coin:0.278, cash:3174.523, sell price:5591.413\n",
      "After selling: coin sold:0.028, transaction fees:0.389, coin now:0.250, cash now:3329.562\n",
      "Action end:  Action.SELL , Total value now: 4728.403.  , Return since entry: -5.432 %\n",
      "\n",
      "Current time: 2017-10-15 00:00:00\n",
      "Action start: Action.SELL , Total value before action: 4771.891266394079\n",
      "Before selling: coin:0.276, cash:3169.270, sell price:5807.000\n",
      "After selling: coin sold:0.028, transaction fees:0.401, coin now:0.248, cash now:3329.132\n",
      "Action end:  Action.SELL , Total value now: 4771.491.  , Return since entry: -4.570 %\n",
      "\n",
      "Current time: 2017-10-16 00:00:00\n",
      "Action start: Action.SELL , Total value before action: 4717.1198989628265\n",
      "Before selling: coin:0.278, cash:3132.140, sell price:5703.000\n",
      "After selling: coin sold:0.028, transaction fees:0.396, coin now:0.250, cash now:3290.241\n",
      "Action end:  Action.SELL , Total value now: 4716.724.  , Return since entry: -5.666 %\n",
      "\n",
      "Current time: 2017-10-17 00:00:00\n",
      "Action start: Action.SELL , Total value before action: 4712.026235280509\n",
      "Before selling: coin:0.302, cash:2976.225, sell price:5752.001\n",
      "After selling: coin sold:0.030, transaction fees:0.434, coin now:0.272, cash now:3149.371\n",
      "Action end:  Action.SELL , Total value now: 4711.592.  , Return since entry: -5.768 %\n",
      "\n",
      "Current time: 2017-10-18 00:00:00\n",
      "Action start: Action.SELL , Total value before action: 4636.305813850318\n",
      "Before selling: coin:0.307, cash:2941.633, sell price:5520.097\n",
      "After selling: coin sold:0.031, transaction fees:0.424, coin now:0.276, cash now:3110.677\n",
      "Action end:  Action.SELL , Total value now: 4635.882.  , Return since entry: -7.282 %\n",
      "\n",
      "Current time: 2017-10-19 00:00:00\n",
      "Action start: Action.BUY , Total value before action: 4626.282732333173\n",
      "Before buying: coin:0.229, cash:3360.141, buy price:5539.154\n",
      "After buying: coin bought:0.061, transaction fees:0.840, coin now:0.289, cash now:3023.287\n",
      "Action end:  Action.BUY , Total value now: 4625.443.  , Return since entry: -7.491 %\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current time: 2017-10-20 00:00:00\n",
      "Action start: Action.BUY , Total value before action: 4665.211115667727\n",
      "Before buying: coin:0.244, cash:3269.480, buy price:5711.447\n",
      "After buying: coin bought:0.057, transaction fees:0.817, coin now:0.302, cash now:2941.714\n",
      "Action end:  Action.BUY , Total value now: 4664.394.  , Return since entry: -6.712 %\n",
      "\n",
      "Current time: 2017-10-21 00:00:00\n",
      "Action start: Action.SELL , Total value before action: 4711.528024592947\n",
      "Before selling: coin:0.255, cash:3192.508, sell price:5955.000\n",
      "After selling: coin sold:0.026, transaction fees:0.380, coin now:0.230, cash now:3344.030\n",
      "Action end:  Action.SELL , Total value now: 4711.148.  , Return since entry: -5.777 %\n",
      "\n",
      "Current time: 2017-10-22 00:00:00\n",
      "Action start: Action.SELL , Total value before action: 4719.457995252014\n",
      "Before selling: coin:0.258, cash:3156.448, sell price:6047.379\n",
      "After selling: coin sold:0.026, transaction fees:0.391, coin now:0.233, cash now:3312.358\n",
      "Action end:  Action.SELL , Total value now: 4719.067.  , Return since entry: -5.619 %\n",
      "\n",
      "Current time: 2017-10-23 00:00:00\n",
      "Action start: Action.SELL , Total value before action: 4693.195964792222\n",
      "Before selling: coin:0.281, cash:3018.079, sell price:5963.569\n",
      "After selling: coin sold:0.028, transaction fees:0.419, coin now:0.253, cash now:3185.172\n",
      "Action end:  Action.SELL , Total value now: 4692.777.  , Return since entry: -6.144 %\n",
      "\n",
      "Current time: 2017-10-24 00:00:00\n",
      "Action start: Action.SELL , Total value before action: 4632.221102525141\n",
      "Before selling: coin:0.292, cash:2940.151, sell price:5790.916\n",
      "After selling: coin sold:0.029, transaction fees:0.423, coin now:0.263, cash now:3108.935\n",
      "Action end:  Action.SELL , Total value now: 4631.798.  , Return since entry: -7.364 %\n",
      "\n",
      "Current time: 2017-10-25 00:00:00\n",
      "Action start: Action.SELL , Total value before action: 4531.718557413957\n",
      "Before selling: coin:0.300, cash:2884.548, sell price:5487.000\n",
      "After selling: coin sold:0.030, transaction fees:0.412, coin now:0.270, cash now:3048.854\n",
      "Action end:  Action.SELL , Total value now: 4531.307.  , Return since entry: -9.374 %\n",
      "\n",
      "Current time: 2017-10-26 00:00:00\n",
      "Action start: Action.BUY , Total value before action: 4587.551831444915\n",
      "Before buying: coin:0.227, cash:3291.200, buy price:5715.200\n",
      "After buying: coin bought:0.058, transaction fees:0.823, coin now:0.284, cash now:2961.257\n",
      "Action end:  Action.BUY , Total value now: 4586.729.  , Return since entry: -8.265 %\n",
      "\n",
      "Current time: 2017-10-27 00:00:00\n",
      "Action start: Action.BUY , Total value before action: 4643.133429852793\n",
      "Before buying: coin:0.235, cash:3232.878, buy price:5998.000\n",
      "After buying: coin bought:0.054, transaction fees:0.808, coin now:0.289, cash now:2908.782\n",
      "Action end:  Action.BUY , Total value now: 4642.325.  , Return since entry: -7.153 %\n",
      "\n",
      "Current time: 2017-10-28 00:00:00\n",
      "Action start: Action.BUY , Total value before action: 4583.317928267991\n",
      "Before buying: coin:0.237, cash:3204.279, buy price:5830.000\n",
      "After buying: coin bought:0.055, transaction fees:0.801, coin now:0.292, cash now:2883.050\n",
      "Action end:  Action.BUY , Total value now: 4582.517.  , Return since entry: -8.350 %\n",
      "\n",
      "Current time: 2017-10-29 00:00:00\n",
      "Action start: Action.BUY , Total value before action: 4538.445746485526\n",
      "Before buying: coin:0.237, cash:3185.789, buy price:5697.191\n",
      "After buying: coin bought:0.056, transaction fees:0.796, coin now:0.293, cash now:2866.414\n",
      "Action end:  Action.BUY , Total value now: 4537.649.  , Return since entry: -9.247 %\n",
      "\n",
      "Current time: 2017-10-30 00:00:00\n",
      "Action start: Action.SELL , Total value before action: 4624.519764896499\n",
      "Before selling: coin:0.242, cash:3143.321, sell price:6125.051\n",
      "After selling: coin sold:0.024, transaction fees:0.370, coin now:0.218, cash now:3291.071\n",
      "Action end:  Action.SELL , Total value now: 4624.149.  , Return since entry: -7.517 %\n",
      "\n",
      "Current time: 2017-10-31 00:00:00\n",
      "Action start: Action.SELL , Total value before action: 4609.917456742354\n",
      "Before selling: coin:0.250, cash:3078.209, sell price:6138.031\n",
      "After selling: coin sold:0.025, transaction fees:0.383, coin now:0.225, cash now:3230.997\n",
      "Action end:  Action.SELL , Total value now: 4609.535.  , Return since entry: -7.809 %\n",
      "\n",
      "Current time: 2017-11-01 00:00:00\n",
      "Action start: Action.SELL , Total value before action: 4654.099873942122\n",
      "Before selling: coin:0.261, cash:2989.550, sell price:6389.000\n",
      "After selling: coin sold:0.026, transaction fees:0.416, coin now:0.234, cash now:3155.589\n",
      "Action end:  Action.SELL , Total value now: 4653.684.  , Return since entry: -6.926 %\n",
      "\n",
      "Current time: 2017-11-02 00:00:00\n",
      "Action start: Action.SELL , Total value before action: 4734.437958680522\n",
      "Before selling: coin:0.262, cash:2952.385, sell price:6792.595\n",
      "After selling: coin sold:0.026, transaction fees:0.446, coin now:0.236, cash now:3130.145\n",
      "Action end:  Action.SELL , Total value now: 4733.992.  , Return since entry: -5.320 %\n",
      "\n",
      "Current time: 2017-11-03 00:00:00\n",
      "Action start: Action.BUY , Total value before action: 4762.309208022334\n",
      "Before buying: coin:0.203, cash:3346.577, buy price:6979.059\n",
      "After buying: coin bought:0.048, transaction fees:0.837, coin now:0.251, cash now:3011.083\n",
      "Action end:  Action.BUY , Total value now: 4761.473.  , Return since entry: -4.771 %\n",
      "\n",
      "Current time: 2017-11-04 00:00:00\n",
      "Action start: Action.SELL , Total value before action: 4762.223608378648\n",
      "Before selling: coin:0.217, cash:3236.579, sell price:7020.500\n",
      "After selling: coin sold:0.022, transaction fees:0.381, coin now:0.196, cash now:3388.762\n",
      "Action end:  Action.SELL , Total value now: 4761.842.  , Return since entry: -4.763 %\n",
      "\n",
      "Current time: 2017-11-05 00:00:00\n",
      "Action start: Action.SELL , Total value before action: 4811.38014569794\n",
      "Before selling: coin:0.219, cash:3206.866, sell price:7334.290\n",
      "After selling: coin sold:0.022, transaction fees:0.401, coin now:0.197, cash now:3366.916\n",
      "Action end:  Action.SELL , Total value now: 4810.979.  , Return since entry: -3.780 %\n",
      "\n",
      "Current time: 2017-11-06 00:00:00\n",
      "Action start: Action.SELL , Total value before action: 4796.299189572321\n",
      "Before selling: coin:0.215, cash:3219.610, sell price:7325.112\n",
      "After selling: coin sold:0.022, transaction fees:0.394, coin now:0.194, cash now:3376.885\n",
      "Action end:  Action.SELL , Total value now: 4795.905.  , Return since entry: -4.082 %\n",
      "\n",
      "Current time: 2017-11-07 00:00:00\n",
      "Action start: Action.SELL , Total value before action: 4735.399928494246\n",
      "Before selling: coin:0.221, cash:3175.700, sell price:7051.662\n",
      "After selling: coin sold:0.022, transaction fees:0.390, coin now:0.199, cash now:3331.280\n",
      "Action end:  Action.SELL , Total value now: 4735.010.  , Return since entry: -5.300 %\n",
      "\n",
      "Current time: 2017-11-08 00:00:00\n",
      "Action start: Action.SELL , Total value before action: 4730.795255392051\n",
      "Before selling: coin:0.222, cash:3158.906, sell price:7070.900\n",
      "After selling: coin sold:0.022, transaction fees:0.393, coin now:0.200, cash now:3315.702\n",
      "Action end:  Action.SELL , Total value now: 4730.402.  , Return since entry: -5.392 %\n",
      "\n",
      "Current time: 2017-11-09 00:00:00\n",
      "Action start: Action.SELL , Total value before action: 4755.213110677849\n",
      "Before selling: coin:0.236, cash:3018.131, sell price:7346.366\n",
      "After selling: coin sold:0.024, transaction fees:0.434, coin now:0.213, cash now:3191.405\n",
      "Action end:  Action.SELL , Total value now: 4754.779.  , Return since entry: -4.904 %\n",
      "\n",
      "Current time: 2017-11-10 00:00:00\n",
      "Action start: Action.SELL , Total value before action: 4714.840598637648\n",
      "Before selling: coin:0.243, cash:2967.646, sell price:7198.000\n",
      "After selling: coin sold:0.024, transaction fees:0.437, coin now:0.218, cash now:3141.928\n",
      "Action end:  Action.SELL , Total value now: 4714.404.  , Return since entry: -5.712 %\n",
      "\n",
      "Current time: 2017-11-11 00:00:00\n",
      "Action start: Action.SELL , Total value before action: 4620.851886981098\n",
      "Before selling: coin:0.254, cash:2911.417, sell price:6737.000\n",
      "After selling: coin sold:0.025, transaction fees:0.427, coin now:0.228, cash now:3081.933\n",
      "Action end:  Action.SELL , Total value now: 4620.425.  , Return since entry: -7.592 %\n",
      "\n",
      "Current time: 2017-11-12 00:00:00\n",
      "Action start: Action.SELL , Total value before action: 4476.477982228034\n",
      "Before selling: coin:0.261, cash:2859.649, sell price:6206.196\n",
      "After selling: coin sold:0.026, transaction fees:0.404, coin now:0.234, cash now:3020.928\n",
      "Action end:  Action.SELL , Total value now: 4476.074.  , Return since entry: -10.479 %\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current time: 2017-11-13 00:00:00\n",
      "Action start: Action.SELL , Total value before action: 4400.384940863468\n",
      "Before selling: coin:0.235, cash:2993.000, sell price:5986.736\n",
      "After selling: coin sold:0.024, transaction fees:0.352, coin now:0.212, cash now:3133.387\n",
      "Action end:  Action.SELL , Total value now: 4400.033.  , Return since entry: -11.999 %\n",
      "\n",
      "Current time: 2017-11-14 00:00:00\n",
      "Action start: Action.SELL , Total value before action: 4560.390019825452\n",
      "Before selling: coin:0.257, cash:2858.661, sell price:6616.350\n",
      "After selling: coin sold:0.026, transaction fees:0.425, coin now:0.231, cash now:3028.408\n",
      "Action end:  Action.SELL , Total value now: 4559.965.  , Return since entry: -8.801 %\n",
      "\n",
      "Current time: 2017-11-15 00:00:00\n",
      "Action start: Action.SELL , Total value before action: 4580.704825878818\n",
      "Before selling: coin:0.256, cash:2857.102, sell price:6720.466\n",
      "After selling: coin sold:0.026, transaction fees:0.431, coin now:0.231, cash now:3029.031\n",
      "Action end:  Action.SELL , Total value now: 4580.274.  , Return since entry: -8.395 %\n",
      "\n",
      "Current time: 2017-11-16 00:00:00\n",
      "Action start: Action.BUY , Total value before action: 4665.5021305253385\n",
      "Before buying: coin:0.194, cash:3265.734, buy price:7205.000\n",
      "After buying: coin bought:0.045, transaction fees:0.816, coin now:0.240, cash now:2938.345\n",
      "Action end:  Action.BUY , Total value now: 4664.686.  , Return since entry: -6.706 %\n",
      "\n",
      "Current time: 2017-11-17 00:00:00\n",
      "Action start: Action.SELL , Total value before action: 4810.7451938986615\n",
      "Before selling: coin:0.204, cash:3195.250, sell price:7926.000\n",
      "After selling: coin sold:0.020, transaction fees:0.404, coin now:0.183, cash now:3356.396\n",
      "Action end:  Action.SELL , Total value now: 4810.341.  , Return since entry: -3.793 %\n",
      "\n",
      "Current time: 2017-11-18 00:00:00\n",
      "Action start: Action.SELL , Total value before action: 4750.849154115354\n",
      "Before selling: coin:0.219, cash:3088.230, sell price:7596.496\n",
      "After selling: coin sold:0.022, transaction fees:0.416, coin now:0.197, cash now:3254.076\n",
      "Action end:  Action.SELL , Total value now: 4750.433.  , Return since entry: -4.991 %\n",
      "\n",
      "Current time: 2017-11-19 00:00:00\n",
      "Action start: Action.SELL , Total value before action: 4767.620061669711\n",
      "Before selling: coin:0.227, cash:3009.551, sell price:7749.338\n",
      "After selling: coin sold:0.023, transaction fees:0.440, coin now:0.204, cash now:3184.918\n",
      "Action end:  Action.SELL , Total value now: 4767.181.  , Return since entry: -4.656 %\n",
      "\n",
      "Current time: 2017-11-20 00:00:00\n",
      "Action start: Action.BUY , Total value before action: 4815.827814374424\n",
      "Before buying: coin:0.168, cash:3460.440, buy price:8051.000\n",
      "After buying: coin bought:0.043, transaction fees:0.865, coin now:0.211, cash now:3113.531\n",
      "Action end:  Action.BUY , Total value now: 4814.963.  , Return since entry: -3.701 %\n",
      "\n",
      "Current time: 2017-11-21 00:00:00\n",
      "Action start: Action.BUY , Total value before action: 4846.459494386472\n",
      "Before buying: coin:0.176, cash:3392.523, buy price:8261.079\n",
      "After buying: coin bought:0.041, transaction fees:0.848, coin now:0.217, cash now:3052.422\n",
      "Action end:  Action.BUY , Total value now: 4845.611.  , Return since entry: -3.088 %\n",
      "\n",
      "Current time: 2017-11-22 00:00:00\n",
      "Action start: Action.SELL , Total value before action: 4829.098780894128\n",
      "Before selling: coin:0.189, cash:3287.802, sell price:8134.228\n",
      "After selling: coin sold:0.019, transaction fees:0.385, coin now:0.171, cash now:3441.546\n",
      "Action end:  Action.SELL , Total value now: 4828.713.  , Return since entry: -3.426 %\n",
      "\n",
      "Current time: 2017-11-23 00:00:00\n",
      "Action start: Action.SELL , Total value before action: 4830.968155786686\n",
      "Before selling: coin:0.217, cash:3050.446, sell price:8210.806\n",
      "After selling: coin sold:0.022, transaction fees:0.445, coin now:0.195, cash now:3228.053\n",
      "Action end:  Action.SELL , Total value now: 4830.523.  , Return since entry: -3.390 %\n",
      "\n",
      "Current time: 2017-11-24 00:00:00\n",
      "Action start: Action.SELL , Total value before action: 4770.647521974863\n",
      "Before selling: coin:0.218, cash:3031.002, sell price:7974.767\n",
      "After selling: coin sold:0.022, transaction fees:0.435, coin now:0.196, cash now:3204.532\n",
      "Action end:  Action.SELL , Total value now: 4770.213.  , Return since entry: -4.596 %\n",
      "\n",
      "Current time: 2017-11-25 00:00:00\n",
      "Action start: Action.SELL , Total value before action: 4794.841497599431\n",
      "Before selling: coin:0.216, cash:3024.446, sell price:8183.988\n",
      "After selling: coin sold:0.022, transaction fees:0.443, coin now:0.195, cash now:3201.043\n",
      "Action end:  Action.SELL , Total value now: 4794.399.  , Return since entry: -4.112 %\n",
      "\n",
      "Current time: 2017-11-26 00:00:00\n",
      "Action start: Action.BUY , Total value before action: 4889.3654304665415\n",
      "Before buying: coin:0.165, cash:3437.852, buy price:8781.000\n",
      "After buying: coin bought:0.039, transaction fees:0.859, coin now:0.204, cash now:3093.207\n",
      "Action end:  Action.BUY , Total value now: 4888.506.  , Return since entry: -2.230 %\n",
      "\n",
      "Current time: 2017-11-27 00:00:00\n",
      "Action start: Action.SELL , Total value before action: 5019.770342542382\n",
      "Before selling: coin:0.172, cash:3381.560, sell price:9544.632\n",
      "After selling: coin sold:0.017, transaction fees:0.410, coin now:0.154, cash now:3544.971\n",
      "Action end:  Action.SELL , Total value now: 5019.361.  , Return since entry: 0.387 %\n",
      "\n",
      "Current time: 2017-11-28 00:00:00\n",
      "Action start: Action.SELL , Total value before action: 5026.979586139394\n",
      "Before selling: coin:0.174, cash:3347.308, sell price:9680.092\n",
      "After selling: coin sold:0.017, transaction fees:0.420, coin now:0.156, cash now:3514.855\n",
      "Action end:  Action.SELL , Total value now: 5026.560.  , Return since entry: 0.531 %\n",
      "\n",
      "Current time: 2017-11-29 00:00:00\n",
      "Action start: Action.SELL , Total value before action: 5055.350055501877\n",
      "Before selling: coin:0.170, cash:3366.794, sell price:9918.000\n",
      "After selling: coin sold:0.017, transaction fees:0.422, coin now:0.153, cash now:3535.227\n",
      "Action end:  Action.SELL , Total value now: 5054.928.  , Return since entry: 1.099 %\n",
      "\n",
      "Current time: 2017-11-30 00:00:00\n",
      "Action start: Action.SELL , Total value before action: 5085.61793674846\n",
      "Before selling: coin:0.185, cash:3246.736, sell price:9929.772\n",
      "After selling: coin sold:0.019, transaction fees:0.460, coin now:0.167, cash now:3430.165\n",
      "Action end:  Action.SELL , Total value now: 5085.158.  , Return since entry: 1.703 %\n",
      "\n",
      "Current time: 2017-12-01 00:00:00\n",
      "Action start: Action.SELL , Total value before action: 5047.258576104108\n",
      "Before selling: coin:0.191, cash:3169.240, sell price:9830.000\n",
      "After selling: coin sold:0.019, transaction fees:0.470, coin now:0.172, cash now:3356.572\n",
      "Action end:  Action.SELL , Total value now: 5046.789.  , Return since entry: 0.936 %\n",
      "\n",
      "Current time: 2017-12-02 00:00:00\n",
      "Action start: Action.BUY , Total value before action: 5203.538510393517\n",
      "Before buying: coin:0.143, cash:3649.868, buy price:10879.217\n",
      "After buying: coin bought:0.034, transaction fees:0.912, coin now:0.176, cash now:3283.969\n",
      "Action end:  Action.BUY , Total value now: 5202.626.  , Return since entry: 4.053 %\n",
      "\n",
      "Current time: 2017-12-03 00:00:00\n",
      "Action start: Action.BUY , Total value before action: 5183.994807956386\n",
      "Before buying: coin:0.142, cash:3640.880, buy price:10850.906\n",
      "After buying: coin bought:0.034, transaction fees:0.910, coin now:0.176, cash now:3275.882\n",
      "Action end:  Action.BUY , Total value now: 5183.085.  , Return since entry: 3.662 %\n",
      "\n",
      "Current time: 2017-12-04 00:00:00\n",
      "Action start: Action.SELL , Total value before action: 5225.948493849105\n",
      "Before selling: coin:0.150, cash:3548.578, sell price:11205.000\n",
      "After selling: coin sold:0.015, transaction fees:0.419, coin now:0.135, cash now:3715.896\n",
      "Action end:  Action.SELL , Total value now: 5225.529.  , Return since entry: 4.511 %\n",
      "\n",
      "Current time: 2017-12-05 00:00:00\n",
      "Action start: Action.SELL , Total value before action: 5262.025739681054\n",
      "Before selling: coin:0.153, cash:3492.661, sell price:11560.000\n",
      "After selling: coin sold:0.015, transaction fees:0.442, coin now:0.138, cash now:3669.155\n",
      "Action end:  Action.SELL , Total value now: 5261.583.  , Return since entry: 5.232 %\n",
      "\n",
      "Current time: 2017-12-06 00:00:00\n",
      "Action start: Action.SELL , Total value before action: 5289.410559988783\n",
      "Before selling: coin:0.150, cash:3514.865, sell price:11810.000\n",
      "After selling: coin sold:0.015, transaction fees:0.444, coin now:0.135, cash now:3691.876\n",
      "Action end:  Action.SELL , Total value now: 5288.967.  , Return since entry: 5.779 %\n",
      "\n",
      "Current time: 2017-12-07 00:00:00\n",
      "Action start: Action.BUY , Total value before action: 5573.055813793224\n",
      "Before buying: coin:0.114, cash:3992.946, buy price:13875.904\n",
      "After buying: coin bought:0.029, transaction fees:0.998, coin now:0.143, cash now:3592.653\n",
      "Action end:  Action.BUY , Total value now: 5572.058.  , Return since entry: 11.441 %\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current time: 2017-12-08 00:00:00\n",
      "Action start: Action.SELL , Total value before action: 5952.357743186307\n",
      "Before selling: coin:0.121, cash:3912.351, sell price:16849.930\n",
      "After selling: coin sold:0.012, transaction fees:0.510, coin now:0.109, cash now:4115.842\n",
      "Action end:  Action.SELL , Total value now: 5951.848.  , Return since entry: 19.037 %\n",
      "\n",
      "Current time: 2017-12-09 00:00:00\n",
      "Action start: Action.SELL , Total value before action: 5832.615500080685\n",
      "Before selling: coin:0.138, cash:3664.965, sell price:15700.000\n",
      "After selling: coin sold:0.014, transaction fees:0.542, coin now:0.124, cash now:3881.188\n",
      "Action end:  Action.SELL , Total value now: 5832.074.  , Return since entry: 16.641 %\n",
      "\n",
      "Current time: 2017-12-10 00:00:00\n",
      "Action start: Action.SELL , Total value before action: 5621.773941895847\n",
      "Before selling: coin:0.144, cash:3584.109, sell price:14148.454\n",
      "After selling: coin sold:0.014, transaction fees:0.509, coin now:0.130, cash now:3787.367\n",
      "Action end:  Action.SELL , Total value now: 5621.265.  , Return since entry: 12.425 %\n",
      "\n",
      "Current time: 2017-12-11 00:00:00\n",
      "Action start: Action.BUY , Total value before action: 5772.987130145239\n",
      "Before buying: coin:0.113, cash:4051.764, buy price:15259.420\n",
      "After buying: coin bought:0.027, transaction fees:1.013, coin now:0.139, cash now:3645.575\n",
      "Action end:  Action.BUY , Total value now: 5771.974.  , Return since entry: 15.439 %\n",
      "\n",
      "Current time: 2017-12-12 00:00:00\n",
      "Action start: Action.SELL , Total value before action: 5901.621205204913\n",
      "Before selling: coin:0.118, cash:3965.515, sell price:16388.236\n",
      "After selling: coin sold:0.012, transaction fees:0.484, coin now:0.106, cash now:4158.642\n",
      "Action end:  Action.SELL , Total value now: 5901.137.  , Return since entry: 18.023 %\n",
      "\n",
      "Current time: 2017-12-13 00:00:00\n",
      "Action start: Action.SELL , Total value before action: 5888.071154268844\n",
      "Before selling: coin:0.119, cash:3927.273, sell price:16500.000\n",
      "After selling: coin sold:0.012, transaction fees:0.490, coin now:0.107, cash now:4122.863\n",
      "Action end:  Action.SELL , Total value now: 5887.581.  , Return since entry: 17.752 %\n",
      "\n",
      "Current time: 2017-12-14 00:00:00\n",
      "Action start: Action.SELL , Total value before action: 5830.08120102208\n",
      "Before selling: coin:0.121, cash:3876.791, sell price:16150.000\n",
      "After selling: coin sold:0.012, transaction fees:0.488, coin now:0.109, cash now:4071.632\n",
      "Action end:  Action.SELL , Total value now: 5829.593.  , Return since entry: 16.592 %\n",
      "\n",
      "Current time: 2017-12-15 00:00:00\n",
      "Action start: Action.SELL , Total value before action: 5904.487494305515\n",
      "Before selling: coin:0.119, cash:3884.310, sell price:16950.000\n",
      "After selling: coin sold:0.012, transaction fees:0.505, coin now:0.107, cash now:4085.823\n",
      "Action end:  Action.SELL , Total value now: 5903.982.  , Return since entry: 18.080 %\n",
      "\n",
      "Current time: 2017-12-16 00:00:00\n",
      "Action start: Action.SELL , Total value before action: 5952.790945492148\n",
      "Before selling: coin:0.125, cash:3764.834, sell price:17563.438\n",
      "After selling: coin sold:0.012, transaction fees:0.547, coin now:0.112, cash now:3983.082\n",
      "Action end:  Action.SELL , Total value now: 5952.244.  , Return since entry: 19.045 %\n",
      "\n",
      "Current time: 2017-12-17 00:00:00\n",
      "Action start: Action.BUY , Total value before action: 6120.8866506054455\n",
      "Before buying: coin:0.093, cash:4346.753, buy price:19127.573\n",
      "After buying: coin bought:0.023, transaction fees:1.087, coin now:0.115, cash now:3910.991\n",
      "Action end:  Action.BUY , Total value now: 6119.800.  , Return since entry: 22.396 %\n",
      "\n",
      "Current time: 2017-12-18 00:00:00\n",
      "Action start: Action.BUY , Total value before action: 6000.734990870999\n",
      "Before buying: coin:0.095, cash:4288.193, buy price:17969.910\n",
      "After buying: coin bought:0.024, transaction fees:1.072, coin now:0.119, cash now:3858.301\n",
      "Action end:  Action.BUY , Total value now: 5999.663.  , Return since entry: 19.993 %\n",
      "\n",
      "Current time: 2017-12-19 00:00:00\n",
      "Action start: Action.BUY , Total value before action: 6072.833443115353\n",
      "Before buying: coin:0.097, cash:4251.017, buy price:18749.990\n",
      "After buying: coin bought:0.023, transaction fees:1.063, coin now:0.120, cash now:3824.853\n",
      "Action end:  Action.BUY , Total value now: 6071.771.  , Return since entry: 21.435 %\n",
      "\n",
      "Current time: 2017-12-20 00:00:00\n",
      "Action start: Action.BUY , Total value before action: 5823.677562111018\n",
      "Before buying: coin:0.100, cash:4167.277, buy price:16490.990\n",
      "After buying: coin bought:0.025, transaction fees:1.042, coin now:0.126, cash now:3749.507\n",
      "Action end:  Action.BUY , Total value now: 5822.636.  , Return since entry: 16.453 %\n",
      "\n",
      "Current time: 2017-12-21 00:00:00\n",
      "Action start: Action.BUY , Total value before action: 5829.299778493488\n",
      "Before buying: coin:0.104, cash:4088.472, buy price:16659.885\n",
      "After buying: coin bought:0.025, transaction fees:1.022, coin now:0.129, cash now:3678.602\n",
      "Action end:  Action.BUY , Total value now: 5828.278.  , Return since entry: 16.566 %\n",
      "\n",
      "Current time: 2017-12-22 00:00:00\n",
      "Action start: Action.BUY , Total value before action: 5621.4457080198445\n",
      "Before buying: coin:0.109, cash:3986.372, buy price:14982.748\n",
      "After buying: coin bought:0.027, transaction fees:0.997, coin now:0.136, cash now:3586.738\n",
      "Action end:  Action.BUY , Total value now: 5620.449.  , Return since entry: 12.409 %\n",
      "\n",
      "Current time: 2017-12-23 00:00:00\n",
      "Action start: Action.BUY , Total value before action: 5410.960723986756\n",
      "Before buying: coin:0.118, cash:3823.518, buy price:13407.949\n",
      "After buying: coin bought:0.029, transaction fees:0.956, coin now:0.147, cash now:3440.210\n",
      "Action end:  Action.BUY , Total value now: 5410.005.  , Return since entry: 8.200 %\n",
      "\n",
      "Current time: 2017-12-24 00:00:00\n",
      "Action start: Action.SELL , Total value before action: 5446.903375555961\n",
      "Before selling: coin:0.125, cash:3728.190, sell price:13800.000\n",
      "After selling: coin sold:0.012, transaction fees:0.430, coin now:0.112, cash now:3899.631\n",
      "Action end:  Action.SELL , Total value now: 5446.474.  , Return since entry: 8.929 %\n",
      "\n",
      "Current time: 2017-12-25 00:00:00\n",
      "Action start: Action.SELL , Total value before action: 5393.009502592264\n",
      "Before selling: coin:0.136, cash:3567.971, sell price:13412.000\n",
      "After selling: coin sold:0.014, transaction fees:0.456, coin now:0.122, cash now:3750.019\n",
      "Action end:  Action.SELL , Total value now: 5392.553.  , Return since entry: 7.851 %\n",
      "\n",
      "Current time: 2017-12-26 00:00:00\n",
      "Action start: Action.SELL , Total value before action: 5441.102177535601\n",
      "Before selling: coin:0.144, cash:3466.204, sell price:13740.387\n",
      "After selling: coin sold:0.014, transaction fees:0.494, coin now:0.129, cash now:3663.200\n",
      "Action end:  Action.SELL , Total value now: 5440.608.  , Return since entry: 8.812 %\n",
      "\n",
      "Current time: 2017-12-27 00:00:00\n",
      "Action start: Action.BUY , Total value before action: 5666.398901280902\n",
      "Before buying: coin:0.108, cash:3971.014, buy price:15673.000\n",
      "After buying: coin bought:0.025, transaction fees:0.993, coin now:0.134, cash now:3572.920\n",
      "Action end:  Action.BUY , Total value now: 5665.406.  , Return since entry: 13.308 %\n",
      "\n",
      "Current time: 2017-12-28 00:00:00\n",
      "Action start: Action.SELL , Total value before action: 5619.82803231804\n",
      "Before selling: coin:0.121, cash:3790.284, sell price:15174.023\n",
      "After selling: coin sold:0.012, transaction fees:0.457, coin now:0.109, cash now:3972.781\n",
      "Action end:  Action.SELL , Total value now: 5619.371.  , Return since entry: 12.387 %\n",
      "\n",
      "Current time: 2017-12-29 00:00:00\n",
      "Action start: Action.SELL , Total value before action: 5486.353852913313\n",
      "Before selling: coin:0.129, cash:3647.134, sell price:14236.000\n",
      "After selling: coin sold:0.013, transaction fees:0.460, coin now:0.116, cash now:3830.596\n",
      "Action end:  Action.SELL , Total value now: 5485.894.  , Return since entry: 9.718 %\n",
      "\n",
      "Current time: 2017-12-30 00:00:00\n",
      "Action start: Action.SELL , Total value before action: 5471.415939700606\n",
      "Before selling: coin:0.140, cash:3482.130, sell price:14250.000\n",
      "After selling: coin sold:0.014, transaction fees:0.497, coin now:0.126, cash now:3680.562\n",
      "Action end:  Action.SELL , Total value now: 5470.919.  , Return since entry: 9.418 %\n",
      "\n",
      "Current time: 2017-12-31 00:00:00\n",
      "Action start: Action.SELL , Total value before action: 5283.015534125567\n",
      "Before selling: coin:0.152, cash:3322.189, sell price:12912.000\n",
      "After selling: coin sold:0.015, transaction fees:0.490, coin now:0.137, cash now:3517.781\n",
      "Action end:  Action.SELL , Total value now: 5282.525.  , Return since entry: 5.651 %\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current time: 2018-01-01 00:00:00\n",
      "Action start: Action.BUY , Total value before action: 5389.355607077374\n",
      "Before buying: coin:0.113, cash:3831.408, buy price:13800.000\n",
      "After buying: coin bought:0.028, transaction fees:0.958, coin now:0.141, cash now:3447.309\n",
      "Action end:  Action.BUY , Total value now: 5388.398.  , Return since entry: 7.768 %\n",
      "\n",
      "Percentage return: 7.767955100869913\n"
     ]
    }
   ],
   "source": [
    "test_start = datetime.datetime(2017,9,1,0)\n",
    "test_end = datetime.datetime(2018,1,1,0)\n",
    "agent.test(sess, start_time = test_start, end_time = test_end, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x576 with 4 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_test(test_start, test_end)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test on unseen time period (2018-01-01 ~ 2018-04-01 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing from  2018-01-01 00:00:00  to 2018-04-01 00:00:00 :  ~ 90 days\n",
      "\n",
      "Current time: 2018-01-01 00:00:00\n",
      "Action start: Action.BUY , Total value before action: 5000.0\n",
      "Before buying: coin:0.000, cash:5000.000, buy price:13564.000\n",
      "After buying: coin bought:0.037, transaction fees:1.250, coin now:0.037, cash now:4498.750\n",
      "Action end:  Action.BUY , Total value now: 4998.750.  , Return since entry: -0.025 %\n",
      "\n",
      "Current time: 2018-01-02 00:00:00\n",
      "Action start: Action.BUY , Total value before action: 5019.407747525067\n",
      "Before buying: coin:0.110, cash:3496.175, buy price:13811.000\n",
      "After buying: coin bought:0.025, transaction fees:0.874, coin now:0.136, cash now:3145.684\n",
      "Action end:  Action.BUY , Total value now: 5018.534.  , Return since entry: 0.371 %\n",
      "\n",
      "Current time: 2018-01-03 00:00:00\n",
      "Action start: Action.SELL , Total value before action: 5113.589968431124\n",
      "Before selling: coin:0.110, cash:3489.334, sell price:14764.477\n",
      "After selling: coin sold:0.011, transaction fees:0.406, coin now:0.099, cash now:3651.353\n",
      "Action end:  Action.SELL , Total value now: 5113.184.  , Return since entry: 2.264 %\n",
      "\n",
      "Current time: 2018-01-04 00:00:00\n",
      "Action start: Action.SELL , Total value before action: 5141.245799275695\n",
      "Before selling: coin:0.114, cash:3422.639, sell price:15111.416\n",
      "After selling: coin sold:0.011, transaction fees:0.430, coin now:0.102, cash now:3594.070\n",
      "Action end:  Action.SELL , Total value now: 5140.816.  , Return since entry: 2.816 %\n",
      "\n",
      "Current time: 2018-01-05 00:00:00\n",
      "Action start: Action.SELL , Total value before action: 5110.849131029863\n",
      "Before selling: coin:0.114, cash:3395.908, sell price:15028.397\n",
      "After selling: coin sold:0.011, transaction fees:0.429, coin now:0.103, cash now:3566.973\n",
      "Action end:  Action.SELL , Total value now: 5110.420.  , Return since entry: 2.208 %\n",
      "\n",
      "Current time: 2018-01-06 00:00:00\n",
      "Action start: Action.SELL , Total value before action: 5306.682243938263\n",
      "Before selling: coin:0.117, cash:3331.686, sell price:16925.245\n",
      "After selling: coin sold:0.012, transaction fees:0.494, coin now:0.105, cash now:3528.691\n",
      "Action end:  Action.SELL , Total value now: 5306.188.  , Return since entry: 6.124 %\n",
      "\n",
      "Current time: 2018-01-07 00:00:00\n",
      "Action start: Action.SELL , Total value before action: 5295.955707488711\n",
      "Before selling: coin:0.116, cash:3325.167, sell price:17003.374\n",
      "After selling: coin sold:0.012, transaction fees:0.493, coin now:0.104, cash now:3521.753\n",
      "Action end:  Action.SELL , Total value now: 5295.463.  , Return since entry: 5.909 %\n",
      "\n",
      "Current time: 2018-01-08 00:00:00\n",
      "Action start: Action.SELL , Total value before action: 5174.299851935153\n",
      "Before selling: coin:0.118, cash:3288.691, sell price:16000.000\n",
      "After selling: coin sold:0.012, transaction fees:0.471, coin now:0.106, cash now:3476.781\n",
      "Action end:  Action.SELL , Total value now: 5173.828.  , Return since entry: 3.477 %\n",
      "\n",
      "Current time: 2018-01-09 00:00:00\n",
      "Action start: Action.SELL , Total value before action: 5072.132069843234\n",
      "Before selling: coin:0.124, cash:3192.137, sell price:15174.751\n",
      "After selling: coin sold:0.012, transaction fees:0.470, coin now:0.112, cash now:3379.666\n",
      "Action end:  Action.SELL , Total value now: 5071.662.  , Return since entry: 1.433 %\n",
      "\n",
      "Current time: 2018-01-10 00:00:00\n",
      "Action start: Action.SELL , Total value before action: 4920.8634652378805\n",
      "Before selling: coin:0.126, cash:3154.265, sell price:14074.358\n",
      "After selling: coin sold:0.013, transaction fees:0.442, coin now:0.113, cash now:3330.483\n",
      "Action end:  Action.SELL , Total value now: 4920.422.  , Return since entry: -1.592 %\n",
      "\n",
      "Current time: 2018-01-11 00:00:00\n",
      "Action start: Action.BUY , Total value before action: 4994.715467635846\n",
      "Before buying: coin:0.098, cash:3554.006, buy price:14642.000\n",
      "After buying: coin bought:0.024, transaction fees:0.889, coin now:0.123, cash now:3197.717\n",
      "Action end:  Action.BUY , Total value now: 4993.827.  , Return since entry: -0.123 %\n",
      "\n",
      "Current time: 2018-01-12 00:00:00\n",
      "Action start: Action.BUY , Total value before action: 4795.830410798893\n",
      "Before buying: coin:0.106, cash:3411.090, buy price:13039.149\n",
      "After buying: coin bought:0.026, transaction fees:0.853, coin now:0.132, cash now:3069.128\n",
      "Action end:  Action.BUY , Total value now: 4794.978.  , Return since entry: -4.100 %\n",
      "\n",
      "Current time: 2018-01-13 00:00:00\n",
      "Action start: Action.SELL , Total value before action: 4924.848590274456\n",
      "Before selling: coin:0.115, cash:3299.191, sell price:14108.000\n",
      "After selling: coin sold:0.012, transaction fees:0.406, coin now:0.104, cash now:3461.351\n",
      "Action end:  Action.SELL , Total value now: 4924.442.  , Return since entry: -1.511 %\n",
      "\n",
      "Current time: 2018-01-14 00:00:00\n",
      "Action start: Action.SELL , Total value before action: 4919.793656626398\n",
      "Before selling: coin:0.115, cash:3285.604, sell price:14235.000\n",
      "After selling: coin sold:0.011, transaction fees:0.409, coin now:0.103, cash now:3448.614\n",
      "Action end:  Action.SELL , Total value now: 4919.385.  , Return since entry: -1.612 %\n",
      "\n",
      "Current time: 2018-01-15 00:00:00\n",
      "Action start: Action.SELL , Total value before action: 4844.042685865942\n",
      "Before selling: coin:0.119, cash:3218.606, sell price:13712.000\n",
      "After selling: coin sold:0.012, transaction fees:0.406, coin now:0.107, cash now:3380.743\n",
      "Action end:  Action.SELL , Total value now: 4843.636.  , Return since entry: -3.127 %\n",
      "\n",
      "Current time: 2018-01-16 00:00:00\n",
      "Action start: Action.SELL , Total value before action: 4778.580437991006\n",
      "Before selling: coin:0.127, cash:3108.002, sell price:13200.000\n",
      "After selling: coin sold:0.013, transaction fees:0.418, coin now:0.114, cash now:3274.642\n",
      "Action end:  Action.SELL , Total value now: 4778.163.  , Return since entry: -4.437 %\n",
      "\n",
      "Current time: 2018-01-17 00:00:00\n",
      "Action start: Action.SELL , Total value before action: 4497.462343415714\n",
      "Before selling: coin:0.146, cash:2916.484, sell price:10850.000\n",
      "After selling: coin sold:0.015, transaction fees:0.395, coin now:0.131, cash now:3074.186\n",
      "Action end:  Action.SELL , Total value now: 4497.067.  , Return since entry: -10.059 %\n",
      "\n",
      "Current time: 2018-01-18 00:00:00\n",
      "Action start: Action.BUY , Total value before action: 4525.060330009206\n",
      "Before buying: coin:0.116, cash:3219.832, buy price:11228.448\n",
      "After buying: coin bought:0.029, transaction fees:0.805, coin now:0.145, cash now:2897.044\n",
      "Action end:  Action.BUY , Total value now: 4524.255.  , Return since entry: -9.515 %\n",
      "\n",
      "Current time: 2018-01-19 00:00:00\n",
      "Action start: Action.SELL , Total value before action: 4521.457884645488\n",
      "Before selling: coin:0.125, cash:3111.067, sell price:11294.751\n",
      "After selling: coin sold:0.012, transaction fees:0.353, coin now:0.112, cash now:3251.754\n",
      "Action end:  Action.SELL , Total value now: 4521.105.  , Return since entry: -9.578 %\n",
      "\n",
      "Current time: 2018-01-20 00:00:00\n",
      "Action start: Action.SELL , Total value before action: 4533.180712206747\n",
      "Before selling: coin:0.132, cash:3013.573, sell price:11500.000\n",
      "After selling: coin sold:0.013, transaction fees:0.380, coin now:0.119, cash now:3165.154\n",
      "Action end:  Action.SELL , Total value now: 4532.801.  , Return since entry: -9.344 %\n",
      "\n",
      "Current time: 2018-01-21 00:00:00\n",
      "Action start: Action.SELL , Total value before action: 4622.008504972534\n",
      "Before selling: coin:0.135, cash:2960.221, sell price:12316.000\n",
      "After selling: coin sold:0.013, transaction fees:0.415, coin now:0.121, cash now:3125.984\n",
      "Action end:  Action.SELL , Total value now: 4621.593.  , Return since entry: -7.568 %\n",
      "\n",
      "Current time: 2018-01-22 00:00:00\n",
      "Action start: Action.SELL , Total value before action: 4529.9296253026605\n",
      "Before selling: coin:0.144, cash:2851.942, sell price:11690.000\n",
      "After selling: coin sold:0.014, transaction fees:0.419, coin now:0.129, cash now:3019.322\n",
      "Action end:  Action.SELL , Total value now: 4529.510.  , Return since entry: -9.410 %\n",
      "\n",
      "Current time: 2018-01-23 00:00:00\n",
      "Action start: Action.SELL , Total value before action: 4400.5307318024425\n",
      "Before selling: coin:0.150, cash:2780.824, sell price:10779.951\n",
      "After selling: coin sold:0.015, transaction fees:0.405, coin now:0.135, cash now:2942.390\n",
      "Action end:  Action.SELL , Total value now: 4400.126.  , Return since entry: -11.997 %\n",
      "\n",
      "Current time: 2018-01-24 00:00:00\n",
      "Action start: Action.BUY , Total value before action: 4370.509506324402\n",
      "Before buying: coin:0.115, cash:3149.447, buy price:10593.924\n",
      "After buying: coin bought:0.030, transaction fees:0.787, coin now:0.145, cash now:2833.715\n",
      "Action end:  Action.BUY , Total value now: 4369.722.  , Return since entry: -12.606 %\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current time: 2018-01-25 00:00:00\n",
      "Action start: Action.SELL , Total value before action: 4443.1093596059445\n",
      "Before selling: coin:0.127, cash:3009.988, sell price:11297.645\n",
      "After selling: coin sold:0.013, transaction fees:0.358, coin now:0.114, cash now:3152.942\n",
      "Action end:  Action.SELL , Total value now: 4442.751.  , Return since entry: -11.145 %\n",
      "\n",
      "Current time: 2018-01-26 00:00:00\n",
      "Action start: Action.SELL , Total value before action: 4447.673202496388\n",
      "Before selling: coin:0.131, cash:2955.455, sell price:11399.100\n",
      "After selling: coin sold:0.013, transaction fees:0.373, coin now:0.118, cash now:3104.304\n",
      "Action end:  Action.SELL , Total value now: 4447.300.  , Return since entry: -11.054 %\n",
      "\n",
      "Current time: 2018-01-27 00:00:00\n",
      "Action start: Action.SELL , Total value before action: 4402.30632191488\n",
      "Before selling: coin:0.145, cash:2795.964, sell price:11092.090\n",
      "After selling: coin sold:0.014, transaction fees:0.402, coin now:0.130, cash now:2956.197\n",
      "Action end:  Action.SELL , Total value now: 4401.905.  , Return since entry: -11.962 %\n",
      "\n",
      "Current time: 2018-01-28 00:00:00\n",
      "Action start: Action.BUY , Total value before action: 4456.675322628686\n",
      "Before buying: coin:0.111, cash:3177.528, buy price:11550.000\n",
      "After buying: coin bought:0.028, transaction fees:0.794, coin now:0.138, cash now:2858.981\n",
      "Action end:  Action.BUY , Total value now: 4455.881.  , Return since entry: -10.882 %\n",
      "\n",
      "Current time: 2018-01-29 00:00:00\n",
      "Action start: Action.BUY , Total value before action: 4487.685735606163\n",
      "Before buying: coin:0.113, cash:3143.024, buy price:11895.000\n",
      "After buying: coin bought:0.026, transaction fees:0.786, coin now:0.139, cash now:2827.936\n",
      "Action end:  Action.BUY , Total value now: 4486.900.  , Return since entry: -10.262 %\n",
      "\n",
      "Current time: 2018-01-30 00:00:00\n",
      "Action start: Action.BUY , Total value before action: 4382.80061570241\n",
      "Before buying: coin:0.116, cash:3084.935, buy price:11211.740\n",
      "After buying: coin bought:0.028, transaction fees:0.771, coin now:0.143, cash now:2775.671\n",
      "Action end:  Action.BUY , Total value now: 4382.029.  , Return since entry: -12.359 %\n",
      "\n",
      "Current time: 2018-01-31 00:00:00\n",
      "Action start: Action.BUY , Total value before action: 4203.64658308281\n",
      "Before buying: coin:0.122, cash:3002.538, buy price:9861.000\n",
      "After buying: coin bought:0.030, transaction fees:0.751, coin now:0.152, cash now:2701.534\n",
      "Action end:  Action.BUY , Total value now: 4202.896.  , Return since entry: -15.942 %\n",
      "\n",
      "Current time: 2018-02-01 00:00:00\n",
      "Action start: Action.SELL , Total value before action: 4247.729169326602\n",
      "Before selling: coin:0.125, cash:2968.066, sell price:10243.975\n",
      "After selling: coin sold:0.012, transaction fees:0.320, coin now:0.112, cash now:3095.713\n",
      "Action end:  Action.SELL , Total value now: 4247.409.  , Return since entry: -15.052 %\n",
      "\n",
      "Current time: 2018-02-02 00:00:00\n",
      "Action start: Action.SELL , Total value before action: 4070.3140239701215\n",
      "Before selling: coin:0.146, cash:2774.530, sell price:8900.000\n",
      "After selling: coin sold:0.015, transaction fees:0.324, coin now:0.131, cash now:2903.785\n",
      "Action end:  Action.SELL , Total value now: 4069.990.  , Return since entry: -18.600 %\n",
      "\n",
      "Current time: 2018-02-03 00:00:00\n",
      "Action start: Action.SELL , Total value before action: 4040.581569934632\n",
      "Before selling: coin:0.163, cash:2608.078, sell price:8780.000\n",
      "After selling: coin sold:0.016, transaction fees:0.358, coin now:0.147, cash now:2750.971\n",
      "Action end:  Action.SELL , Total value now: 4040.223.  , Return since entry: -19.196 %\n",
      "\n",
      "Current time: 2018-02-04 00:00:00\n",
      "Action start: Action.BUY , Total value before action: 4068.2026242182546\n",
      "Before buying: coin:0.128, cash:2905.736, buy price:9067.081\n",
      "After buying: coin bought:0.032, transaction fees:0.726, coin now:0.160, cash now:2614.436\n",
      "Action end:  Action.BUY , Total value now: 4067.476.  , Return since entry: -18.650 %\n",
      "\n",
      "Current time: 2018-02-05 00:00:00\n",
      "Action start: Action.BUY , Total value before action: 3940.2455522798946\n",
      "Before buying: coin:0.139, cash:2792.751, buy price:8245.443\n",
      "After buying: coin bought:0.034, transaction fees:0.698, coin now:0.173, cash now:2512.777\n",
      "Action end:  Action.BUY , Total value now: 3939.547.  , Return since entry: -21.209 %\n",
      "\n",
      "Current time: 2018-02-06 00:00:00\n",
      "Action start: Action.BUY , Total value before action: 3713.137272706188\n",
      "Before buying: coin:0.153, cash:2646.772, buy price:6992.412\n",
      "After buying: coin bought:0.038, transaction fees:0.662, coin now:0.190, cash now:2381.433\n",
      "Action end:  Action.BUY , Total value now: 3712.476.  , Return since entry: -25.750 %\n",
      "\n",
      "Current time: 2018-02-07 00:00:00\n",
      "Action start: Action.SELL , Total value before action: 3783.9256488152087\n",
      "Before selling: coin:0.170, cash:2492.759, sell price:7610.000\n",
      "After selling: coin sold:0.017, transaction fees:0.323, coin now:0.153, cash now:2621.553\n",
      "Action end:  Action.SELL , Total value now: 3783.603.  , Return since entry: -24.328 %\n",
      "\n",
      "Current time: 2018-02-08 00:00:00\n",
      "Action start: Action.BUY , Total value before action: 3797.1115332001027\n",
      "Before buying: coin:0.138, cash:2728.976, buy price:7723.000\n",
      "After buying: coin bought:0.035, transaction fees:0.682, coin now:0.174, cash now:2455.397\n",
      "Action end:  Action.BUY , Total value now: 3796.429.  , Return since entry: -24.071 %\n",
      "\n",
      "Current time: 2018-02-09 00:00:00\n",
      "Action start: Action.SELL , Total value before action: 3819.1554100333683\n",
      "Before selling: coin:0.152, cash:2614.287, sell price:7929.687\n",
      "After selling: coin sold:0.015, transaction fees:0.301, coin now:0.137, cash now:2734.473\n",
      "Action end:  Action.SELL , Total value now: 3818.854.  , Return since entry: -23.623 %\n",
      "\n",
      "Current time: 2018-02-10 00:00:00\n",
      "Action start: Action.SELL , Total value before action: 3953.504465181998\n",
      "Before selling: coin:0.166, cash:2478.759, sell price:8888.000\n",
      "After selling: coin sold:0.017, transaction fees:0.369, coin now:0.149, cash now:2625.864\n",
      "Action end:  Action.SELL , Total value now: 3953.136.  , Return since entry: -20.937 %\n",
      "\n",
      "Current time: 2018-02-11 00:00:00\n",
      "Action start: Action.BUY , Total value before action: 3879.2286460433206\n",
      "Before buying: coin:0.136, cash:2740.955, buy price:8380.990\n",
      "After buying: coin bought:0.033, transaction fees:0.685, coin now:0.169, cash now:2466.175\n",
      "Action end:  Action.BUY , Total value now: 3878.543.  , Return since entry: -22.429 %\n",
      "\n",
      "Current time: 2018-02-12 00:00:00\n",
      "Action start: Action.SELL , Total value before action: 3833.9526949665324\n",
      "Before selling: coin:0.148, cash:2613.503, sell price:8223.970\n",
      "After selling: coin sold:0.015, transaction fees:0.305, coin now:0.134, cash now:2735.243\n",
      "Action end:  Action.SELL , Total value now: 3833.648.  , Return since entry: -23.327 %\n",
      "\n",
      "Current time: 2018-02-13 00:00:00\n",
      "Action start: Action.SELL , Total value before action: 3933.3085309840117\n",
      "Before selling: coin:0.162, cash:2493.092, sell price:8912.570\n",
      "After selling: coin sold:0.016, transaction fees:0.360, coin now:0.145, cash now:2636.753\n",
      "Action end:  Action.SELL , Total value now: 3932.948.  , Return since entry: -21.341 %\n",
      "\n",
      "Current time: 2018-02-14 00:00:00\n",
      "Action start: Action.SELL , Total value before action: 3871.3085511943727\n",
      "Before selling: coin:0.166, cash:2444.966, sell price:8590.990\n",
      "After selling: coin sold:0.017, transaction fees:0.357, coin now:0.149, cash now:2587.244\n",
      "Action end:  Action.SELL , Total value now: 3870.952.  , Return since entry: -22.581 %\n",
      "\n",
      "Current time: 2018-02-15 00:00:00\n",
      "Action start: Action.SELL , Total value before action: 3968.607755287705\n",
      "Before selling: coin:0.127, cash:2778.655, sell price:9354.000\n",
      "After selling: coin sold:0.013, transaction fees:0.297, coin now:0.114, cash now:2897.353\n",
      "Action end:  Action.SELL , Total value now: 3968.310.  , Return since entry: -20.634 %\n",
      "\n",
      "Current time: 2018-02-16 00:00:00\n",
      "Action start: Action.SELL , Total value before action: 4075.885002588968\n",
      "Before selling: coin:0.142, cash:2626.623, sell price:10225.075\n",
      "After selling: coin sold:0.014, transaction fees:0.362, coin now:0.128, cash now:2771.187\n",
      "Action end:  Action.SELL , Total value now: 4075.523.  , Return since entry: -18.490 %\n",
      "\n",
      "Current time: 2018-02-17 00:00:00\n",
      "Action start: Action.BUY , Total value before action: 4049.095818751892\n",
      "Before buying: coin:0.108, cash:2951.699, buy price:10173.057\n",
      "After buying: coin bought:0.029, transaction fees:0.738, coin now:0.137, cash now:2655.792\n",
      "Action end:  Action.BUY , Total value now: 4048.358.  , Return since entry: -19.033 %\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current time: 2018-02-18 00:00:00\n",
      "Action start: Action.BUY , Total value before action: 4146.475813470727\n",
      "Before buying: coin:0.111, cash:2919.457, buy price:11065.000\n",
      "After buying: coin bought:0.026, transaction fees:0.730, coin now:0.137, cash now:2626.781\n",
      "Action end:  Action.BUY , Total value now: 4145.746.  , Return since entry: -17.085 %\n",
      "\n",
      "Current time: 2018-02-19 00:00:00\n",
      "Action start: Action.SELL , Total value before action: 4078.181966558863\n",
      "Before selling: coin:0.121, cash:2813.375, sell price:10438.706\n",
      "After selling: coin sold:0.012, transaction fees:0.316, coin now:0.109, cash now:2939.539\n",
      "Action end:  Action.SELL , Total value now: 4077.866.  , Return since entry: -18.443 %\n",
      "\n",
      "Current time: 2018-02-20 00:00:00\n",
      "Action start: Action.SELL , Total value before action: 4182.487700670737\n",
      "Before selling: coin:0.133, cash:2666.184, sell price:11425.000\n",
      "After selling: coin sold:0.013, transaction fees:0.379, coin now:0.119, cash now:2817.435\n",
      "Action end:  Action.SELL , Total value now: 4182.109.  , Return since entry: -16.358 %\n",
      "\n",
      "Current time: 2018-02-21 00:00:00\n",
      "Action start: Action.SELL , Total value before action: 4131.565018678553\n",
      "Before selling: coin:0.134, cash:2642.511, sell price:11075.595\n",
      "After selling: coin sold:0.013, transaction fees:0.372, coin now:0.121, cash now:2791.044\n",
      "Action end:  Action.SELL , Total value now: 4131.193.  , Return since entry: -17.376 %\n",
      "\n",
      "Current time: 2018-02-22 00:00:00\n",
      "Action start: Action.SELL , Total value before action: 4047.9383186881623\n",
      "Before selling: coin:0.141, cash:2569.872, sell price:10455.792\n",
      "After selling: coin sold:0.014, transaction fees:0.370, coin now:0.127, cash now:2717.309\n",
      "Action end:  Action.SELL , Total value now: 4047.569.  , Return since entry: -19.049 %\n",
      "\n",
      "Current time: 2018-02-23 00:00:00\n",
      "Action start: Action.SELL , Total value before action: 3925.810517910489\n",
      "Before selling: coin:0.146, cash:2514.511, sell price:9660.059\n",
      "After selling: coin sold:0.015, transaction fees:0.353, coin now:0.131, cash now:2655.288\n",
      "Action end:  Action.SELL , Total value now: 3925.458.  , Return since entry: -21.491 %\n",
      "\n",
      "Current time: 2018-02-24 00:00:00\n",
      "Action start: Action.BUY , Total value before action: 3999.2817875987257\n",
      "Before buying: coin:0.115, cash:2814.451, buy price:10330.000\n",
      "After buying: coin bought:0.027, transaction fees:0.704, coin now:0.142, cash now:2532.302\n",
      "Action end:  Action.BUY , Total value now: 3998.578.  , Return since entry: -20.028 %\n",
      "\n",
      "Current time: 2018-02-25 00:00:00\n",
      "Action start: Action.BUY , Total value before action: 3898.3578735250767\n",
      "Before buying: coin:0.120, cash:2743.008, buy price:9640.362\n",
      "After buying: coin bought:0.028, transaction fees:0.686, coin now:0.148, cash now:2468.021\n",
      "Action end:  Action.BUY , Total value now: 3897.672.  , Return since entry: -22.047 %\n",
      "\n",
      "Current time: 2018-02-26 00:00:00\n",
      "Action start: Action.BUY , Total value before action: 3882.8620406689256\n",
      "Before buying: coin:0.122, cash:2718.213, buy price:9580.000\n",
      "After buying: coin bought:0.028, transaction fees:0.680, coin now:0.150, cash now:2445.712\n",
      "Action end:  Action.BUY , Total value now: 3882.182.  , Return since entry: -22.356 %\n",
      "\n",
      "Current time: 2018-02-27 00:00:00\n",
      "Action start: Action.SELL , Total value before action: 3972.994809885601\n",
      "Before selling: coin:0.137, cash:2550.051, sell price:10365.306\n",
      "After selling: coin sold:0.014, transaction fees:0.356, coin now:0.124, cash now:2691.989\n",
      "Action end:  Action.SELL , Total value now: 3972.639.  , Return since entry: -20.547 %\n",
      "\n",
      "Current time: 2018-02-28 00:00:00\n",
      "Action start: Action.BUY , Total value before action: 4000.5901494989575\n",
      "Before buying: coin:0.110, cash:2828.716, buy price:10649.457\n",
      "After buying: coin bought:0.027, transaction fees:0.707, coin now:0.137, cash now:2545.137\n",
      "Action end:  Action.BUY , Total value now: 3999.883.  , Return since entry: -20.002 %\n",
      "\n",
      "Current time: 2018-03-01 00:00:00\n",
      "Action start: Action.SELL , Total value before action: 3960.5599857385077\n",
      "Before selling: coin:0.116, cash:2752.107, sell price:10395.745\n",
      "After selling: coin sold:0.012, transaction fees:0.302, coin now:0.105, cash now:2872.650\n",
      "Action end:  Action.SELL , Total value now: 3960.258.  , Return since entry: -20.795 %\n",
      "\n",
      "Current time: 2018-03-02 00:00:00\n",
      "Action start: Action.SELL , Total value before action: 4025.789070320796\n",
      "Before selling: coin:0.126, cash:2633.602, sell price:11045.845\n",
      "After selling: coin sold:0.013, transaction fees:0.348, coin now:0.113, cash now:2772.472\n",
      "Action end:  Action.SELL , Total value now: 4025.441.  , Return since entry: -19.491 %\n",
      "\n",
      "Current time: 2018-03-03 00:00:00\n",
      "Action start: Action.SELL , Total value before action: 4024.9808841847607\n",
      "Before selling: coin:0.134, cash:2536.393, sell price:11095.269\n",
      "After selling: coin sold:0.013, transaction fees:0.372, coin now:0.121, cash now:2684.879\n",
      "Action end:  Action.SELL , Total value now: 4024.609.  , Return since entry: -19.508 %\n",
      "\n",
      "Current time: 2018-03-04 00:00:00\n",
      "Action start: Action.BUY , Total value before action: 4046.571941357662\n",
      "Before buying: coin:0.105, cash:2849.848, buy price:11360.000\n",
      "After buying: coin bought:0.025, transaction fees:0.712, coin now:0.130, cash now:2564.151\n",
      "Action end:  Action.BUY , Total value now: 4045.859.  , Return since entry: -19.083 %\n",
      "\n",
      "Current time: 2018-03-05 00:00:00\n",
      "Action start: Action.SELL , Total value before action: 4050.180285073384\n",
      "Before selling: coin:0.110, cash:2790.542, sell price:11475.000\n",
      "After selling: coin sold:0.011, transaction fees:0.315, coin now:0.099, cash now:2916.191\n",
      "Action end:  Action.SELL , Total value now: 4049.865.  , Return since entry: -19.003 %\n",
      "\n",
      "Current time: 2018-03-06 00:00:00\n",
      "Action start: Action.SELL , Total value before action: 4015.5666949186093\n",
      "Before selling: coin:0.116, cash:2706.307, sell price:11300.000\n",
      "After selling: coin sold:0.012, transaction fees:0.327, coin now:0.104, cash now:2836.906\n",
      "Action end:  Action.SELL , Total value now: 4015.239.  , Return since entry: -19.695 %\n",
      "\n",
      "Current time: 2018-03-07 00:00:00\n",
      "Action start: Action.SELL , Total value before action: 3958.082785835976\n",
      "Before selling: coin:0.121, cash:2647.715, sell price:10861.000\n",
      "After selling: coin sold:0.012, transaction fees:0.328, coin now:0.109, cash now:2778.424\n",
      "Action end:  Action.SELL , Total value now: 3957.755.  , Return since entry: -20.845 %\n",
      "\n",
      "Current time: 2018-03-08 00:00:00\n",
      "Action start: Action.SELL , Total value before action: 3846.6943737064557\n",
      "Before selling: coin:0.133, cash:2528.706, sell price:9931.618\n",
      "After selling: coin sold:0.013, transaction fees:0.329, coin now:0.119, cash now:2660.176\n",
      "Action end:  Action.SELL , Total value now: 3846.365.  , Return since entry: -23.073 %\n",
      "\n",
      "Current time: 2018-03-09 00:00:00\n",
      "Action start: Action.SELL , Total value before action: 3774.83099025395\n",
      "Before selling: coin:0.146, cash:2403.829, sell price:9371.090\n",
      "After selling: coin sold:0.015, transaction fees:0.343, coin now:0.132, cash now:2540.586\n",
      "Action end:  Action.SELL , Total value now: 3774.488.  , Return since entry: -24.510 %\n",
      "\n",
      "Current time: 2018-03-10 00:00:00\n",
      "Action start: Action.BUY , Total value before action: 3754.991185075172\n",
      "Before buying: coin:0.112, cash:2716.316, buy price:9306.972\n",
      "After buying: coin bought:0.029, transaction fees:0.679, coin now:0.141, cash now:2444.005\n",
      "Action end:  Action.BUY , Total value now: 3754.312.  , Return since entry: -24.914 %\n",
      "\n",
      "Current time: 2018-03-11 00:00:00\n",
      "Action start: Action.BUY , Total value before action: 3642.7463578967345\n",
      "Before buying: coin:0.121, cash:2614.248, buy price:8470.742\n",
      "After buying: coin bought:0.031, transaction fees:0.654, coin now:0.152, cash now:2352.170\n",
      "Action end:  Action.BUY , Total value now: 3642.093.  , Return since entry: -27.158 %\n",
      "\n",
      "Current time: 2018-03-12 00:00:00\n",
      "Action start: Action.SELL , Total value before action: 3793.328394148416\n",
      "Before selling: coin:0.131, cash:2539.253, sell price:9594.000\n",
      "After selling: coin sold:0.013, transaction fees:0.314, coin now:0.118, cash now:2664.347\n",
      "Action end:  Action.SELL , Total value now: 3793.015.  , Return since entry: -24.140 %\n",
      "\n",
      "Current time: 2018-03-13 00:00:00\n",
      "Action start: Action.SELL , Total value before action: 3747.3568569585354\n",
      "Before selling: coin:0.147, cash:2389.126, sell price:9224.100\n",
      "After selling: coin sold:0.015, transaction fees:0.340, coin now:0.133, cash now:2524.610\n",
      "Action end:  Action.SELL , Total value now: 3747.017.  , Return since entry: -25.060 %\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current time: 2018-03-14 00:00:00\n",
      "Action start: Action.BUY , Total value before action: 3743.4789017126104\n",
      "Before buying: coin:0.120, cash:2626.872, buy price:9307.537\n",
      "After buying: coin bought:0.028, transaction fees:0.657, coin now:0.148, cash now:2363.528\n",
      "Action end:  Action.BUY , Total value now: 3742.822.  , Return since entry: -25.144 %\n",
      "\n",
      "Current time: 2018-03-15 00:00:00\n",
      "Action start: Action.BUY , Total value before action: 3568.7876349954113\n",
      "Before buying: coin:0.127, cash:2540.643, buy price:8079.956\n",
      "After buying: coin bought:0.031, transaction fees:0.635, coin now:0.159, cash now:2285.944\n",
      "Action end:  Action.BUY , Total value now: 3568.152.  , Return since entry: -28.637 %\n",
      "\n",
      "Current time: 2018-03-16 00:00:00\n",
      "Action start: Action.SELL , Total value before action: 3553.414976997174\n",
      "Before selling: coin:0.142, cash:2412.114, sell price:8051.883\n",
      "After selling: coin sold:0.014, transaction fees:0.285, coin now:0.128, cash now:2525.959\n",
      "Action end:  Action.SELL , Total value now: 3553.130.  , Return since entry: -28.937 %\n",
      "\n",
      "Current time: 2018-03-17 00:00:00\n",
      "Action start: Action.SELL , Total value before action: 3581.2020011133764\n",
      "Before selling: coin:0.153, cash:2318.798, sell price:8274.703\n",
      "After selling: coin sold:0.015, transaction fees:0.316, coin now:0.137, cash now:2444.722\n",
      "Action end:  Action.SELL , Total value now: 3580.886.  , Return since entry: -28.382 %\n",
      "\n",
      "Current time: 2018-03-18 00:00:00\n",
      "Action start: Action.SELL , Total value before action: 3481.581467953282\n",
      "Before selling: coin:0.163, cash:2230.272, sell price:7686.000\n",
      "After selling: coin sold:0.016, transaction fees:0.313, coin now:0.147, cash now:2355.090\n",
      "Action end:  Action.SELL , Total value now: 3481.269.  , Return since entry: -30.375 %\n",
      "\n",
      "Current time: 2018-03-19 00:00:00\n",
      "Action start: Action.BUY , Total value before action: 3560.2688930867694\n",
      "Before buying: coin:0.122, cash:2549.107, buy price:8289.735\n",
      "After buying: coin bought:0.031, transaction fees:0.637, coin now:0.153, cash now:2293.559\n",
      "Action end:  Action.BUY , Total value now: 3559.632.  , Return since entry: -28.807 %\n",
      "\n",
      "Current time: 2018-03-20 00:00:00\n",
      "Action start: Action.SELL , Total value before action: 3591.452725209654\n",
      "Before selling: coin:0.136, cash:2436.144, sell price:8508.335\n",
      "After selling: coin sold:0.014, transaction fees:0.289, coin now:0.122, cash now:2551.386\n",
      "Action end:  Action.SELL , Total value now: 3591.164.  , Return since entry: -28.177 %\n",
      "\n",
      "Current time: 2018-03-21 00:00:00\n",
      "Action start: Action.SELL , Total value before action: 3645.5132378893113\n",
      "Before selling: coin:0.148, cash:2320.337, sell price:8936.000\n",
      "After selling: coin sold:0.015, transaction fees:0.331, coin now:0.133, cash now:2452.523\n",
      "Action end:  Action.SELL , Total value now: 3645.182.  , Return since entry: -27.096 %\n",
      "\n",
      "Current time: 2018-03-22 00:00:00\n",
      "Action start: Action.BUY , Total value before action: 3641.814168384907\n",
      "Before buying: coin:0.119, cash:2562.705, buy price:9030.671\n",
      "After buying: coin bought:0.028, transaction fees:0.641, coin now:0.148, cash now:2305.794\n",
      "Action end:  Action.BUY , Total value now: 3641.173.  , Return since entry: -27.177 %\n",
      "\n",
      "Current time: 2018-03-23 00:00:00\n",
      "Action start: Action.SELL , Total value before action: 3577.0538549351495\n",
      "Before selling: coin:0.123, cash:2514.918, sell price:8635.147\n",
      "After selling: coin sold:0.012, transaction fees:0.266, coin now:0.111, cash now:2620.866\n",
      "Action end:  Action.SELL , Total value now: 3576.788.  , Return since entry: -28.464 %\n",
      "\n",
      "Current time: 2018-03-24 00:00:00\n",
      "Action start: Action.SELL , Total value before action: 3602.588467086838\n",
      "Before selling: coin:0.140, cash:2353.222, sell price:8908.706\n",
      "After selling: coin sold:0.014, transaction fees:0.312, coin now:0.126, cash now:2477.847\n",
      "Action end:  Action.SELL , Total value now: 3602.276.  , Return since entry: -27.954 %\n",
      "\n",
      "Current time: 2018-03-25 00:00:00\n",
      "Action start: Action.SELL , Total value before action: 3544.9609820183823\n",
      "Before selling: coin:0.150, cash:2267.088, sell price:8535.150\n",
      "After selling: coin sold:0.015, transaction fees:0.319, coin now:0.135, cash now:2394.556\n",
      "Action end:  Action.SELL , Total value now: 3544.642.  , Return since entry: -29.107 %\n",
      "\n",
      "Current time: 2018-03-26 00:00:00\n",
      "Action start: Action.BUY , Total value before action: 3516.5579043949524\n",
      "Before buying: coin:0.119, cash:2521.763, buy price:8373.286\n",
      "After buying: coin bought:0.030, transaction fees:0.630, coin now:0.149, cash now:2268.956\n",
      "Action end:  Action.BUY , Total value now: 3515.927.  , Return since entry: -29.681 %\n",
      "\n",
      "Current time: 2018-03-27 00:00:00\n",
      "Action start: Action.SELL , Total value before action: 3487.237516545314\n",
      "Before selling: coin:0.128, cash:2438.935, sell price:8195.748\n",
      "After selling: coin sold:0.013, transaction fees:0.262, coin now:0.115, cash now:2543.503\n",
      "Action end:  Action.SELL , Total value now: 3486.975.  , Return since entry: -30.260 %\n",
      "\n",
      "Current time: 2018-03-28 00:00:00\n",
      "Action start: Action.SELL , Total value before action: 3428.3575299254653\n",
      "Before selling: coin:0.148, cash:2277.397, sell price:7798.405\n",
      "After selling: coin sold:0.015, transaction fees:0.288, coin now:0.133, cash now:2392.205\n",
      "Action end:  Action.SELL , Total value now: 3428.070.  , Return since entry: -31.439 %\n",
      "\n",
      "Current time: 2018-03-29 00:00:00\n",
      "Action start: Action.BUY , Total value before action: 3442.9457589453377\n",
      "Before buying: coin:0.116, cash:2521.500, buy price:7951.217\n",
      "After buying: coin bought:0.032, transaction fees:0.630, coin now:0.148, cash now:2268.720\n",
      "Action end:  Action.BUY , Total value now: 3442.315.  , Return since entry: -31.154 %\n",
      "\n",
      "Current time: 2018-03-30 00:00:00\n",
      "Action start: Action.HOLD , Total value before action: 3282.112816614116\n",
      "Action end:  Action.HOLD , Total value now: 3282.113.  , Return since entry: -34.358 %\n",
      "\n",
      "Current time: 2018-03-31 00:00:00\n",
      "Action start: Action.HOLD , Total value before action: 3306.4091381337225\n",
      "Action end:  Action.HOLD , Total value now: 3306.409.  , Return since entry: -33.872 %\n",
      "\n",
      "Current time: 2018-04-01 00:00:00\n",
      "Action start: Action.HOLD , Total value before action: 3296.268704984059\n",
      "Action end:  Action.HOLD , Total value now: 3296.269.  , Return since entry: -34.075 %\n",
      "\n",
      "Percentage return: -34.07462590031882\n"
     ]
    }
   ],
   "source": [
    "test_start = datetime.datetime(2018,1,1,0)\n",
    "test_end = datetime.datetime(2018,4,1,0)\n",
    "agent.test(sess, start_time = test_start, end_time = test_end, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x576 with 4 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_test(test_start, test_end)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Always buy agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from simple_agents import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "always_buy_agent = AlwaysBuyAgent(Action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = Environment()\n",
    "now = datetime.datetime(2017,12,1,0)\n",
    "test_end = datetime.datetime(2018,1,1,0)\n",
    "env.set_current_time(now)\n",
    "portfolio = Portfolio(cash_supply=1e8)\n",
    "while True:\n",
    "    action = always_buy_agent.get_action()\n",
    "    if (env.current_index - env.start_index) / env.time_delta % 24 == 0:\n",
    "        verbose = True\n",
    "        print('Current time:', env.current_index)\n",
    "    else:\n",
    "        verbose = False\n",
    "    current_price = env.getCurrentPrice()\n",
    "    action = portfolio.apply_action(current_price, action, verbose=verbose)\n",
    "    is_done, state = env.step()\n",
    "    if env.current_index == test_end:\n",
    "        break\n",
    "print(\"Initial cash supply:\", portfolio.starting_cash)\n",
    "print(\"Final holdings:\", portfolio.getCurrentHoldings(env.getCurrentPrice()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
