{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>USDT_BTC_high</th>\n",
       "      <th>USDT_BTC_low</th>\n",
       "      <th>USDT_BTC_close</th>\n",
       "      <th>USDT_BTC_open</th>\n",
       "      <th>USDT_BTC_volume</th>\n",
       "      <th>USDT_BTC_quoteVolume</th>\n",
       "      <th>USDT_BTC_weighted_mean</th>\n",
       "      <th>USDT_BTC_pctChange</th>\n",
       "      <th>USDT_ETH_high</th>\n",
       "      <th>USDT_ETH_low</th>\n",
       "      <th>...</th>\n",
       "      <th>BTC_LTC_weighted_mean</th>\n",
       "      <th>BTC_LTC_pctChange</th>\n",
       "      <th>BTC_XRP_high</th>\n",
       "      <th>BTC_XRP_low</th>\n",
       "      <th>BTC_XRP_close</th>\n",
       "      <th>BTC_XRP_open</th>\n",
       "      <th>BTC_XRP_volume</th>\n",
       "      <th>BTC_XRP_quoteVolume</th>\n",
       "      <th>BTC_XRP_weighted_mean</th>\n",
       "      <th>BTC_XRP_pctChange</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>time</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2016-01-02 12:00:00</th>\n",
       "      <td>432.5000</td>\n",
       "      <td>432.50</td>\n",
       "      <td>432.500000</td>\n",
       "      <td>432.50000</td>\n",
       "      <td>40.041239</td>\n",
       "      <td>0.092581</td>\n",
       "      <td>432.500000</td>\n",
       "      <td>2.220446e-16</td>\n",
       "      <td>0.959136</td>\n",
       "      <td>0.959136</td>\n",
       "      <td>...</td>\n",
       "      <td>0.008063</td>\n",
       "      <td>-0.002293</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.033605</td>\n",
       "      <td>2408.822942</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>-0.002859</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-01-02 13:00:00</th>\n",
       "      <td>432.5000</td>\n",
       "      <td>432.50</td>\n",
       "      <td>432.500000</td>\n",
       "      <td>432.50000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>432.986941</td>\n",
       "      <td>1.125876e-03</td>\n",
       "      <td>0.959136</td>\n",
       "      <td>0.959136</td>\n",
       "      <td>...</td>\n",
       "      <td>0.008060</td>\n",
       "      <td>-0.000333</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.004704</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-01-02 14:00:00</th>\n",
       "      <td>437.3635</td>\n",
       "      <td>432.48</td>\n",
       "      <td>433.336667</td>\n",
       "      <td>433.52799</td>\n",
       "      <td>359.269753</td>\n",
       "      <td>0.828819</td>\n",
       "      <td>433.473883</td>\n",
       "      <td>1.124610e-03</td>\n",
       "      <td>0.959136</td>\n",
       "      <td>0.957000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.008073</td>\n",
       "      <td>0.001623</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>1.141981</td>\n",
       "      <td>81071.098773</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.004682</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-01-02 15:00:00</th>\n",
       "      <td>432.4800</td>\n",
       "      <td>432.48</td>\n",
       "      <td>432.480000</td>\n",
       "      <td>432.48000</td>\n",
       "      <td>60.859598</td>\n",
       "      <td>0.140722</td>\n",
       "      <td>432.480000</td>\n",
       "      <td>-2.292832e-03</td>\n",
       "      <td>0.957000</td>\n",
       "      <td>0.957000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.008089</td>\n",
       "      <td>0.002002</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>2.120423</td>\n",
       "      <td>150622.792769</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>-0.000492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-01-02 16:00:00</th>\n",
       "      <td>432.4800</td>\n",
       "      <td>432.48</td>\n",
       "      <td>432.480000</td>\n",
       "      <td>432.48000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>432.350000</td>\n",
       "      <td>-3.005919e-04</td>\n",
       "      <td>0.957000</td>\n",
       "      <td>0.957000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.008079</td>\n",
       "      <td>-0.001224</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.491516</td>\n",
       "      <td>35178.793196</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>-0.007526</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 56 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                     USDT_BTC_high  USDT_BTC_low  USDT_BTC_close  \\\n",
       "time                                                               \n",
       "2016-01-02 12:00:00       432.5000        432.50      432.500000   \n",
       "2016-01-02 13:00:00       432.5000        432.50      432.500000   \n",
       "2016-01-02 14:00:00       437.3635        432.48      433.336667   \n",
       "2016-01-02 15:00:00       432.4800        432.48      432.480000   \n",
       "2016-01-02 16:00:00       432.4800        432.48      432.480000   \n",
       "\n",
       "                     USDT_BTC_open  USDT_BTC_volume  USDT_BTC_quoteVolume  \\\n",
       "time                                                                        \n",
       "2016-01-02 12:00:00      432.50000        40.041239              0.092581   \n",
       "2016-01-02 13:00:00      432.50000         0.000000              0.000000   \n",
       "2016-01-02 14:00:00      433.52799       359.269753              0.828819   \n",
       "2016-01-02 15:00:00      432.48000        60.859598              0.140722   \n",
       "2016-01-02 16:00:00      432.48000         0.000000              0.000000   \n",
       "\n",
       "                     USDT_BTC_weighted_mean  USDT_BTC_pctChange  \\\n",
       "time                                                              \n",
       "2016-01-02 12:00:00              432.500000        2.220446e-16   \n",
       "2016-01-02 13:00:00              432.986941        1.125876e-03   \n",
       "2016-01-02 14:00:00              433.473883        1.124610e-03   \n",
       "2016-01-02 15:00:00              432.480000       -2.292832e-03   \n",
       "2016-01-02 16:00:00              432.350000       -3.005919e-04   \n",
       "\n",
       "                     USDT_ETH_high  USDT_ETH_low        ...          \\\n",
       "time                                                    ...           \n",
       "2016-01-02 12:00:00       0.959136      0.959136        ...           \n",
       "2016-01-02 13:00:00       0.959136      0.959136        ...           \n",
       "2016-01-02 14:00:00       0.959136      0.957000        ...           \n",
       "2016-01-02 15:00:00       0.957000      0.957000        ...           \n",
       "2016-01-02 16:00:00       0.957000      0.957000        ...           \n",
       "\n",
       "                     BTC_LTC_weighted_mean  BTC_LTC_pctChange  BTC_XRP_high  \\\n",
       "time                                                                          \n",
       "2016-01-02 12:00:00               0.008063          -0.002293      0.000014   \n",
       "2016-01-02 13:00:00               0.008060          -0.000333      0.000014   \n",
       "2016-01-02 14:00:00               0.008073           0.001623      0.000014   \n",
       "2016-01-02 15:00:00               0.008089           0.002002      0.000014   \n",
       "2016-01-02 16:00:00               0.008079          -0.001224      0.000014   \n",
       "\n",
       "                     BTC_XRP_low  BTC_XRP_close  BTC_XRP_open  BTC_XRP_volume  \\\n",
       "time                                                                            \n",
       "2016-01-02 12:00:00     0.000014       0.000014      0.000014        0.033605   \n",
       "2016-01-02 13:00:00     0.000014       0.000014      0.000014        0.000000   \n",
       "2016-01-02 14:00:00     0.000014       0.000014      0.000014        1.141981   \n",
       "2016-01-02 15:00:00     0.000014       0.000014      0.000014        2.120423   \n",
       "2016-01-02 16:00:00     0.000014       0.000014      0.000014        0.491516   \n",
       "\n",
       "                     BTC_XRP_quoteVolume  BTC_XRP_weighted_mean  \\\n",
       "time                                                              \n",
       "2016-01-02 12:00:00          2408.822942               0.000014   \n",
       "2016-01-02 13:00:00             0.000000               0.000014   \n",
       "2016-01-02 14:00:00         81071.098773               0.000014   \n",
       "2016-01-02 15:00:00        150622.792769               0.000014   \n",
       "2016-01-02 16:00:00         35178.793196               0.000014   \n",
       "\n",
       "                     BTC_XRP_pctChange  \n",
       "time                                    \n",
       "2016-01-02 12:00:00          -0.002859  \n",
       "2016-01-02 13:00:00           0.004704  \n",
       "2016-01-02 14:00:00           0.004682  \n",
       "2016-01-02 15:00:00          -0.000492  \n",
       "2016-01-02 16:00:00          -0.007526  \n",
       "\n",
       "[5 rows x 56 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "__author__ = \"Yicheng Li\"\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import statsmodels.api as sm\n",
    "import random\n",
    "from sklearn import preprocessing\n",
    "import tensorflow as tf\n",
    "import os\n",
    "\n",
    "dir_path = ''\n",
    "df = pd.read_pickle(dir_path+'df_hourly_poloniex.pickle')\n",
    "df = df.dropna()\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = pd.read_pickle(dir_path+'df_hourly_news_sentiment_reach.pickle')\n",
    "df2 = df2.reset_index()\n",
    "\n",
    "df2['date'] = df2['datetime']\n",
    "df2['datetime'] = pd.to_datetime(df2['datetime'],unit='ms')\n",
    "\n",
    "df2 = df2.set_index('datetime')\n",
    "df2 = df2.rename(columns={'date':'timestamp'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>social_neutral</th>\n",
       "      <th>social_positive</th>\n",
       "      <th>social_negative</th>\n",
       "      <th>social_sentiment_mean</th>\n",
       "      <th>social_daily_sentiment_mean</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>datetime</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2017-02-02 00:00:00</th>\n",
       "      <td>11.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.424115</td>\n",
       "      <td>0.250033</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-02-02 01:00:00</th>\n",
       "      <td>19.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.288782</td>\n",
       "      <td>0.250033</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-02-02 02:00:00</th>\n",
       "      <td>12.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.205475</td>\n",
       "      <td>0.250033</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-02-02 03:00:00</th>\n",
       "      <td>10.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.367691</td>\n",
       "      <td>0.250033</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-02-02 04:00:00</th>\n",
       "      <td>16.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.233968</td>\n",
       "      <td>0.250033</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     social_neutral  social_positive  social_negative  \\\n",
       "datetime                                                                \n",
       "2017-02-02 00:00:00            11.0             13.0              1.0   \n",
       "2017-02-02 01:00:00            19.0             14.0              4.0   \n",
       "2017-02-02 02:00:00            12.0              7.0              2.0   \n",
       "2017-02-02 03:00:00            10.0             12.0              3.0   \n",
       "2017-02-02 04:00:00            16.0             10.0              3.0   \n",
       "\n",
       "                     social_sentiment_mean  social_daily_sentiment_mean  \n",
       "datetime                                                                 \n",
       "2017-02-02 00:00:00               0.424115                     0.250033  \n",
       "2017-02-02 01:00:00               0.288782                     0.250033  \n",
       "2017-02-02 02:00:00               0.205475                     0.250033  \n",
       "2017-02-02 03:00:00               0.367691                     0.250033  \n",
       "2017-02-02 04:00:00               0.233968                     0.250033  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df3 = pd.read_pickle(dir_path+'df_hourly_social_sentiment.pickle')\n",
    "\n",
    "df3 = df3.reset_index()\n",
    "\n",
    "df3['date'] = df3['datetime']\n",
    "df3['datetime'] = pd.to_datetime(df3['datetime'],unit='ms')\n",
    "\n",
    "df3 = df3.set_index('datetime')\n",
    "df3 = df3.rename(columns={'date':'timestamp'}).drop(columns='timestamp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "df4 = df.join(df2).join(df3).dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler_features = preprocessing.MinMaxScaler(feature_range=(0.1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to create train, validation, test data given sequence length\n",
    "def load_data(df, seq_len, feature_set, test_size=-1):\n",
    "    # prepare one-hot labels\n",
    "    labels = df['USDT_BTC_pctChange'].as_matrix().reshape([-1,1])\n",
    "    labels = np.concatenate([(labels > 3e-3)*1, ((3e-3 > labels)&(labels > -3e-3))*1, (labels < -3e-3)*1],1)\n",
    "    \n",
    "    data_raw = df.as_matrix() # convert to numpy array\n",
    "    # fit scaler\n",
    "    data_raw = scaler_features.fit_transform(data_raw[:, feature_set])\n",
    "    data = []\n",
    "    \n",
    "    # create all possible sequences of length seq_len\n",
    "    for index in range(len(data_raw) - seq_len + 1): \n",
    "        data.append(data_raw[index: index + seq_len, :])\n",
    "    \n",
    "    data = np.array(data)\n",
    "    \n",
    "    if test_size == -1: # take train and valid sets first\n",
    "        n_train_valid_pairs = 3\n",
    "        each_train_set_size_pct = 25\n",
    "        each_valid_set_size_pct = 5\n",
    "\n",
    "        each_train_set_size = round(each_train_set_size_pct/100*data.shape[0])\n",
    "        each_valid_set_size = round(each_valid_set_size_pct/100*data.shape[0])\n",
    "\n",
    "        x_train_sets = []\n",
    "        y_train_sets = []\n",
    "        x_valid_sets = []\n",
    "        y_valid_sets = []\n",
    "        used = 0\n",
    "\n",
    "        for i in range(n_train_valid_pairs):\n",
    "            x_train_sets.append(data[used : used + each_train_set_size,:-1,:]) # cannot see last day, which we aim to predict\n",
    "            y_train_sets.append(labels[used + seq_len-1 : used + each_train_set_size + seq_len-1, :])\n",
    "            used += each_train_set_size\n",
    "\n",
    "            x_valid_sets.append(data[used : used + each_valid_set_size,:-1,:])\n",
    "            y_valid_sets.append(labels[used + seq_len-1 : used + each_valid_set_size + seq_len-1, :])\n",
    "            used += each_valid_set_size\n",
    "\n",
    "        x_test = data[used : , :-1, :]\n",
    "        y_test = labels[seq_len-1 + used : , :]\n",
    "\n",
    "        x_train = np.concatenate(x_train_sets, axis=0)\n",
    "        y_train = np.concatenate(y_train_sets, axis=0)\n",
    "        x_valid = np.concatenate(x_valid_sets, axis=0)\n",
    "        y_valid = np.concatenate(y_valid_sets, axis=0)\n",
    "    \n",
    "    else: # take test set first\n",
    "        labels = labels[seq_len-1:] # so labels and data has same length\n",
    "        x_test = data[-test_size : , :-1, :]\n",
    "        y_test = labels[-test_size : , :]\n",
    "        \n",
    "        valid_start = data.shape[0] - test_size - int(test_size/2)\n",
    "        x_valid = data[valid_start:-test_size, :-1, :]\n",
    "        y_valid = labels[valid_start:-test_size, :]\n",
    "        \n",
    "        x_train = data[:valid_start, :-1, :]\n",
    "        y_train = labels[:valid_start, :]\n",
    "        \n",
    "    return [x_train, y_train, x_valid, y_valid, x_test, y_test]\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train.shape =  (6697, 9, 69)\n",
      "y_train.shape =  (6697, 3)\n",
      "x_valid.shape =  (998, 9, 69)\n",
      "y_valid.shape =  (998, 3)\n",
      "x_test.shape =  (1996, 9, 69)\n",
      "y_test.shape =  (1996, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shared/anaconda3/lib/python3.6/site-packages/sklearn/utils/validation.py:475: DataConversionWarning: Data with input dtype object was converted to float64 by MinMaxScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n"
     ]
    }
   ],
   "source": [
    "# create train, test data\n",
    "seq_len = 10 # choose sequence length\n",
    "feature_set = [x for x in range(df4.shape[-1])] #[0,1,2,3,4,6,7]\n",
    "x_train, y_train, x_valid, y_valid, x_test, y_test = load_data(df4, seq_len, feature_set, test_size=1996) # test_size = 1996\n",
    "# y_train = y_train.reshape([-1,1])\n",
    "# y_valid = y_valid.reshape([-1,1])\n",
    "# y_test = y_test.reshape([-1,1])\n",
    "print('x_train.shape = ',x_train.shape)\n",
    "print('y_train.shape = ', y_train.shape)\n",
    "print('x_valid.shape = ',x_valid.shape)\n",
    "print('y_valid.shape = ', y_valid.shape)\n",
    "print('x_test.shape = ', x_test.shape)\n",
    "print('y_test.shape = ',y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f1_score_single(y_true, y_pred):\n",
    "    TP = y_true.dot(y_pred) # zero or one\n",
    "    FP = np.sum(y_pred > y_true) # sum over all k classes, zero or one\n",
    "    FN = np.sum(y_pred < y_true) # sum over all k classes, zero or one\n",
    "    \n",
    "    if TP == 0: return 0.\n",
    "    p = 1. * TP / (TP + FP)\n",
    "    r = 1. * TP / (TP + FN)\n",
    "    return 2 * p * r / (p + r)\n",
    "    \n",
    "def acc(y_true, y_pred):\n",
    "    return np.mean([f1_score_single(x, y) for x, y in zip(y_true, y_pred)])\n",
    "\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "def F1(y_true, y_pred):\n",
    "    y_true = np.argmax(y_true, axis=1)\n",
    "    y_pred = np.argmax(y_pred, axis=1)\n",
    "    return f1_score(y_true, y_pred, average='macro')  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "baseline dev_acc= 0.39618856569709127\n",
      "baseline test_acc= 0.3712424849699399\n"
     ]
    }
   ],
   "source": [
    "y_pred = np.roll(y_valid,1, axis=0)\n",
    "print('baseline dev_acc=',acc(y_valid[1:], y_pred[1:]))\n",
    "y_pred = np.roll(y_test,1, axis=0)\n",
    "y_pred[0] = y_valid[-1] # be careful here\n",
    "print('baseline test_acc=',acc(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_in_epoch = 0;\n",
    "perm_array  = np.arange(x_train.shape[0])\n",
    "np.random.shuffle(perm_array)\n",
    "\n",
    "# function to get the next batch\n",
    "def get_next_batch(batch_size):\n",
    "    global index_in_epoch, x_train, perm_array   \n",
    "    start = index_in_epoch\n",
    "    index_in_epoch += batch_size\n",
    "    \n",
    "    if index_in_epoch > x_train.shape[0]:\n",
    "        np.random.shuffle(perm_array) # shuffle permutation array\n",
    "        start = 0 # start next epoch\n",
    "        index_in_epoch = batch_size\n",
    "        \n",
    "    end = index_in_epoch\n",
    "    return x_train[perm_array[start:end]], y_train[perm_array[start:end]]\n",
    "\n",
    "x_1000_train, y_1000_train = get_next_batch(1000) # special batch of 1000 records in training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM_Model(object):\n",
    "    def __init__(self, seq_len, use_batch_normalization):\n",
    "        # parameters\n",
    "        self.n_steps = seq_len-1 \n",
    "        self.n_inputs = x_train.shape[-1]\n",
    "        self.n_neurons = 50  # cell.state_size\n",
    "        self.n_bins = 3 # be careful if you want to change this\n",
    "        self.n_layers = 2\n",
    "        self.batch_size = 100\n",
    "        self.n_epochs = 0 # 0 means to train indefinitely\n",
    "        self.train_set_size = x_train.shape[0]\n",
    "        self.test_set_size = x_test.shape[0]\n",
    "        self.keep_prob = tf.placeholder(tf.float32, [])\n",
    "        self.beta = tf.placeholder(tf.float32, []) # to control l2 regularization\n",
    "        self.is_training = tf.placeholder(tf.bool)\n",
    "        self.max_gradient_norm = 5\n",
    "        with tf.variable_scope(\"LSTM_Model\", initializer=tf.contrib.layers.xavier_initializer()):\n",
    "            self.X = tf.placeholder(tf.float32, [None, self.n_steps, self.n_inputs])\n",
    "            self.y = tf.placeholder(tf.float32, [None, self.n_bins])\n",
    "\n",
    "            with tf.variable_scope(\"LSTM_set_1\"):\n",
    "                set1_layers = [tf.contrib.rnn.LSTMCell(num_units=self.n_neurons, \\\n",
    "                                                  initializer=tf.contrib.layers.xavier_initializer(), \\\n",
    "                                                  activation=tf.nn.elu)\n",
    "                         for layer in range(self.n_layers)]\n",
    "\n",
    "                set1_multi_layer_cell = tf.contrib.rnn.MultiRNNCell(set1_layers)\n",
    "\n",
    "                outputs_1, _ = tf.nn.dynamic_rnn(set1_multi_layer_cell, self.X[:,:,:-13], dtype=tf.float32)\n",
    "                \n",
    "            with tf.variable_scope(\"LSTM_set_2\"):\n",
    "                set2_layers = [tf.contrib.rnn.LSTMCell(num_units=self.n_neurons, \\\n",
    "                                                  initializer=tf.contrib.layers.xavier_initializer(), \\\n",
    "                                                  activation=tf.nn.elu)]\n",
    "\n",
    "                set2_multi_layer_cell = tf.contrib.rnn.MultiRNNCell(set2_layers)\n",
    "\n",
    "                outputs_2, _ = tf.nn.dynamic_rnn(set2_multi_layer_cell, self.X[:,:,-13:], dtype=tf.float32)\n",
    "            \n",
    "            outputs_1 = outputs_1[:,-1,:] # take last timestep\n",
    "            \n",
    "            outputs_2 = outputs_2[:,-1,:]\n",
    "            \n",
    "            stacked_outputs = tf.concat([outputs_1, outputs_2], axis = -1)\n",
    "            \n",
    "#             stacked_outputs = tf.nn.dropout(stacked_outputs, self.keep_prob) # dropout after LSTM\n",
    "            # 'outputs' is a tensor of shape [batch_size, n_steps, n_neurons(cell.state_size)]\n",
    "            \n",
    "            stacked_outputs = tf.layers.dense(stacked_outputs, 20, activation=tf.nn.elu)  # first FC\n",
    "            # batch normalization\n",
    "            if use_batch_normalization:\n",
    "                stacked_outputs = tf.contrib.layers.batch_norm(stacked_outputs, center=True, \\\n",
    "                                                               scale=False, is_training=self.is_training, scope='bn')\n",
    "            stacked_outputs = tf.nn.dropout(stacked_outputs, self.keep_prob)\n",
    "            stacked_outputs = tf.layers.dense(stacked_outputs, self.n_bins, activation=tf.nn.elu)  # second FC\n",
    "            \n",
    "            self.final_logits = tf.reshape(stacked_outputs, [-1, self.n_bins])\n",
    "            \n",
    "            self.final_logits = tf.nn.dropout(self.final_logits, self.keep_prob) # dropout\n",
    "            \n",
    "            self.indices = tf.argmax(self.final_logits, axis=-1) # (batchsize, 1)\n",
    "            self.preds = tf.one_hot(self.indices, depth=self.n_bins)\n",
    "            \n",
    "            self.each_loss = tf.nn.softmax_cross_entropy_with_logits(logits=self.final_logits, labels=self.y)\n",
    "            self.loss = tf.reduce_mean(self.each_loss) \n",
    "           \n",
    "            params = tf.trainable_variables()\n",
    "            \n",
    "            self.loss += self.beta * tf.add_n([ tf.nn.l2_loss(v) for v in params ])\n",
    "            \n",
    "            gradients = tf.gradients(self.loss, params)\n",
    "            self.gradient_norm = tf.global_norm(gradients)\n",
    "            clipped_gradients, _ = tf.clip_by_global_norm(gradients, self.max_gradient_norm)\n",
    "            clipped_norm = tf.global_norm(clipped_gradients)\n",
    "            self.param_norm = tf.global_norm(params)\n",
    "            self.learning_rate_placeholder = tf.placeholder(tf.float32, [], name='learning_rate')\n",
    "            optimizer = tf.train.RMSPropOptimizer(learning_rate=self.learning_rate_placeholder) \n",
    "            # training_op = optimizer.minimize(loss)\n",
    "            self.training_op = optimizer.apply_gradients(zip(clipped_gradients, params))\n",
    "\n",
    "            # initialize parameters\n",
    "#             sess = tf.Session()\n",
    "            self.global_step = tf.Variable(0, name='global_step', trainable=False)\n",
    "            self.saver = tf.train.Saver(max_to_keep=2)\n",
    "            self.bestmodel_saver = tf.train.Saver(max_to_keep=2)\n",
    "    \n",
    "    def train(self, session, experiment_name, keep_prob_val, beta):\n",
    "        \n",
    "        bestmodel_dir = experiment_name+'/best_ckpt'\n",
    "        bestmodel_ckpt_path = bestmodel_dir+'/best.ckpt'\n",
    "        best_valid_f1 = None\n",
    "        # Make bestmodel dir if necessary\n",
    "        if not os.path.exists(bestmodel_dir):\n",
    "            os.makedirs(bestmodel_dir)\n",
    "        \n",
    "        ckpt = tf.train.get_checkpoint_state(experiment_name)\n",
    "        v2_path = ckpt.model_checkpoint_path + \".index\" if ckpt else \"\"\n",
    "        if ckpt and (tf.gfile.Exists(ckpt.model_checkpoint_path) or tf.gfile.Exists(v2_path)):\n",
    "            self.saver.restore(sess, ckpt.model_checkpoint_path)\n",
    "            iteration = self.global_step.eval(session=sess) # get last global_step\n",
    "            print(\"Start from iteration:\", iteration)\n",
    "            lr = 1e-3\n",
    "        else:\n",
    "            print('There is not saved parameters. Creating model with fresh parameters.')\n",
    "            sess.run(tf.global_variables_initializer())\n",
    "            iteration = 0\n",
    "            lr = 1e-2 # should fix this...\n",
    "\n",
    "        old_loss = 1000\n",
    "        \n",
    "        while self.n_epochs == 0 or iteration*self.batch_size/self.train_set_size < self.n_epochs:\n",
    "            iteration = iteration + 1\n",
    "            x_batch, y_batch = get_next_batch(self.batch_size) # fetch the next training batch \n",
    "\n",
    "            # train on this batch\n",
    "            sess.run(self.training_op, feed_dict={self.X: x_batch, self.y: y_batch, self.learning_rate_placeholder:lr, \\\n",
    "                                                 self.keep_prob: keep_prob_val, self.beta: beta, self.is_training: True})\n",
    "\n",
    "            if iteration % 100 == 0:\n",
    "                y_1000_train_pred = []\n",
    "                loss_val = []\n",
    "                for idx in range(0, 1000, 100):\n",
    "                    y_1000_train_pred_chunk, loss_val_chunk, param_norm_val, grad_norm_val = \\\n",
    "                        sess.run([self.preds, self.loss, self.param_norm, self.gradient_norm],\\\n",
    "                            feed_dict={self.X: x_1000_train[idx:idx+100,:,:], self.y:y_1000_train[idx:idx+100,:], \\\n",
    "                                        self.learning_rate_placeholder:lr, self.keep_prob: 1, \\\n",
    "                                        self.beta: beta, self.is_training: False})\n",
    "                    y_1000_train_pred.append(y_1000_train_pred_chunk)\n",
    "                    loss_val.append(loss_val_chunk)\n",
    "                y_1000_train_pred = np.concatenate(y_1000_train_pred, axis=0)\n",
    "                loss_val = np.mean(loss_val)\n",
    "                    \n",
    "                if loss_val > old_loss * 1.2:\n",
    "                    lr /= 2\n",
    "                old_loss = loss_val\n",
    "\n",
    "                y_valid_pred = sess.run(self.preds, feed_dict={self.X: x_valid, self.keep_prob: 1,\\\n",
    "                                                              self.is_training: False})\n",
    "                \n",
    "                valid_f1 = acc(y_valid, y_valid_pred)\n",
    "                print('%.2f epochs, iter %d: train_loss = %.9f, param_norm = %.3f, grad_norm = %.3f, train_acc/valid_acc = %.6f/%.6f' \\\n",
    "                      %(iteration*self.batch_size/self.train_set_size, iteration, loss_val, param_norm_val, grad_norm_val, \\\n",
    "                        acc(y_1000_train, y_1000_train_pred), \\\n",
    "                        valid_f1))\n",
    "\n",
    "                if best_valid_f1 is None or valid_f1 > best_valid_f1:\n",
    "                    best_valid_f1 = valid_f1\n",
    "                    self.global_step.assign(iteration).eval(session=sess)\n",
    "                    print(\"======New best valid F1. Saving to %s...\" % bestmodel_ckpt_path)\n",
    "                    self.bestmodel_saver.save(sess, bestmodel_ckpt_path, global_step=self.global_step)\n",
    "                \n",
    "            if iteration % 1000 == 0:\n",
    "                self.global_step.assign(iteration).eval(session=sess) # set and update(eval) global_step with index, i\n",
    "                save_path = self.saver.save(sess, \"./\"+experiment_name+\"/model.ckpt\", global_step=self.global_step)\n",
    "                print('Saved parameters to %s' % save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## run experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_name=\"04-29-03\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "lstm_model = LSTM_Model(seq_len=seq_len, use_batch_normalization=False)\n",
    "sess = tf.Session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.1071486\n",
      "2.8860598\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shared/anaconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 50: train_loss = 1.083558083, train_F1= 0.193853\n",
      "iter 100: train_loss = 1.057697654, train_F1= 0.352005\n",
      "iter 150: train_loss = 1.014568448, train_F1= 0.414587\n",
      "iter 200: train_loss = 0.943552732, train_F1= 0.477160\n",
      "iter 250: train_loss = 0.872360945, train_F1= 0.539590\n",
      "iter 300: train_loss = 0.759383440, train_F1= 0.651793\n",
      "iter 350: train_loss = 0.748818457, train_F1= 0.582966\n",
      "iter 400: train_loss = 0.633967996, train_F1= 0.739263\n",
      "iter 450: train_loss = 0.569624782, train_F1= 0.797337\n",
      "iter 500: train_loss = 0.599731505, train_F1= 0.690629\n",
      "iter 550: train_loss = 0.474582285, train_F1= 0.810825\n",
      "iter 600: train_loss = 0.316559494, train_F1= 0.822157\n",
      "iter 650: train_loss = 0.547431171, train_F1= 0.817095\n",
      "iter 700: train_loss = 0.217297688, train_F1= 0.907511\n",
      "iter 750: train_loss = 0.203553900, train_F1= 0.926099\n",
      "iter 800: train_loss = 0.221336916, train_F1= 0.927721\n",
      "iter 850: train_loss = 0.164127231, train_F1= 0.947208\n",
      "iter 900: train_loss = 0.201678574, train_F1= 0.908490\n",
      "iter 950: train_loss = 0.122360855, train_F1= 0.970779\n",
      "iter 1000: train_loss = 0.042746402, train_F1= 0.979347\n",
      "iter 1050: train_loss = 0.468819737, train_F1= 0.874091\n"
     ]
    }
   ],
   "source": [
    "# sanity check\n",
    "sess.run(tf.global_variables_initializer())\n",
    "# first check random loss\n",
    "sanity_loss = sess.run(lstm_model.loss, feed_dict={lstm_model.X: x_1000_train, lstm_model.y:y_1000_train, \\\n",
    "                                        lstm_model.learning_rate_placeholder:1e-3, lstm_model.keep_prob: 1, \\\n",
    "                                                   lstm_model.beta: 0, lstm_model.is_training: False})\n",
    "print(sanity_loss) # should be around 1.09\n",
    "\n",
    "sanity_loss = sess.run(lstm_model.loss, feed_dict={lstm_model.X: x_1000_train, lstm_model.y:y_1000_train, \\\n",
    "                                        lstm_model.learning_rate_placeholder:1e-3, lstm_model.keep_prob: 1, \\\n",
    "                                                   lstm_model.beta: 0.005, lstm_model.is_training: False})\n",
    "print(sanity_loss) # should be around 1.09\n",
    "\n",
    "x_100_train, y_100_train = get_next_batch(100)\n",
    "iter_ = 0\n",
    "while True:\n",
    "    iter_ += 1\n",
    "    _, y_100_train_pred, loss_val = sess.run([lstm_model.training_op, lstm_model.preds, lstm_model.loss], \\\n",
    "                                             feed_dict={lstm_model.X: x_100_train, lstm_model.y:y_100_train, \\\n",
    "                                        lstm_model.learning_rate_placeholder:1e-3, lstm_model.keep_prob: 1, \\\n",
    "                                                        lstm_model.beta: 0, lstm_model.is_training: True})\n",
    "    if iter_ % 50 == 0:\n",
    "        print('iter %d: train_loss = %.9f, train_F1= %.6f' \\\n",
    "                      %(iter_, loss_val, F1(y_100_train, y_100_train_pred)))\n",
    "    if loss_val < 0.01:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There is not saved parameters. Creating model with fresh parameters.\n",
      "1.49 epochs, iter 100: train_loss = 1.112728834, param_norm = 20.052, grad_norm = 0.538, train_acc/valid_acc = 0.387000/0.236473\n",
      "======New best valid F1. Saving to 04-29-03/best_ckpt/best.ckpt...\n",
      "2.99 epochs, iter 200: train_loss = 1.109373689, param_norm = 14.710, grad_norm = 0.002, train_acc/valid_acc = 0.387000/0.397796\n",
      "======New best valid F1. Saving to 04-29-03/best_ckpt/best.ckpt...\n",
      "4.48 epochs, iter 300: train_loss = 1.083248496, param_norm = 9.477, grad_norm = 0.130, train_acc/valid_acc = 0.394000/0.413828\n",
      "======New best valid F1. Saving to 04-29-03/best_ckpt/best.ckpt...\n",
      "5.97 epochs, iter 400: train_loss = 1.097630262, param_norm = 8.210, grad_norm = 0.627, train_acc/valid_acc = 0.366000/0.419840\n",
      "======New best valid F1. Saving to 04-29-03/best_ckpt/best.ckpt...\n",
      "7.47 epochs, iter 500: train_loss = 1.099191189, param_norm = 7.206, grad_norm = 0.208, train_acc/valid_acc = 0.373000/0.395792\n",
      "8.96 epochs, iter 600: train_loss = 1.066260815, param_norm = 6.173, grad_norm = 0.227, train_acc/valid_acc = 0.418000/0.419840\n",
      "10.45 epochs, iter 700: train_loss = 1.076040506, param_norm = 7.216, grad_norm = 0.372, train_acc/valid_acc = 0.416000/0.419840\n",
      "11.95 epochs, iter 800: train_loss = 1.072216749, param_norm = 6.890, grad_norm = 0.112, train_acc/valid_acc = 0.415000/0.419840\n",
      "13.44 epochs, iter 900: train_loss = 1.064707994, param_norm = 7.631, grad_norm = 0.297, train_acc/valid_acc = 0.414000/0.419840\n",
      "14.93 epochs, iter 1000: train_loss = 1.060588241, param_norm = 6.857, grad_norm = 0.246, train_acc/valid_acc = 0.429000/0.419840\n",
      "Saved parameters to ./04-29-03/model.ckpt-1000\n",
      "16.43 epochs, iter 1100: train_loss = 1.060087442, param_norm = 8.649, grad_norm = 0.338, train_acc/valid_acc = 0.439000/0.412826\n",
      "17.92 epochs, iter 1200: train_loss = 1.066540956, param_norm = 8.995, grad_norm = 0.169, train_acc/valid_acc = 0.414000/0.399800\n",
      "19.41 epochs, iter 1300: train_loss = 1.098963976, param_norm = 8.811, grad_norm = 0.588, train_acc/valid_acc = 0.404000/0.219439\n",
      "20.90 epochs, iter 1400: train_loss = 1.076429963, param_norm = 8.188, grad_norm = 0.200, train_acc/valid_acc = 0.400000/0.321643\n",
      "22.40 epochs, iter 1500: train_loss = 1.060735703, param_norm = 8.859, grad_norm = 0.346, train_acc/valid_acc = 0.428000/0.255511\n",
      "23.89 epochs, iter 1600: train_loss = 1.061077356, param_norm = 9.502, grad_norm = 0.302, train_acc/valid_acc = 0.441000/0.275551\n",
      "25.38 epochs, iter 1700: train_loss = 1.053101540, param_norm = 9.109, grad_norm = 0.330, train_acc/valid_acc = 0.442000/0.284569\n",
      "26.88 epochs, iter 1800: train_loss = 1.059843659, param_norm = 8.885, grad_norm = 0.446, train_acc/valid_acc = 0.418000/0.312625\n",
      "28.37 epochs, iter 1900: train_loss = 1.057629585, param_norm = 9.793, grad_norm = 0.337, train_acc/valid_acc = 0.430000/0.238477\n",
      "29.86 epochs, iter 2000: train_loss = 1.061479688, param_norm = 8.571, grad_norm = 0.301, train_acc/valid_acc = 0.428000/0.263527\n",
      "Saved parameters to ./04-29-03/model.ckpt-2000\n",
      "31.36 epochs, iter 2100: train_loss = 1.071199536, param_norm = 9.674, grad_norm = 0.478, train_acc/valid_acc = 0.417000/0.226453\n",
      "32.85 epochs, iter 2200: train_loss = 1.050708413, param_norm = 9.535, grad_norm = 0.316, train_acc/valid_acc = 0.435000/0.270541\n",
      "34.34 epochs, iter 2300: train_loss = 1.051688910, param_norm = 8.681, grad_norm = 0.425, train_acc/valid_acc = 0.436000/0.263527\n",
      "35.84 epochs, iter 2400: train_loss = 1.049307346, param_norm = 9.320, grad_norm = 0.410, train_acc/valid_acc = 0.447000/0.233467\n",
      "37.33 epochs, iter 2500: train_loss = 1.045205355, param_norm = 8.915, grad_norm = 0.396, train_acc/valid_acc = 0.452000/0.244489\n",
      "38.82 epochs, iter 2600: train_loss = 1.049655199, param_norm = 9.459, grad_norm = 0.345, train_acc/valid_acc = 0.443000/0.244489\n",
      "40.32 epochs, iter 2700: train_loss = 1.061758995, param_norm = 9.110, grad_norm = 0.381, train_acc/valid_acc = 0.419000/0.282565\n",
      "41.81 epochs, iter 2800: train_loss = 1.046416879, param_norm = 9.586, grad_norm = 0.400, train_acc/valid_acc = 0.445000/0.225451\n",
      "43.30 epochs, iter 2900: train_loss = 1.050012112, param_norm = 9.797, grad_norm = 0.331, train_acc/valid_acc = 0.452000/0.239479\n",
      "44.80 epochs, iter 3000: train_loss = 1.054491878, param_norm = 9.689, grad_norm = 0.458, train_acc/valid_acc = 0.441000/0.225451\n",
      "Saved parameters to ./04-29-03/model.ckpt-3000\n",
      "46.29 epochs, iter 3100: train_loss = 1.069849849, param_norm = 9.559, grad_norm = 0.556, train_acc/valid_acc = 0.425000/0.224449\n",
      "47.78 epochs, iter 3200: train_loss = 1.053910136, param_norm = 9.740, grad_norm = 0.438, train_acc/valid_acc = 0.439000/0.229459\n",
      "49.28 epochs, iter 3300: train_loss = 1.047940850, param_norm = 10.253, grad_norm = 0.349, train_acc/valid_acc = 0.441000/0.233467\n",
      "50.77 epochs, iter 3400: train_loss = 1.072083473, param_norm = 10.518, grad_norm = 0.489, train_acc/valid_acc = 0.424000/0.220441\n",
      "52.26 epochs, iter 3500: train_loss = 1.065767765, param_norm = 10.668, grad_norm = 0.512, train_acc/valid_acc = 0.427000/0.226453\n",
      "53.76 epochs, iter 3600: train_loss = 1.050815701, param_norm = 10.311, grad_norm = 0.486, train_acc/valid_acc = 0.438000/0.268537\n",
      "55.25 epochs, iter 3700: train_loss = 1.052079558, param_norm = 10.057, grad_norm = 0.287, train_acc/valid_acc = 0.440000/0.253507\n",
      "56.74 epochs, iter 3800: train_loss = 1.055308104, param_norm = 10.294, grad_norm = 0.227, train_acc/valid_acc = 0.452000/0.232465\n",
      "58.24 epochs, iter 3900: train_loss = 1.049851418, param_norm = 10.625, grad_norm = 0.392, train_acc/valid_acc = 0.442000/0.267535\n",
      "59.73 epochs, iter 4000: train_loss = 1.055221915, param_norm = 10.712, grad_norm = 0.385, train_acc/valid_acc = 0.428000/0.219439\n",
      "Saved parameters to ./04-29-03/model.ckpt-4000\n",
      "61.22 epochs, iter 4100: train_loss = 1.044474363, param_norm = 10.703, grad_norm = 0.343, train_acc/valid_acc = 0.445000/0.244489\n",
      "62.71 epochs, iter 4200: train_loss = 1.048276305, param_norm = 10.858, grad_norm = 0.407, train_acc/valid_acc = 0.448000/0.229459\n",
      "64.21 epochs, iter 4300: train_loss = 1.124102712, param_norm = 10.599, grad_norm = 0.981, train_acc/valid_acc = 0.389000/0.312625\n",
      "65.70 epochs, iter 4400: train_loss = 1.048064590, param_norm = 11.477, grad_norm = 0.364, train_acc/valid_acc = 0.442000/0.239479\n",
      "67.19 epochs, iter 4500: train_loss = 1.056067705, param_norm = 11.483, grad_norm = 0.406, train_acc/valid_acc = 0.451000/0.222445\n",
      "68.69 epochs, iter 4600: train_loss = 1.053594351, param_norm = 10.855, grad_norm = 0.329, train_acc/valid_acc = 0.440000/0.250501\n",
      "70.18 epochs, iter 4700: train_loss = 1.055505276, param_norm = 12.106, grad_norm = 0.269, train_acc/valid_acc = 0.439000/0.252505\n",
      "71.67 epochs, iter 4800: train_loss = 1.052501082, param_norm = 11.669, grad_norm = 0.335, train_acc/valid_acc = 0.455000/0.245491\n",
      "73.17 epochs, iter 4900: train_loss = 1.047261477, param_norm = 11.149, grad_norm = 0.317, train_acc/valid_acc = 0.447000/0.232465\n",
      "74.66 epochs, iter 5000: train_loss = 1.055966020, param_norm = 11.768, grad_norm = 0.346, train_acc/valid_acc = 0.439000/0.232465\n",
      "Saved parameters to ./04-29-03/model.ckpt-5000\n",
      "76.15 epochs, iter 5100: train_loss = 1.051489592, param_norm = 11.086, grad_norm = 0.463, train_acc/valid_acc = 0.450000/0.235471\n",
      "77.65 epochs, iter 5200: train_loss = 1.045217752, param_norm = 11.207, grad_norm = 0.387, train_acc/valid_acc = 0.450000/0.244489\n",
      "79.14 epochs, iter 5300: train_loss = 1.052075267, param_norm = 11.553, grad_norm = 0.436, train_acc/valid_acc = 0.435000/0.271543\n",
      "80.63 epochs, iter 5400: train_loss = 1.043739080, param_norm = 11.639, grad_norm = 0.371, train_acc/valid_acc = 0.451000/0.237475\n",
      "82.13 epochs, iter 5500: train_loss = 1.045889139, param_norm = 11.453, grad_norm = 0.372, train_acc/valid_acc = 0.441000/0.242485\n",
      "83.62 epochs, iter 5600: train_loss = 1.043931723, param_norm = 11.029, grad_norm = 0.475, train_acc/valid_acc = 0.452000/0.234469\n",
      "85.11 epochs, iter 5700: train_loss = 1.044447899, param_norm = 11.995, grad_norm = 0.322, train_acc/valid_acc = 0.453000/0.248497\n",
      "86.61 epochs, iter 5800: train_loss = 1.065481663, param_norm = 11.506, grad_norm = 0.399, train_acc/valid_acc = 0.414000/0.254509\n",
      "88.10 epochs, iter 5900: train_loss = 1.055515051, param_norm = 11.396, grad_norm = 0.441, train_acc/valid_acc = 0.447000/0.231463\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "89.59 epochs, iter 6000: train_loss = 1.048379898, param_norm = 11.596, grad_norm = 0.279, train_acc/valid_acc = 0.455000/0.237475\n",
      "Saved parameters to ./04-29-03/model.ckpt-6000\n",
      "91.09 epochs, iter 6100: train_loss = 1.044320226, param_norm = 11.827, grad_norm = 0.344, train_acc/valid_acc = 0.451000/0.233467\n",
      "92.58 epochs, iter 6200: train_loss = 1.045494080, param_norm = 11.714, grad_norm = 0.344, train_acc/valid_acc = 0.461000/0.233467\n",
      "94.07 epochs, iter 6300: train_loss = 1.056118011, param_norm = 12.539, grad_norm = 0.299, train_acc/valid_acc = 0.445000/0.244489\n",
      "95.57 epochs, iter 6400: train_loss = 1.045501828, param_norm = 11.740, grad_norm = 0.372, train_acc/valid_acc = 0.451000/0.235471\n",
      "97.06 epochs, iter 6500: train_loss = 1.051348567, param_norm = 12.093, grad_norm = 0.301, train_acc/valid_acc = 0.438000/0.258517\n",
      "98.55 epochs, iter 6600: train_loss = 1.055734277, param_norm = 11.980, grad_norm = 0.427, train_acc/valid_acc = 0.432000/0.236473\n",
      "100.04 epochs, iter 6700: train_loss = 1.048235893, param_norm = 12.134, grad_norm = 0.352, train_acc/valid_acc = 0.452000/0.245491\n",
      "101.54 epochs, iter 6800: train_loss = 1.048495650, param_norm = 12.590, grad_norm = 0.346, train_acc/valid_acc = 0.445000/0.230461\n",
      "103.03 epochs, iter 6900: train_loss = 1.059994578, param_norm = 11.941, grad_norm = 0.495, train_acc/valid_acc = 0.433000/0.278557\n",
      "104.52 epochs, iter 7000: train_loss = 1.047066450, param_norm = 12.272, grad_norm = 0.327, train_acc/valid_acc = 0.454000/0.226453\n",
      "Saved parameters to ./04-29-03/model.ckpt-7000\n",
      "106.02 epochs, iter 7100: train_loss = 1.046964049, param_norm = 12.112, grad_norm = 0.382, train_acc/valid_acc = 0.447000/0.233467\n",
      "107.51 epochs, iter 7200: train_loss = 1.049484968, param_norm = 12.406, grad_norm = 0.405, train_acc/valid_acc = 0.446000/0.230461\n",
      "109.00 epochs, iter 7300: train_loss = 1.061636209, param_norm = 12.161, grad_norm = 0.442, train_acc/valid_acc = 0.428000/0.253507\n",
      "110.50 epochs, iter 7400: train_loss = 1.047708869, param_norm = 11.963, grad_norm = 0.402, train_acc/valid_acc = 0.448000/0.232465\n",
      "111.99 epochs, iter 7500: train_loss = 1.048873186, param_norm = 12.479, grad_norm = 0.348, train_acc/valid_acc = 0.440000/0.226453\n",
      "113.48 epochs, iter 7600: train_loss = 1.097085714, param_norm = 12.415, grad_norm = 0.561, train_acc/valid_acc = 0.413000/0.207415\n",
      "114.98 epochs, iter 7700: train_loss = 1.049762726, param_norm = 12.263, grad_norm = 0.435, train_acc/valid_acc = 0.450000/0.222445\n",
      "116.47 epochs, iter 7800: train_loss = 1.047218800, param_norm = 11.951, grad_norm = 0.355, train_acc/valid_acc = 0.451000/0.234469\n",
      "117.96 epochs, iter 7900: train_loss = 1.049105525, param_norm = 12.344, grad_norm = 0.380, train_acc/valid_acc = 0.452000/0.231463\n",
      "119.46 epochs, iter 8000: train_loss = 1.419312716, param_norm = 12.448, grad_norm = 3.435, train_acc/valid_acc = 0.389000/0.186373\n",
      "Saved parameters to ./04-29-03/model.ckpt-8000\n",
      "120.95 epochs, iter 8100: train_loss = 1.044936776, param_norm = 11.855, grad_norm = 0.348, train_acc/valid_acc = 0.465000/0.226453\n",
      "122.44 epochs, iter 8200: train_loss = 1.043197155, param_norm = 11.574, grad_norm = 0.444, train_acc/valid_acc = 0.458000/0.232465\n",
      "123.94 epochs, iter 8300: train_loss = 1.047761321, param_norm = 11.566, grad_norm = 0.521, train_acc/valid_acc = 0.446000/0.229459\n",
      "125.43 epochs, iter 8400: train_loss = 1.057346940, param_norm = 11.497, grad_norm = 0.640, train_acc/valid_acc = 0.445000/0.228457\n",
      "126.92 epochs, iter 8500: train_loss = 1.041809797, param_norm = 11.524, grad_norm = 0.458, train_acc/valid_acc = 0.459000/0.223447\n",
      "128.42 epochs, iter 8600: train_loss = 1.040248632, param_norm = 11.388, grad_norm = 0.384, train_acc/valid_acc = 0.471000/0.224449\n",
      "129.91 epochs, iter 8700: train_loss = 1.048867464, param_norm = 11.405, grad_norm = 0.589, train_acc/valid_acc = 0.444000/0.228457\n",
      "131.40 epochs, iter 8800: train_loss = 1.045335650, param_norm = 11.387, grad_norm = 0.382, train_acc/valid_acc = 0.444000/0.234469\n",
      "132.90 epochs, iter 8900: train_loss = 1.056572199, param_norm = 11.355, grad_norm = 0.384, train_acc/valid_acc = 0.426000/0.253507\n",
      "134.39 epochs, iter 9000: train_loss = 1.044824600, param_norm = 11.462, grad_norm = 0.404, train_acc/valid_acc = 0.448000/0.241483\n",
      "Saved parameters to ./04-29-03/model.ckpt-9000\n",
      "135.88 epochs, iter 9100: train_loss = 1.042022586, param_norm = 11.420, grad_norm = 0.329, train_acc/valid_acc = 0.449000/0.224449\n",
      "137.37 epochs, iter 9200: train_loss = 1.043298960, param_norm = 11.362, grad_norm = 0.354, train_acc/valid_acc = 0.460000/0.230461\n",
      "138.87 epochs, iter 9300: train_loss = 1.040916443, param_norm = 11.307, grad_norm = 0.402, train_acc/valid_acc = 0.472000/0.226453\n",
      "140.36 epochs, iter 9400: train_loss = 1.042676091, param_norm = 11.368, grad_norm = 0.431, train_acc/valid_acc = 0.456000/0.245491\n",
      "141.85 epochs, iter 9500: train_loss = 1.043616414, param_norm = 11.325, grad_norm = 0.343, train_acc/valid_acc = 0.457000/0.227455\n",
      "143.35 epochs, iter 9600: train_loss = 1.043731451, param_norm = 11.356, grad_norm = 0.576, train_acc/valid_acc = 0.451000/0.222445\n",
      "144.84 epochs, iter 9700: train_loss = 1.040086746, param_norm = 11.393, grad_norm = 0.402, train_acc/valid_acc = 0.477000/0.222445\n",
      "146.33 epochs, iter 9800: train_loss = 1.043389559, param_norm = 11.357, grad_norm = 0.548, train_acc/valid_acc = 0.453000/0.216433\n",
      "147.83 epochs, iter 9900: train_loss = 1.044546366, param_norm = 11.369, grad_norm = 0.469, train_acc/valid_acc = 0.454000/0.241483\n",
      "149.32 epochs, iter 10000: train_loss = 1.041345835, param_norm = 11.404, grad_norm = 0.437, train_acc/valid_acc = 0.463000/0.224449\n",
      "Saved parameters to ./04-29-03/model.ckpt-10000\n",
      "150.81 epochs, iter 10100: train_loss = 1.043755054, param_norm = 11.447, grad_norm = 0.558, train_acc/valid_acc = 0.451000/0.226453\n",
      "152.31 epochs, iter 10200: train_loss = 1.041310430, param_norm = 11.420, grad_norm = 0.523, train_acc/valid_acc = 0.461000/0.232465\n",
      "153.80 epochs, iter 10300: train_loss = 1.042811871, param_norm = 11.393, grad_norm = 0.553, train_acc/valid_acc = 0.452000/0.229459\n",
      "155.29 epochs, iter 10400: train_loss = 1.041358113, param_norm = 11.469, grad_norm = 0.413, train_acc/valid_acc = 0.454000/0.230461\n",
      "156.79 epochs, iter 10500: train_loss = 1.043828726, param_norm = 11.419, grad_norm = 0.384, train_acc/valid_acc = 0.447000/0.231463\n",
      "158.28 epochs, iter 10600: train_loss = 1.045025468, param_norm = 11.359, grad_norm = 0.330, train_acc/valid_acc = 0.465000/0.226453\n",
      "159.77 epochs, iter 10700: train_loss = 1.041457534, param_norm = 11.458, grad_norm = 0.483, train_acc/valid_acc = 0.456000/0.229459\n",
      "161.27 epochs, iter 10800: train_loss = 1.042589545, param_norm = 11.462, grad_norm = 0.342, train_acc/valid_acc = 0.465000/0.223447\n",
      "162.76 epochs, iter 10900: train_loss = 1.041306973, param_norm = 11.455, grad_norm = 0.380, train_acc/valid_acc = 0.461000/0.231463\n",
      "164.25 epochs, iter 11000: train_loss = 1.040417910, param_norm = 11.375, grad_norm = 0.476, train_acc/valid_acc = 0.465000/0.226453\n",
      "Saved parameters to ./04-29-03/model.ckpt-11000\n",
      "165.75 epochs, iter 11100: train_loss = 1.040431738, param_norm = 11.455, grad_norm = 0.530, train_acc/valid_acc = 0.459000/0.239479\n",
      "167.24 epochs, iter 11200: train_loss = 1.045235991, param_norm = 11.508, grad_norm = 0.365, train_acc/valid_acc = 0.459000/0.243487\n",
      "168.73 epochs, iter 11300: train_loss = 1.042555094, param_norm = 11.528, grad_norm = 0.602, train_acc/valid_acc = 0.457000/0.218437\n",
      "170.23 epochs, iter 11400: train_loss = 1.048854589, param_norm = 11.467, grad_norm = 0.299, train_acc/valid_acc = 0.438000/0.228457\n",
      "171.72 epochs, iter 11500: train_loss = 1.050989747, param_norm = 11.510, grad_norm = 0.629, train_acc/valid_acc = 0.449000/0.221443\n",
      "173.21 epochs, iter 11600: train_loss = 1.040551424, param_norm = 11.436, grad_norm = 0.514, train_acc/valid_acc = 0.461000/0.228457\n",
      "174.71 epochs, iter 11700: train_loss = 1.039523482, param_norm = 11.637, grad_norm = 0.570, train_acc/valid_acc = 0.458000/0.225451\n",
      "176.20 epochs, iter 11800: train_loss = 1.038487673, param_norm = 11.479, grad_norm = 0.425, train_acc/valid_acc = 0.473000/0.230461\n",
      "177.69 epochs, iter 11900: train_loss = 1.038507342, param_norm = 11.529, grad_norm = 0.496, train_acc/valid_acc = 0.471000/0.233467\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "179.18 epochs, iter 12000: train_loss = 1.041124582, param_norm = 11.490, grad_norm = 0.381, train_acc/valid_acc = 0.463000/0.227455\n",
      "Saved parameters to ./04-29-03/model.ckpt-12000\n",
      "180.68 epochs, iter 12100: train_loss = 1.062494159, param_norm = 11.503, grad_norm = 0.309, train_acc/valid_acc = 0.424000/0.244489\n",
      "182.17 epochs, iter 12200: train_loss = 1.058387518, param_norm = 11.509, grad_norm = 0.456, train_acc/valid_acc = 0.420000/0.251503\n",
      "183.66 epochs, iter 12300: train_loss = 1.039213538, param_norm = 11.576, grad_norm = 0.389, train_acc/valid_acc = 0.470000/0.235471\n",
      "185.16 epochs, iter 12400: train_loss = 1.042135477, param_norm = 11.680, grad_norm = 0.590, train_acc/valid_acc = 0.459000/0.226453\n",
      "186.65 epochs, iter 12500: train_loss = 1.042138100, param_norm = 11.558, grad_norm = 0.367, train_acc/valid_acc = 0.462000/0.246493\n",
      "188.14 epochs, iter 12600: train_loss = 1.040363669, param_norm = 11.679, grad_norm = 0.527, train_acc/valid_acc = 0.465000/0.222445\n",
      "189.64 epochs, iter 12700: train_loss = 1.042469978, param_norm = 11.679, grad_norm = 0.662, train_acc/valid_acc = 0.471000/0.223447\n",
      "191.13 epochs, iter 12800: train_loss = 1.040006280, param_norm = 11.584, grad_norm = 0.372, train_acc/valid_acc = 0.462000/0.229459\n",
      "192.62 epochs, iter 12900: train_loss = 1.042515516, param_norm = 11.516, grad_norm = 0.418, train_acc/valid_acc = 0.469000/0.227455\n",
      "194.12 epochs, iter 13000: train_loss = 1.037107229, param_norm = 11.609, grad_norm = 0.463, train_acc/valid_acc = 0.470000/0.230461\n",
      "Saved parameters to ./04-29-03/model.ckpt-13000\n",
      "195.61 epochs, iter 13100: train_loss = 1.037472486, param_norm = 11.568, grad_norm = 0.407, train_acc/valid_acc = 0.482000/0.231463\n",
      "197.10 epochs, iter 13200: train_loss = 1.047454119, param_norm = 11.631, grad_norm = 0.667, train_acc/valid_acc = 0.458000/0.219439\n",
      "198.60 epochs, iter 13300: train_loss = 1.041105032, param_norm = 11.704, grad_norm = 0.575, train_acc/valid_acc = 0.463000/0.223447\n",
      "200.09 epochs, iter 13400: train_loss = 1.042649388, param_norm = 11.813, grad_norm = 0.405, train_acc/valid_acc = 0.457000/0.257515\n",
      "201.58 epochs, iter 13500: train_loss = 1.046724319, param_norm = 11.721, grad_norm = 0.598, train_acc/valid_acc = 0.451000/0.266533\n",
      "203.08 epochs, iter 13600: train_loss = 1.073315620, param_norm = 11.726, grad_norm = 0.348, train_acc/valid_acc = 0.410000/0.281563\n",
      "204.57 epochs, iter 13700: train_loss = 1.045133352, param_norm = 11.703, grad_norm = 0.523, train_acc/valid_acc = 0.448000/0.255511\n",
      "206.06 epochs, iter 13800: train_loss = 1.052142382, param_norm = 11.675, grad_norm = 0.680, train_acc/valid_acc = 0.461000/0.223447\n",
      "207.56 epochs, iter 13900: train_loss = 1.066144228, param_norm = 11.738, grad_norm = 0.809, train_acc/valid_acc = 0.444000/0.220441\n",
      "209.05 epochs, iter 14000: train_loss = 1.041242599, param_norm = 11.787, grad_norm = 0.502, train_acc/valid_acc = 0.450000/0.253507\n",
      "Saved parameters to ./04-29-03/model.ckpt-14000\n",
      "210.54 epochs, iter 14100: train_loss = 1.036382914, param_norm = 11.936, grad_norm = 0.662, train_acc/valid_acc = 0.472000/0.234469\n",
      "212.04 epochs, iter 14200: train_loss = 1.040895224, param_norm = 11.877, grad_norm = 0.375, train_acc/valid_acc = 0.458000/0.246493\n",
      "213.53 epochs, iter 14300: train_loss = 1.035879374, param_norm = 11.957, grad_norm = 0.559, train_acc/valid_acc = 0.472000/0.238477\n",
      "215.02 epochs, iter 14400: train_loss = 1.050572276, param_norm = 12.113, grad_norm = 0.525, train_acc/valid_acc = 0.441000/0.281563\n",
      "216.51 epochs, iter 14500: train_loss = 1.033604741, param_norm = 12.044, grad_norm = 0.498, train_acc/valid_acc = 0.477000/0.246493\n",
      "218.01 epochs, iter 14600: train_loss = 1.033244848, param_norm = 12.071, grad_norm = 0.504, train_acc/valid_acc = 0.483000/0.254509\n",
      "219.50 epochs, iter 14700: train_loss = 1.034178615, param_norm = 12.067, grad_norm = 0.617, train_acc/valid_acc = 0.464000/0.223447\n",
      "220.99 epochs, iter 14800: train_loss = 1.036904454, param_norm = 11.989, grad_norm = 0.723, train_acc/valid_acc = 0.473000/0.247495\n",
      "222.49 epochs, iter 14900: train_loss = 1.035390139, param_norm = 12.067, grad_norm = 0.404, train_acc/valid_acc = 0.467000/0.242485\n",
      "223.98 epochs, iter 15000: train_loss = 1.037652612, param_norm = 12.105, grad_norm = 0.611, train_acc/valid_acc = 0.460000/0.219439\n",
      "Saved parameters to ./04-29-03/model.ckpt-15000\n",
      "225.47 epochs, iter 15100: train_loss = 1.054004908, param_norm = 12.271, grad_norm = 0.525, train_acc/valid_acc = 0.436000/0.258517\n",
      "226.97 epochs, iter 15200: train_loss = 1.036309481, param_norm = 12.235, grad_norm = 0.621, train_acc/valid_acc = 0.480000/0.228457\n",
      "228.46 epochs, iter 15300: train_loss = 1.031522036, param_norm = 12.322, grad_norm = 0.616, train_acc/valid_acc = 0.485000/0.228457\n",
      "229.95 epochs, iter 15400: train_loss = 1.033849478, param_norm = 12.287, grad_norm = 0.717, train_acc/valid_acc = 0.480000/0.233467\n",
      "231.45 epochs, iter 15500: train_loss = 1.032593369, param_norm = 12.226, grad_norm = 0.613, train_acc/valid_acc = 0.481000/0.227455\n",
      "232.94 epochs, iter 15600: train_loss = 1.033009768, param_norm = 12.418, grad_norm = 0.641, train_acc/valid_acc = 0.475000/0.221443\n",
      "234.43 epochs, iter 15700: train_loss = 1.034706950, param_norm = 12.352, grad_norm = 0.709, train_acc/valid_acc = 0.468000/0.223447\n",
      "235.93 epochs, iter 15800: train_loss = 1.036179304, param_norm = 12.445, grad_norm = 0.361, train_acc/valid_acc = 0.475000/0.232465\n",
      "237.42 epochs, iter 15900: train_loss = 1.030288696, param_norm = 12.521, grad_norm = 0.564, train_acc/valid_acc = 0.476000/0.236473\n",
      "238.91 epochs, iter 16000: train_loss = 1.041640043, param_norm = 12.510, grad_norm = 0.714, train_acc/valid_acc = 0.470000/0.223447\n",
      "Saved parameters to ./04-29-03/model.ckpt-16000\n",
      "240.41 epochs, iter 16100: train_loss = 1.040295362, param_norm = 12.440, grad_norm = 0.632, train_acc/valid_acc = 0.481000/0.229459\n",
      "241.90 epochs, iter 16200: train_loss = 1.037519693, param_norm = 12.514, grad_norm = 0.393, train_acc/valid_acc = 0.458000/0.261523\n",
      "243.39 epochs, iter 16300: train_loss = 1.032952309, param_norm = 12.514, grad_norm = 0.440, train_acc/valid_acc = 0.461000/0.255511\n",
      "244.89 epochs, iter 16400: train_loss = 1.036287546, param_norm = 12.558, grad_norm = 0.561, train_acc/valid_acc = 0.469000/0.244489\n",
      "246.38 epochs, iter 16500: train_loss = 1.035688639, param_norm = 12.563, grad_norm = 0.675, train_acc/valid_acc = 0.477000/0.241483\n",
      "247.87 epochs, iter 16600: train_loss = 1.031700611, param_norm = 12.687, grad_norm = 0.598, train_acc/valid_acc = 0.469000/0.258517\n",
      "249.37 epochs, iter 16700: train_loss = 1.030428529, param_norm = 12.833, grad_norm = 0.474, train_acc/valid_acc = 0.471000/0.253507\n",
      "250.86 epochs, iter 16800: train_loss = 1.034848213, param_norm = 12.831, grad_norm = 0.467, train_acc/valid_acc = 0.463000/0.256513\n",
      "252.35 epochs, iter 16900: train_loss = 1.031043410, param_norm = 12.752, grad_norm = 0.597, train_acc/valid_acc = 0.468000/0.237475\n"
     ]
    }
   ],
   "source": [
    "lstm_model.train(session=sess, experiment_name=experiment_name, keep_prob_val=1, beta=1e-4)\n",
    "# IMPORTANT:\n",
    "# when you think F1 is not going to improve anymore, wait another 50-100 epochs. \n",
    "# if you see any better iteration that has not appeared before, keep waiting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from 04-29-03/best_ckpt/best.ckpt-87500\n",
      "train acc: 0.602\n",
      "dev acc: 0.46993987975951906\n",
      "test acc: 0.40180360721442887\n"
     ]
    }
   ],
   "source": [
    "# load best checkpoint (based on dev f1) and evaluate\n",
    "ckpt = tf.train.get_checkpoint_state(experiment_name+'/best_ckpt')\n",
    "v2_path = ckpt.model_checkpoint_path + \".index\" if ckpt else \"\"\n",
    "if ckpt and (tf.gfile.Exists(ckpt.model_checkpoint_path) or tf.gfile.Exists(v2_path)):\n",
    "    lstm_model.saver.restore(sess, ckpt.model_checkpoint_path)\n",
    "else:\n",
    "    raise ValueError('What? you dont have a best checkpoint?')\n",
    "\n",
    "y_1000_train_pred = sess.run(lstm_model.preds, feed_dict={lstm_model.X: x_1000_train, lstm_model.keep_prob: 1, \\\n",
    "                                                          lstm_model.beta:0, lstm_model.is_training: False})\n",
    "print(\"train acc:\",acc(y_1000_train, y_1000_train_pred))\n",
    "y_valid_pred = sess.run(lstm_model.preds, feed_dict={lstm_model.X: x_valid, lstm_model.keep_prob: 1, \\\n",
    "                                                     lstm_model.beta:0, lstm_model.is_training: False})\n",
    "print(\"dev acc:\",acc(y_valid, y_valid_pred))\n",
    "y_test_pred = sess.run(lstm_model.preds, feed_dict={lstm_model.X: x_test, lstm_model.keep_prob: 1, \\\n",
    "                                                    lstm_model.beta:0, lstm_model.is_training: False})\n",
    "print(\"test acc:\",acc(y_test, y_test_pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
