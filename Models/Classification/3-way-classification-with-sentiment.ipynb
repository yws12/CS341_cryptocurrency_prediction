{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>USDT_BTC_high</th>\n",
       "      <th>USDT_BTC_low</th>\n",
       "      <th>USDT_BTC_close</th>\n",
       "      <th>USDT_BTC_open</th>\n",
       "      <th>USDT_BTC_volume</th>\n",
       "      <th>USDT_BTC_quoteVolume</th>\n",
       "      <th>USDT_BTC_weighted_mean</th>\n",
       "      <th>USDT_BTC_pctChange</th>\n",
       "      <th>USDT_ETH_high</th>\n",
       "      <th>USDT_ETH_low</th>\n",
       "      <th>...</th>\n",
       "      <th>BTC_LTC_weighted_mean</th>\n",
       "      <th>BTC_LTC_pctChange</th>\n",
       "      <th>BTC_XRP_high</th>\n",
       "      <th>BTC_XRP_low</th>\n",
       "      <th>BTC_XRP_close</th>\n",
       "      <th>BTC_XRP_open</th>\n",
       "      <th>BTC_XRP_volume</th>\n",
       "      <th>BTC_XRP_quoteVolume</th>\n",
       "      <th>BTC_XRP_weighted_mean</th>\n",
       "      <th>BTC_XRP_pctChange</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>time</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2016-01-02 12:00:00</th>\n",
       "      <td>432.5000</td>\n",
       "      <td>432.50</td>\n",
       "      <td>432.500000</td>\n",
       "      <td>432.50000</td>\n",
       "      <td>40.041239</td>\n",
       "      <td>0.092581</td>\n",
       "      <td>432.500000</td>\n",
       "      <td>2.220446e-16</td>\n",
       "      <td>0.959136</td>\n",
       "      <td>0.959136</td>\n",
       "      <td>...</td>\n",
       "      <td>0.008063</td>\n",
       "      <td>-0.002293</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.033605</td>\n",
       "      <td>2408.822942</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>-0.002859</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-01-02 13:00:00</th>\n",
       "      <td>432.5000</td>\n",
       "      <td>432.50</td>\n",
       "      <td>432.500000</td>\n",
       "      <td>432.50000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>432.986941</td>\n",
       "      <td>1.125876e-03</td>\n",
       "      <td>0.959136</td>\n",
       "      <td>0.959136</td>\n",
       "      <td>...</td>\n",
       "      <td>0.008060</td>\n",
       "      <td>-0.000333</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.004704</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-01-02 14:00:00</th>\n",
       "      <td>437.3635</td>\n",
       "      <td>432.48</td>\n",
       "      <td>433.336667</td>\n",
       "      <td>433.52799</td>\n",
       "      <td>359.269753</td>\n",
       "      <td>0.828819</td>\n",
       "      <td>433.473883</td>\n",
       "      <td>1.124610e-03</td>\n",
       "      <td>0.959136</td>\n",
       "      <td>0.957000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.008073</td>\n",
       "      <td>0.001623</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>1.141981</td>\n",
       "      <td>81071.098773</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.004682</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-01-02 15:00:00</th>\n",
       "      <td>432.4800</td>\n",
       "      <td>432.48</td>\n",
       "      <td>432.480000</td>\n",
       "      <td>432.48000</td>\n",
       "      <td>60.859598</td>\n",
       "      <td>0.140722</td>\n",
       "      <td>432.480000</td>\n",
       "      <td>-2.292832e-03</td>\n",
       "      <td>0.957000</td>\n",
       "      <td>0.957000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.008089</td>\n",
       "      <td>0.002002</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>2.120423</td>\n",
       "      <td>150622.792769</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>-0.000492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-01-02 16:00:00</th>\n",
       "      <td>432.4800</td>\n",
       "      <td>432.48</td>\n",
       "      <td>432.480000</td>\n",
       "      <td>432.48000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>432.350000</td>\n",
       "      <td>-3.005919e-04</td>\n",
       "      <td>0.957000</td>\n",
       "      <td>0.957000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.008079</td>\n",
       "      <td>-0.001224</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.491516</td>\n",
       "      <td>35178.793196</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>-0.007526</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 56 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                     USDT_BTC_high  USDT_BTC_low  USDT_BTC_close  \\\n",
       "time                                                               \n",
       "2016-01-02 12:00:00       432.5000        432.50      432.500000   \n",
       "2016-01-02 13:00:00       432.5000        432.50      432.500000   \n",
       "2016-01-02 14:00:00       437.3635        432.48      433.336667   \n",
       "2016-01-02 15:00:00       432.4800        432.48      432.480000   \n",
       "2016-01-02 16:00:00       432.4800        432.48      432.480000   \n",
       "\n",
       "                     USDT_BTC_open  USDT_BTC_volume  USDT_BTC_quoteVolume  \\\n",
       "time                                                                        \n",
       "2016-01-02 12:00:00      432.50000        40.041239              0.092581   \n",
       "2016-01-02 13:00:00      432.50000         0.000000              0.000000   \n",
       "2016-01-02 14:00:00      433.52799       359.269753              0.828819   \n",
       "2016-01-02 15:00:00      432.48000        60.859598              0.140722   \n",
       "2016-01-02 16:00:00      432.48000         0.000000              0.000000   \n",
       "\n",
       "                     USDT_BTC_weighted_mean  USDT_BTC_pctChange  \\\n",
       "time                                                              \n",
       "2016-01-02 12:00:00              432.500000        2.220446e-16   \n",
       "2016-01-02 13:00:00              432.986941        1.125876e-03   \n",
       "2016-01-02 14:00:00              433.473883        1.124610e-03   \n",
       "2016-01-02 15:00:00              432.480000       -2.292832e-03   \n",
       "2016-01-02 16:00:00              432.350000       -3.005919e-04   \n",
       "\n",
       "                     USDT_ETH_high  USDT_ETH_low        ...          \\\n",
       "time                                                    ...           \n",
       "2016-01-02 12:00:00       0.959136      0.959136        ...           \n",
       "2016-01-02 13:00:00       0.959136      0.959136        ...           \n",
       "2016-01-02 14:00:00       0.959136      0.957000        ...           \n",
       "2016-01-02 15:00:00       0.957000      0.957000        ...           \n",
       "2016-01-02 16:00:00       0.957000      0.957000        ...           \n",
       "\n",
       "                     BTC_LTC_weighted_mean  BTC_LTC_pctChange  BTC_XRP_high  \\\n",
       "time                                                                          \n",
       "2016-01-02 12:00:00               0.008063          -0.002293      0.000014   \n",
       "2016-01-02 13:00:00               0.008060          -0.000333      0.000014   \n",
       "2016-01-02 14:00:00               0.008073           0.001623      0.000014   \n",
       "2016-01-02 15:00:00               0.008089           0.002002      0.000014   \n",
       "2016-01-02 16:00:00               0.008079          -0.001224      0.000014   \n",
       "\n",
       "                     BTC_XRP_low  BTC_XRP_close  BTC_XRP_open  BTC_XRP_volume  \\\n",
       "time                                                                            \n",
       "2016-01-02 12:00:00     0.000014       0.000014      0.000014        0.033605   \n",
       "2016-01-02 13:00:00     0.000014       0.000014      0.000014        0.000000   \n",
       "2016-01-02 14:00:00     0.000014       0.000014      0.000014        1.141981   \n",
       "2016-01-02 15:00:00     0.000014       0.000014      0.000014        2.120423   \n",
       "2016-01-02 16:00:00     0.000014       0.000014      0.000014        0.491516   \n",
       "\n",
       "                     BTC_XRP_quoteVolume  BTC_XRP_weighted_mean  \\\n",
       "time                                                              \n",
       "2016-01-02 12:00:00          2408.822942               0.000014   \n",
       "2016-01-02 13:00:00             0.000000               0.000014   \n",
       "2016-01-02 14:00:00         81071.098773               0.000014   \n",
       "2016-01-02 15:00:00        150622.792769               0.000014   \n",
       "2016-01-02 16:00:00         35178.793196               0.000014   \n",
       "\n",
       "                     BTC_XRP_pctChange  \n",
       "time                                    \n",
       "2016-01-02 12:00:00          -0.002859  \n",
       "2016-01-02 13:00:00           0.004704  \n",
       "2016-01-02 14:00:00           0.004682  \n",
       "2016-01-02 15:00:00          -0.000492  \n",
       "2016-01-02 16:00:00          -0.007526  \n",
       "\n",
       "[5 rows x 56 columns]"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "__author__ = \"Yicheng Li\"\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import statsmodels.api as sm\n",
    "import random\n",
    "from sklearn import preprocessing\n",
    "import tensorflow as tf\n",
    "import os\n",
    "\n",
    "dir_path = 'CS341-repo/Data/'\n",
    "df = pd.read_pickle(dir_path+'df_hourly_poloniex.pickle')\n",
    "df = df.dropna()\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = pd.read_pickle(dir_path+'df_hourly_news_sentiment_reach.pickle')\n",
    "df2 = df2.reset_index()\n",
    "\n",
    "df2['date'] = df2['datetime']\n",
    "df2['datetime'] = pd.to_datetime(df2['datetime'],unit='ms')\n",
    "\n",
    "df2 = df2.set_index('datetime')\n",
    "df2 = df2.rename(columns={'date':'timestamp'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3 = df.join(df2).dropna() # drop nan later so we don't mess up with test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler_features = preprocessing.MinMaxScaler(feature_range=(0.1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to create train, validation, test data given sequence length\n",
    "def load_data(df, seq_len, feature_set, test_size=-1):\n",
    "    # prepare one-hot labels\n",
    "    labels = df['USDT_BTC_pctChange'].as_matrix().reshape([-1,1])\n",
    "    labels = np.concatenate([(labels > 3e-3)*1, ((3e-3 > labels)&(labels > -3e-3))*1, (labels < -3e-3)*1],1)\n",
    "    \n",
    "    data_raw = df.as_matrix() # convert to numpy array\n",
    "    # fit scaler\n",
    "    data_raw = scaler_features.fit_transform(data_raw[:, feature_set])\n",
    "    data = []\n",
    "    \n",
    "    # create all possible sequences of length seq_len\n",
    "    for index in range(len(data_raw) - seq_len + 1): \n",
    "        data.append(data_raw[index: index + seq_len, :])\n",
    "    \n",
    "    data = np.array(data)\n",
    "    \n",
    "    if test_size == -1: # take train and valid sets first\n",
    "        n_train_valid_pairs = 3\n",
    "        each_train_set_size_pct = 25\n",
    "        each_valid_set_size_pct = 5\n",
    "\n",
    "        each_train_set_size = round(each_train_set_size_pct/100*data.shape[0])\n",
    "        each_valid_set_size = round(each_valid_set_size_pct/100*data.shape[0])\n",
    "\n",
    "        x_train_sets = []\n",
    "        y_train_sets = []\n",
    "        x_valid_sets = []\n",
    "        y_valid_sets = []\n",
    "        used = 0\n",
    "\n",
    "        for i in range(n_train_valid_pairs):\n",
    "            x_train_sets.append(data[used : used + each_train_set_size,:-1,:]) # cannot see last day, which we aim to predict\n",
    "            y_train_sets.append(labels[used + seq_len-1 : used + each_train_set_size + seq_len-1, :])\n",
    "            used += each_train_set_size\n",
    "\n",
    "            x_valid_sets.append(data[used : used + each_valid_set_size,:-1,:])\n",
    "            y_valid_sets.append(labels[used + seq_len-1 : used + each_valid_set_size + seq_len-1, :])\n",
    "            used += each_valid_set_size\n",
    "\n",
    "        x_test = data[used : , :-1, :]\n",
    "        y_test = labels[seq_len-1 + used : , :]\n",
    "\n",
    "        x_train = np.concatenate(x_train_sets, axis=0)\n",
    "        y_train = np.concatenate(y_train_sets, axis=0)\n",
    "        x_valid = np.concatenate(x_valid_sets, axis=0)\n",
    "        y_valid = np.concatenate(y_valid_sets, axis=0)\n",
    "    \n",
    "    else: # take test set first\n",
    "        labels = labels[seq_len-1:] # so labels and data has same length\n",
    "        x_test = data[-test_size : , :-1, :]\n",
    "        y_test = labels[-test_size : , :]\n",
    "        \n",
    "        valid_start = data.shape[0] - test_size - int(test_size/2)\n",
    "        x_valid = data[valid_start:-test_size, :-1, :]\n",
    "        y_valid = labels[valid_start:-test_size, :]\n",
    "        \n",
    "        x_train = data[:valid_start, :-1, :]\n",
    "        y_train = labels[:valid_start, :]\n",
    "        \n",
    "    return [x_train, y_train, x_valid, y_valid, x_test, y_test]\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train.shape =  (6711, 9, 64)\n",
      "y_train.shape =  (6711, 3)\n",
      "x_valid.shape =  (998, 9, 64)\n",
      "y_valid.shape =  (998, 3)\n",
      "x_test.shape =  (1996, 9, 64)\n",
      "y_test.shape =  (1996, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/sklearn/utils/validation.py:475: DataConversionWarning: Data with input dtype object was converted to float64 by MinMaxScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n"
     ]
    }
   ],
   "source": [
    "# create train, test data\n",
    "seq_len = 10 # choose sequence length\n",
    "feature_set = [x for x in range(64)] #[0,1,2,3,4,6,7]\n",
    "x_train, y_train, x_valid, y_valid, x_test, y_test = load_data(df3, seq_len, feature_set, test_size=1996) # test_size = 1996\n",
    "# y_train = y_train.reshape([-1,1])\n",
    "# y_valid = y_valid.reshape([-1,1])\n",
    "# y_test = y_test.reshape([-1,1])\n",
    "print('x_train.shape = ',x_train.shape)\n",
    "print('y_train.shape = ', y_train.shape)\n",
    "print('x_valid.shape = ',x_valid.shape)\n",
    "print('y_valid.shape = ', y_valid.shape)\n",
    "print('x_test.shape = ', x_test.shape)\n",
    "print('y_test.shape = ',y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f1_score_single(y_true, y_pred):\n",
    "    TP = y_true.dot(y_pred) # zero or one\n",
    "    FP = np.sum(y_pred > y_true) # sum over all k classes, zero or one\n",
    "    FN = np.sum(y_pred < y_true) # sum over all k classes, zero or one\n",
    "    \n",
    "    if TP == 0: return 0.\n",
    "    p = 1. * TP / (TP + FP)\n",
    "    r = 1. * TP / (TP + FN)\n",
    "    return 2 * p * r / (p + r)\n",
    "    \n",
    "def F1(y_true, y_pred):\n",
    "    return np.mean([f1_score_single(x, y) for x, y in zip(y_true, y_pred)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "baseline dev_F1= 0.39618856569709127\n",
      "baseline test_F1= 0.3712424849699399\n"
     ]
    }
   ],
   "source": [
    "y_pred = np.roll(y_valid,1, axis=0)\n",
    "print('baseline dev_F1=',F1(y_valid[1:], y_pred[1:]))\n",
    "y_pred = np.roll(y_test,1, axis=0)\n",
    "y_pred[0] = y_valid[-1] # be careful here\n",
    "print('baseline test_F1=',F1(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_in_epoch = 0;\n",
    "perm_array  = np.arange(x_train.shape[0])\n",
    "np.random.shuffle(perm_array)\n",
    "\n",
    "# function to get the next batch\n",
    "def get_next_batch(batch_size):\n",
    "    global index_in_epoch, x_train, perm_array   \n",
    "    start = index_in_epoch\n",
    "    index_in_epoch += batch_size\n",
    "    \n",
    "    if index_in_epoch > x_train.shape[0]:\n",
    "        np.random.shuffle(perm_array) # shuffle permutation array\n",
    "        start = 0 # start next epoch\n",
    "        index_in_epoch = batch_size\n",
    "        \n",
    "    end = index_in_epoch\n",
    "    return x_train[perm_array[start:end]], y_train[perm_array[start:end]]\n",
    "\n",
    "x_1000_train, y_1000_train = get_next_batch(1000) # special batch of 1000 records in training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM_Model(object):\n",
    "    def __init__(self, seq_len):\n",
    "        # parameters\n",
    "        self.n_steps = seq_len-1 \n",
    "        self.n_inputs = x_train.shape[-1]\n",
    "        self.n_neurons = 100  # cell.state_size\n",
    "        self.n_bins = 3 # be careful if you want to change this\n",
    "        self.n_layers = 2\n",
    "        self.batch_size = 100\n",
    "        self.n_epochs = 0 # 0 means to train indefinitely\n",
    "        self.train_set_size = x_train.shape[0]\n",
    "        self.test_set_size = x_test.shape[0]\n",
    "        self.keep_prob = tf.placeholder(tf.float32, [])\n",
    "        self.max_gradient_norm = 5\n",
    "        with tf.variable_scope(\"LSTM_Model\", initializer=tf.contrib.layers.xavier_initializer()):\n",
    "            self.X = tf.placeholder(tf.float32, [None, self.n_steps, self.n_inputs])\n",
    "            self.y = tf.placeholder(tf.float32, [None, self.n_bins])\n",
    "\n",
    "            layers = [tf.contrib.rnn.LSTMCell(num_units=self.n_neurons, \\\n",
    "                                              initializer=tf.contrib.layers.xavier_initializer(), \\\n",
    "                                              activation=tf.nn.elu)\n",
    "                     for layer in range(self.n_layers)]\n",
    "            \n",
    "#             layers = [tf.contrib.rnn.BasicRNNCell(num_units=self.n_neurons, \\\n",
    "#                                               activation=tf.nn.elu)\n",
    "#                      for layer in range(self.n_layers)]\n",
    "            \n",
    "            multi_layer_cell = tf.contrib.rnn.MultiRNNCell(layers)\n",
    "\n",
    "            outputs, states = tf.nn.dynamic_rnn(multi_layer_cell, self.X, dtype=tf.float32)\n",
    "            outputs = tf.nn.dropout(outputs, self.keep_prob) # dropout\n",
    "            # 'outputs' is a tensor of shape [batch_size, n_steps, n_neurons(cell.state_size)]\n",
    "        \n",
    "#             directly output\n",
    "#             logits = outputs[:,self.n_steps-1,:] # keep only last output of sequence\n",
    "            \n",
    "            # ======== attn layer ===================\n",
    "#             sim_mat = tf.matmul(outputs, tf.transpose(outputs, perm=[0,2,1]))\n",
    "#             attn_dist = tf.nn.softmax(sim_mat, 2)\n",
    "#             # (batchsize, n_steps, n_steps)\n",
    "#             attn_outputs = tf.matmul(attn_dist, outputs)\n",
    "#             # (batchsize, n_steps, n_bins)\n",
    "            # ========================================\n",
    "            \n",
    "            stacked_outputs = tf.reshape(outputs, [-1, self.n_neurons]) \n",
    "            stacked_outputs = tf.layers.dense(stacked_outputs, self.n_bins)\n",
    "            final_outputs = tf.reshape(stacked_outputs, [-1, self.n_steps, self.n_bins])\n",
    "            \n",
    "            self.final_logits = final_outputs[:, -1, :] # last timestep\n",
    "            # (batchsize, n_bins), this is logits, not probs!!\n",
    "            self.final_logits = tf.nn.dropout(self.final_logits, self.keep_prob) # dropout\n",
    "            \n",
    "            self.indices = tf.argmax(self.final_logits, axis=-1) # (batchsize, 1)\n",
    "            self.preds = tf.one_hot(self.indices, depth=self.n_bins)\n",
    "            \n",
    "            self.each_loss = tf.nn.softmax_cross_entropy_with_logits(logits=self.final_logits, labels=self.y)\n",
    "            self.loss = tf.reduce_mean(self.each_loss) \n",
    "\n",
    "            params = tf.trainable_variables()\n",
    "            gradients = tf.gradients(self.loss, params)\n",
    "            self.gradient_norm = tf.global_norm(gradients)\n",
    "            clipped_gradients, _ = tf.clip_by_global_norm(gradients, self.max_gradient_norm)\n",
    "            clipped_norm = tf.global_norm(clipped_gradients)\n",
    "            self.param_norm = tf.global_norm(params)\n",
    "            self.learning_rate_placeholder = tf.placeholder(tf.float32, [], name='learning_rate')\n",
    "            optimizer = tf.train.RMSPropOptimizer(learning_rate=self.learning_rate_placeholder) \n",
    "            # training_op = optimizer.minimize(loss)\n",
    "            self.training_op = optimizer.apply_gradients(zip(clipped_gradients, params))\n",
    "\n",
    "            # initialize parameters\n",
    "#             sess = tf.Session()\n",
    "            self.global_step = tf.Variable(0, name='global_step', trainable=False)\n",
    "            self.saver = tf.train.Saver(max_to_keep=2)\n",
    "            self.bestmodel_saver = tf.train.Saver(max_to_keep=2)\n",
    "    \n",
    "    def train(self, session, experiment_name, keep_prob_val):\n",
    "        \n",
    "        bestmodel_dir = experiment_name+'/best_ckpt'\n",
    "        bestmodel_ckpt_path = bestmodel_dir+'/best.ckpt'\n",
    "        best_valid_f1 = None\n",
    "        # Make bestmodel dir if necessary\n",
    "        if not os.path.exists(bestmodel_dir):\n",
    "            os.makedirs(bestmodel_dir)\n",
    "        \n",
    "        ckpt = tf.train.get_checkpoint_state(experiment_name)\n",
    "        v2_path = ckpt.model_checkpoint_path + \".index\" if ckpt else \"\"\n",
    "        if ckpt and (tf.gfile.Exists(ckpt.model_checkpoint_path) or tf.gfile.Exists(v2_path)):\n",
    "            self.saver.restore(sess, ckpt.model_checkpoint_path)\n",
    "            iteration = self.global_step.eval(session=sess) # get last global_step\n",
    "            print(\"Start from iteration:\", iteration)\n",
    "            lr = 1e-3\n",
    "        else:\n",
    "            print('There is not saved parameters. Creating model with fresh parameters.')\n",
    "            sess.run(tf.global_variables_initializer())\n",
    "            iteration = 0\n",
    "            lr = 1e-2 # should fix this...\n",
    "\n",
    "        old_loss = 1000\n",
    "        \n",
    "        while self.n_epochs == 0 or iteration*self.batch_size/self.train_set_size < self.n_epochs:\n",
    "            iteration = iteration + 1\n",
    "            x_batch, y_batch = get_next_batch(self.batch_size) # fetch the next training batch \n",
    "\n",
    "            # train on this batch\n",
    "            sess.run(self.training_op, feed_dict={self.X: x_batch, self.y: y_batch, self.learning_rate_placeholder:lr, \\\n",
    "                                                 self.keep_prob: keep_prob_val})\n",
    "\n",
    "            if iteration % 50 == 0:\n",
    "                y_1000_train_pred, loss_val, param_norm_val, grad_norm_val = \\\n",
    "                    sess.run([self.preds, self.loss, self.param_norm, self.gradient_norm],\\\n",
    "                            feed_dict={self.X: x_1000_train, self.y:y_1000_train, \\\n",
    "                                        self.learning_rate_placeholder:lr, self.keep_prob: keep_prob_val})\n",
    "                    \n",
    "                if loss_val > old_loss * 1.2:\n",
    "                    lr /= 2\n",
    "                old_loss = loss_val\n",
    "\n",
    "                y_valid_pred = sess.run(self.preds, feed_dict={self.X: x_valid, self.keep_prob: keep_prob_val})\n",
    "                \n",
    "                valid_f1 = F1(y_valid, y_valid_pred)\n",
    "                print('%.2f epochs, iter %d: train_loss = %.9f, param_norm = %.3f, grad_norm = %.3f, train_F1/valid_F1 = %.6f/%.6f' \\\n",
    "                      %(iteration*self.batch_size/self.train_set_size, iteration, loss_val, param_norm_val, grad_norm_val, \\\n",
    "                        F1(y_1000_train, y_1000_train_pred), \\\n",
    "                        valid_f1))\n",
    "\n",
    "                if best_valid_f1 is None or valid_f1 > best_valid_f1:\n",
    "                    best_valid_f1 = valid_f1\n",
    "                    print(\"======New best valid F1. Saving to %s...\" % bestmodel_ckpt_path)\n",
    "                    self.bestmodel_saver.save(sess, bestmodel_ckpt_path, global_step=self.global_step)\n",
    "                \n",
    "            if iteration % 100 == 0:\n",
    "                self.global_step.assign(iteration).eval(session=sess) # set and update(eval) global_step with index, i\n",
    "                save_path = self.saver.save(sess, \"./\"+experiment_name+\"/model.ckpt\", global_step=self.global_step)\n",
    "                print('Saved parameters to %s' % save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## run experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_name=\"all_features_with_sentiment_10_seq_100hidden_2layer_keep08\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "lstm_model = LSTM_Model(seq_len=seq_len)\n",
    "sess = tf.Session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There is not saved parameters. Creating model with fresh parameters.\n",
      "0.75 epochs, iter 50: train_loss = 1.098032475, param_norm = 22.490, grad_norm = 0.206, train_F1/valid_F1 = 0.366000/0.212425\n",
      "======New best valid F1. Saving to all_features_with_sentiment_10_seq_100hidden_2layer_keep08/best_ckpt/best.ckpt...\n",
      "1.49 epochs, iter 100: train_loss = 1.099740028, param_norm = 22.471, grad_norm = 0.187, train_F1/valid_F1 = 0.340000/0.434870\n",
      "======New best valid F1. Saving to all_features_with_sentiment_10_seq_100hidden_2layer_keep08/best_ckpt/best.ckpt...\n",
      "Saved parameters to ./all_features_with_sentiment_10_seq_100hidden_2layer_keep08/model.ckpt-100\n",
      "2.24 epochs, iter 150: train_loss = 1.092554450, param_norm = 22.824, grad_norm = 0.049, train_F1/valid_F1 = 0.353000/0.198397\n",
      "2.98 epochs, iter 200: train_loss = 1.124582052, param_norm = 25.347, grad_norm = 0.323, train_F1/valid_F1 = 0.352000/0.418838\n",
      "Saved parameters to ./all_features_with_sentiment_10_seq_100hidden_2layer_keep08/model.ckpt-200\n",
      "3.73 epochs, iter 250: train_loss = 1.088899136, param_norm = 29.801, grad_norm = 0.020, train_F1/valid_F1 = 0.381000/0.380762\n",
      "4.47 epochs, iter 300: train_loss = 1.089793801, param_norm = 31.630, grad_norm = 0.070, train_F1/valid_F1 = 0.369000/0.275551\n",
      "Saved parameters to ./all_features_with_sentiment_10_seq_100hidden_2layer_keep08/model.ckpt-300\n",
      "5.22 epochs, iter 350: train_loss = 1.078333735, param_norm = 33.487, grad_norm = 0.087, train_F1/valid_F1 = 0.401000/0.418838\n",
      "5.96 epochs, iter 400: train_loss = 1.082319260, param_norm = 35.884, grad_norm = 0.065, train_F1/valid_F1 = 0.407000/0.405812\n",
      "Saved parameters to ./all_features_with_sentiment_10_seq_100hidden_2layer_keep08/model.ckpt-400\n",
      "6.71 epochs, iter 450: train_loss = 1.076844692, param_norm = 37.324, grad_norm = 0.058, train_F1/valid_F1 = 0.404000/0.418838\n",
      "7.45 epochs, iter 500: train_loss = 1.088097930, param_norm = 40.009, grad_norm = 0.205, train_F1/valid_F1 = 0.390000/0.421844\n",
      "Saved parameters to ./all_features_with_sentiment_10_seq_100hidden_2layer_keep08/model.ckpt-500\n",
      "8.20 epochs, iter 550: train_loss = 1.071992040, param_norm = 41.616, grad_norm = 0.075, train_F1/valid_F1 = 0.401000/0.419840\n",
      "8.94 epochs, iter 600: train_loss = 1.073143125, param_norm = 43.479, grad_norm = 0.067, train_F1/valid_F1 = 0.423000/0.414830\n",
      "Saved parameters to ./all_features_with_sentiment_10_seq_100hidden_2layer_keep08/model.ckpt-600\n",
      "9.69 epochs, iter 650: train_loss = 1.082272649, param_norm = 45.406, grad_norm = 0.128, train_F1/valid_F1 = 0.391000/0.309619\n",
      "10.43 epochs, iter 700: train_loss = 1.063361049, param_norm = 47.216, grad_norm = 0.031, train_F1/valid_F1 = 0.433000/0.393788\n",
      "Saved parameters to ./all_features_with_sentiment_10_seq_100hidden_2layer_keep08/model.ckpt-700\n",
      "11.18 epochs, iter 750: train_loss = 1.068198442, param_norm = 49.437, grad_norm = 0.064, train_F1/valid_F1 = 0.427000/0.315631\n",
      "11.92 epochs, iter 800: train_loss = 1.071700811, param_norm = 51.935, grad_norm = 0.144, train_F1/valid_F1 = 0.412000/0.401804\n",
      "Saved parameters to ./all_features_with_sentiment_10_seq_100hidden_2layer_keep08/model.ckpt-800\n",
      "12.67 epochs, iter 850: train_loss = 1.064892054, param_norm = 53.122, grad_norm = 0.068, train_F1/valid_F1 = 0.437000/0.337675\n",
      "13.41 epochs, iter 900: train_loss = 1.062479258, param_norm = 54.784, grad_norm = 0.075, train_F1/valid_F1 = 0.429000/0.246493\n",
      "Saved parameters to ./all_features_with_sentiment_10_seq_100hidden_2layer_keep08/model.ckpt-900\n",
      "14.16 epochs, iter 950: train_loss = 1.065263391, param_norm = 56.028, grad_norm = 0.035, train_F1/valid_F1 = 0.445000/0.367735\n",
      "14.90 epochs, iter 1000: train_loss = 1.061307430, param_norm = 57.691, grad_norm = 0.065, train_F1/valid_F1 = 0.432000/0.389780\n",
      "Saved parameters to ./all_features_with_sentiment_10_seq_100hidden_2layer_keep08/model.ckpt-1000\n",
      "15.65 epochs, iter 1050: train_loss = 1.061675310, param_norm = 58.912, grad_norm = 0.045, train_F1/valid_F1 = 0.454000/0.272545\n",
      "16.39 epochs, iter 1100: train_loss = 1.057939410, param_norm = 60.621, grad_norm = 0.113, train_F1/valid_F1 = 0.424000/0.248497\n",
      "Saved parameters to ./all_features_with_sentiment_10_seq_100hidden_2layer_keep08/model.ckpt-1100\n",
      "17.14 epochs, iter 1150: train_loss = 1.064401150, param_norm = 61.591, grad_norm = 0.131, train_F1/valid_F1 = 0.438000/0.238477\n",
      "17.88 epochs, iter 1200: train_loss = 1.075339079, param_norm = 63.098, grad_norm = 0.139, train_F1/valid_F1 = 0.432000/0.252505\n",
      "Saved parameters to ./all_features_with_sentiment_10_seq_100hidden_2layer_keep08/model.ckpt-1200\n",
      "18.63 epochs, iter 1250: train_loss = 1.066807747, param_norm = 64.845, grad_norm = 0.102, train_F1/valid_F1 = 0.425000/0.263527\n",
      "19.37 epochs, iter 1300: train_loss = 1.061870813, param_norm = 65.803, grad_norm = 0.029, train_F1/valid_F1 = 0.445000/0.316633\n",
      "Saved parameters to ./all_features_with_sentiment_10_seq_100hidden_2layer_keep08/model.ckpt-1300\n",
      "20.12 epochs, iter 1350: train_loss = 1.063189983, param_norm = 67.193, grad_norm = 0.203, train_F1/valid_F1 = 0.417000/0.235471\n",
      "20.86 epochs, iter 1400: train_loss = 1.071038365, param_norm = 68.295, grad_norm = 0.268, train_F1/valid_F1 = 0.401000/0.368737\n",
      "Saved parameters to ./all_features_with_sentiment_10_seq_100hidden_2layer_keep08/model.ckpt-1400\n",
      "21.61 epochs, iter 1450: train_loss = 1.059175730, param_norm = 69.781, grad_norm = 0.145, train_F1/valid_F1 = 0.436000/0.262525\n",
      "22.35 epochs, iter 1500: train_loss = 9.097933769, param_norm = 71.032, grad_norm = 79.807, train_F1/valid_F1 = 0.433000/0.387776\n",
      "Saved parameters to ./all_features_with_sentiment_10_seq_100hidden_2layer_keep08/model.ckpt-1500\n",
      "23.10 epochs, iter 1550: train_loss = 1.042939186, param_norm = 71.396, grad_norm = 0.029, train_F1/valid_F1 = 0.471000/0.263527\n",
      "23.84 epochs, iter 1600: train_loss = 1.048351288, param_norm = 71.604, grad_norm = 0.042, train_F1/valid_F1 = 0.454000/0.280561\n",
      "Saved parameters to ./all_features_with_sentiment_10_seq_100hidden_2layer_keep08/model.ckpt-1600\n",
      "24.59 epochs, iter 1650: train_loss = 1.046758294, param_norm = 72.298, grad_norm = 0.087, train_F1/valid_F1 = 0.447000/0.262525\n",
      "25.33 epochs, iter 1700: train_loss = 1.042922974, param_norm = 72.678, grad_norm = 0.037, train_F1/valid_F1 = 0.474000/0.265531\n",
      "Saved parameters to ./all_features_with_sentiment_10_seq_100hidden_2layer_keep08/model.ckpt-1700\n",
      "26.08 epochs, iter 1750: train_loss = 1.053761840, param_norm = 72.971, grad_norm = 0.104, train_F1/valid_F1 = 0.446000/0.298597\n",
      "26.82 epochs, iter 1800: train_loss = 1.050023913, param_norm = 73.572, grad_norm = 0.040, train_F1/valid_F1 = 0.466000/0.256513\n",
      "Saved parameters to ./all_features_with_sentiment_10_seq_100hidden_2layer_keep08/model.ckpt-1800\n",
      "27.57 epochs, iter 1850: train_loss = 1.045215368, param_norm = 73.878, grad_norm = 0.179, train_F1/valid_F1 = 0.459000/0.244489\n",
      "28.31 epochs, iter 1900: train_loss = 1.054140091, param_norm = 74.359, grad_norm = 0.137, train_F1/valid_F1 = 0.458000/0.265531\n",
      "Saved parameters to ./all_features_with_sentiment_10_seq_100hidden_2layer_keep08/model.ckpt-1900\n",
      "29.06 epochs, iter 1950: train_loss = 1.067974091, param_norm = 74.726, grad_norm = 0.353, train_F1/valid_F1 = 0.429000/0.252505\n",
      "29.80 epochs, iter 2000: train_loss = 1.040779591, param_norm = 75.389, grad_norm = 0.190, train_F1/valid_F1 = 0.446000/0.282565\n",
      "Saved parameters to ./all_features_with_sentiment_10_seq_100hidden_2layer_keep08/model.ckpt-2000\n",
      "30.55 epochs, iter 2050: train_loss = 1.040972114, param_norm = 75.690, grad_norm = 0.102, train_F1/valid_F1 = 0.448000/0.286573\n",
      "31.29 epochs, iter 2100: train_loss = 1.039383411, param_norm = 76.127, grad_norm = 0.086, train_F1/valid_F1 = 0.474000/0.276553\n",
      "Saved parameters to ./all_features_with_sentiment_10_seq_100hidden_2layer_keep08/model.ckpt-2100\n",
      "32.04 epochs, iter 2150: train_loss = 1.053463340, param_norm = 76.695, grad_norm = 0.192, train_F1/valid_F1 = 0.451000/0.279559\n",
      "32.78 epochs, iter 2200: train_loss = 1.048905015, param_norm = 77.107, grad_norm = 0.208, train_F1/valid_F1 = 0.457000/0.280561\n",
      "Saved parameters to ./all_features_with_sentiment_10_seq_100hidden_2layer_keep08/model.ckpt-2200\n",
      "33.53 epochs, iter 2250: train_loss = 1.041269064, param_norm = 77.506, grad_norm = 0.220, train_F1/valid_F1 = 0.448000/0.259519\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34.27 epochs, iter 2300: train_loss = 1.043236136, param_norm = 77.961, grad_norm = 0.152, train_F1/valid_F1 = 0.462000/0.264529\n",
      "Saved parameters to ./all_features_with_sentiment_10_seq_100hidden_2layer_keep08/model.ckpt-2300\n",
      "35.02 epochs, iter 2350: train_loss = 1.034645319, param_norm = 78.472, grad_norm = 0.104, train_F1/valid_F1 = 0.462000/0.267535\n",
      "35.76 epochs, iter 2400: train_loss = 1.032690644, param_norm = 78.893, grad_norm = 0.076, train_F1/valid_F1 = 0.469000/0.290581\n",
      "Saved parameters to ./all_features_with_sentiment_10_seq_100hidden_2layer_keep08/model.ckpt-2400\n",
      "36.51 epochs, iter 2450: train_loss = 1.034169912, param_norm = 79.405, grad_norm = 0.100, train_F1/valid_F1 = 0.458000/0.308617\n",
      "37.25 epochs, iter 2500: train_loss = 1.039065957, param_norm = 80.125, grad_norm = 0.219, train_F1/valid_F1 = 0.470000/0.302605\n",
      "Saved parameters to ./all_features_with_sentiment_10_seq_100hidden_2layer_keep08/model.ckpt-2500\n",
      "38.00 epochs, iter 2550: train_loss = 1.036708474, param_norm = 80.597, grad_norm = 0.133, train_F1/valid_F1 = 0.455000/0.290581\n",
      "38.74 epochs, iter 2600: train_loss = 1.030190706, param_norm = 80.902, grad_norm = 0.083, train_F1/valid_F1 = 0.463000/0.298597\n",
      "Saved parameters to ./all_features_with_sentiment_10_seq_100hidden_2layer_keep08/model.ckpt-2600\n",
      "39.49 epochs, iter 2650: train_loss = 1.042550325, param_norm = 81.574, grad_norm = 0.323, train_F1/valid_F1 = 0.476000/0.291583\n",
      "40.23 epochs, iter 2700: train_loss = 1.034619212, param_norm = 81.764, grad_norm = 0.081, train_F1/valid_F1 = 0.472000/0.282565\n",
      "Saved parameters to ./all_features_with_sentiment_10_seq_100hidden_2layer_keep08/model.ckpt-2700\n",
      "40.98 epochs, iter 2750: train_loss = 1.024792671, param_norm = 82.412, grad_norm = 0.083, train_F1/valid_F1 = 0.491000/0.296593\n",
      "41.72 epochs, iter 2800: train_loss = 1.032052517, param_norm = 82.604, grad_norm = 0.132, train_F1/valid_F1 = 0.456000/0.286573\n",
      "Saved parameters to ./all_features_with_sentiment_10_seq_100hidden_2layer_keep08/model.ckpt-2800\n",
      "42.47 epochs, iter 2850: train_loss = 1.022591591, param_norm = 83.388, grad_norm = 0.118, train_F1/valid_F1 = 0.470000/0.290581\n",
      "43.21 epochs, iter 2900: train_loss = 1.017943382, param_norm = 83.787, grad_norm = 0.092, train_F1/valid_F1 = 0.459000/0.303607\n",
      "Saved parameters to ./all_features_with_sentiment_10_seq_100hidden_2layer_keep08/model.ckpt-2900\n",
      "43.96 epochs, iter 2950: train_loss = 1.019291401, param_norm = 84.299, grad_norm = 0.075, train_F1/valid_F1 = 0.465000/0.305611\n",
      "44.70 epochs, iter 3000: train_loss = 1.027530313, param_norm = 84.854, grad_norm = 0.108, train_F1/valid_F1 = 0.486000/0.300601\n",
      "Saved parameters to ./all_features_with_sentiment_10_seq_100hidden_2layer_keep08/model.ckpt-3000\n",
      "45.45 epochs, iter 3050: train_loss = 1.023614407, param_norm = 85.614, grad_norm = 0.065, train_F1/valid_F1 = 0.473000/0.305611\n",
      "46.19 epochs, iter 3100: train_loss = 1.028312206, param_norm = 86.471, grad_norm = 0.077, train_F1/valid_F1 = 0.463000/0.277555\n",
      "Saved parameters to ./all_features_with_sentiment_10_seq_100hidden_2layer_keep08/model.ckpt-3100\n",
      "46.94 epochs, iter 3150: train_loss = 1.029074192, param_norm = 86.781, grad_norm = 0.087, train_F1/valid_F1 = 0.465000/0.293587\n",
      "47.68 epochs, iter 3200: train_loss = 1.028340220, param_norm = 86.886, grad_norm = 0.100, train_F1/valid_F1 = 0.475000/0.301603\n",
      "Saved parameters to ./all_features_with_sentiment_10_seq_100hidden_2layer_keep08/model.ckpt-3200\n",
      "48.43 epochs, iter 3250: train_loss = 1.015539765, param_norm = 87.960, grad_norm = 0.108, train_F1/valid_F1 = 0.480000/0.319639\n",
      "49.17 epochs, iter 3300: train_loss = 1.025394082, param_norm = 88.097, grad_norm = 0.192, train_F1/valid_F1 = 0.471000/0.339679\n",
      "Saved parameters to ./all_features_with_sentiment_10_seq_100hidden_2layer_keep08/model.ckpt-3300\n",
      "49.92 epochs, iter 3350: train_loss = 1.025129437, param_norm = 88.740, grad_norm = 0.215, train_F1/valid_F1 = 0.497000/0.301603\n",
      "50.66 epochs, iter 3400: train_loss = 1.047683358, param_norm = 89.368, grad_norm = 2.752, train_F1/valid_F1 = 0.483000/0.335671\n",
      "Saved parameters to ./all_features_with_sentiment_10_seq_100hidden_2layer_keep08/model.ckpt-3400\n",
      "51.41 epochs, iter 3450: train_loss = 1.004750013, param_norm = 90.168, grad_norm = 0.082, train_F1/valid_F1 = 0.489000/0.327655\n",
      "52.15 epochs, iter 3500: train_loss = 1.008705735, param_norm = 90.376, grad_norm = 0.052, train_F1/valid_F1 = 0.480000/0.314629\n",
      "Saved parameters to ./all_features_with_sentiment_10_seq_100hidden_2layer_keep08/model.ckpt-3500\n",
      "52.90 epochs, iter 3550: train_loss = 1.008906603, param_norm = 90.891, grad_norm = 0.173, train_F1/valid_F1 = 0.489000/0.310621\n",
      "53.64 epochs, iter 3600: train_loss = 1.010835052, param_norm = 91.345, grad_norm = 0.094, train_F1/valid_F1 = 0.486000/0.299599\n",
      "Saved parameters to ./all_features_with_sentiment_10_seq_100hidden_2layer_keep08/model.ckpt-3600\n",
      "54.39 epochs, iter 3650: train_loss = 1.014734030, param_norm = 91.968, grad_norm = 0.121, train_F1/valid_F1 = 0.479000/0.333667\n",
      "55.13 epochs, iter 3700: train_loss = 1.011242032, param_norm = 92.817, grad_norm = 0.083, train_F1/valid_F1 = 0.492000/0.300601\n",
      "Saved parameters to ./all_features_with_sentiment_10_seq_100hidden_2layer_keep08/model.ckpt-3700\n",
      "55.88 epochs, iter 3750: train_loss = 1.016990066, param_norm = 93.167, grad_norm = 0.169, train_F1/valid_F1 = 0.468000/0.305611\n",
      "56.62 epochs, iter 3800: train_loss = 1.034377813, param_norm = 93.568, grad_norm = 0.217, train_F1/valid_F1 = 0.494000/0.304609\n",
      "Saved parameters to ./all_features_with_sentiment_10_seq_100hidden_2layer_keep08/model.ckpt-3800\n",
      "57.37 epochs, iter 3850: train_loss = 1.016129732, param_norm = 94.162, grad_norm = 0.140, train_F1/valid_F1 = 0.483000/0.318637\n",
      "58.11 epochs, iter 3900: train_loss = 1.016069889, param_norm = 95.028, grad_norm = 0.227, train_F1/valid_F1 = 0.479000/0.337675\n",
      "Saved parameters to ./all_features_with_sentiment_10_seq_100hidden_2layer_keep08/model.ckpt-3900\n",
      "58.86 epochs, iter 3950: train_loss = 0.993832409, param_norm = 95.340, grad_norm = 0.140, train_F1/valid_F1 = 0.482000/0.326653\n",
      "59.60 epochs, iter 4000: train_loss = 1.003268957, param_norm = 95.691, grad_norm = 0.125, train_F1/valid_F1 = 0.490000/0.311623\n",
      "Saved parameters to ./all_features_with_sentiment_10_seq_100hidden_2layer_keep08/model.ckpt-4000\n",
      "60.35 epochs, iter 4050: train_loss = 1.011541486, param_norm = 96.523, grad_norm = 0.201, train_F1/valid_F1 = 0.486000/0.323647\n",
      "61.09 epochs, iter 4100: train_loss = 1.000162125, param_norm = 96.832, grad_norm = 0.522, train_F1/valid_F1 = 0.498000/0.314629\n",
      "Saved parameters to ./all_features_with_sentiment_10_seq_100hidden_2layer_keep08/model.ckpt-4100\n",
      "61.84 epochs, iter 4150: train_loss = 0.997382104, param_norm = 97.388, grad_norm = 0.134, train_F1/valid_F1 = 0.495000/0.288577\n",
      "62.58 epochs, iter 4200: train_loss = 1.003644943, param_norm = 98.249, grad_norm = 0.124, train_F1/valid_F1 = 0.485000/0.339679\n",
      "Saved parameters to ./all_features_with_sentiment_10_seq_100hidden_2layer_keep08/model.ckpt-4200\n",
      "63.33 epochs, iter 4250: train_loss = 1.002717137, param_norm = 98.653, grad_norm = 0.257, train_F1/valid_F1 = 0.491000/0.322645\n",
      "64.07 epochs, iter 4300: train_loss = 1.007885218, param_norm = 99.167, grad_norm = 0.196, train_F1/valid_F1 = 0.474000/0.320641\n",
      "Saved parameters to ./all_features_with_sentiment_10_seq_100hidden_2layer_keep08/model.ckpt-4300\n",
      "64.82 epochs, iter 4350: train_loss = 1.012300730, param_norm = 99.481, grad_norm = 0.175, train_F1/valid_F1 = 0.483000/0.309619\n",
      "65.56 epochs, iter 4400: train_loss = 1.012291551, param_norm = 100.263, grad_norm = 0.112, train_F1/valid_F1 = 0.479000/0.294589\n",
      "Saved parameters to ./all_features_with_sentiment_10_seq_100hidden_2layer_keep08/model.ckpt-4400\n",
      "66.31 epochs, iter 4450: train_loss = 0.998097539, param_norm = 100.747, grad_norm = 0.097, train_F1/valid_F1 = 0.494000/0.324649\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-117-172febd6b675>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlstm_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexperiment_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mexperiment_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeep_prob_val\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.8\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;31m# IMPORTANT:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# when you think F1 is not going to improve anymore, wait another 10 epochs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# if you see any better iteration that has not appeared before, keep waiting.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-113-50afe92500c1>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, session, experiment_name, keep_prob_val)\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m             \u001b[0;31m# train on this batch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m             \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mx_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0my_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearning_rate_placeholder\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlr\u001b[0m\u001b[0;34m,\u001b[0m                                                  \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeep_prob\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mkeep_prob_val\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0miteration\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m50\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    903\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    904\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 905\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    906\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    907\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1138\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1139\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1140\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1141\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1142\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1319\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1320\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1321\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1322\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1323\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1325\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1326\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1327\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1328\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1329\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1310\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1311\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1312\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1313\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1314\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1418\u001b[0m         return tf_session.TF_Run(\n\u001b[1;32m   1419\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1420\u001b[0;31m             status, run_metadata)\n\u001b[0m\u001b[1;32m   1421\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1422\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "lstm_model.train(session=sess, experiment_name=experiment_name, keep_prob_val=0.8)\n",
    "# IMPORTANT:\n",
    "# when you think F1 is not going to improve anymore, wait another 10 epochs. \n",
    "# if you see any better iteration that has not appeared before, keep waiting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from all_features_with_sentiment_10_seq_100hidden_2layer_keep08/best_ckpt/best.ckpt-0\n",
      "train F1: 0.36\n",
      "dev F1: 0.40080160320641284\n",
      "test F1: 0.3712424849699399\n"
     ]
    }
   ],
   "source": [
    "# load best checkpoint (based on dev f1) and evaluate\n",
    "ckpt = tf.train.get_checkpoint_state(experiment_name+'/best_ckpt')\n",
    "v2_path = ckpt.model_checkpoint_path + \".index\" if ckpt else \"\"\n",
    "if ckpt and (tf.gfile.Exists(ckpt.model_checkpoint_path) or tf.gfile.Exists(v2_path)):\n",
    "    lstm_model.saver.restore(sess, ckpt.model_checkpoint_path)\n",
    "else:\n",
    "    raise ValueError('What? you dont have a best checkpoint?')\n",
    "\n",
    "y_1000_train_pred = sess.run(lstm_model.preds, feed_dict={lstm_model.X: x_1000_train, lstm_model.keep_prob: 0.8})\n",
    "print(\"train F1:\",F1(y_1000_train, y_1000_train_pred))\n",
    "y_valid_pred = sess.run(lstm_model.preds, feed_dict={lstm_model.X: x_valid, lstm_model.keep_prob: 0.8})\n",
    "print(\"dev F1:\",F1(y_valid, y_valid_pred))\n",
    "y_test_pred = sess.run(lstm_model.preds, feed_dict={lstm_model.X: x_test, lstm_model.keep_prob: 0.8})\n",
    "print(\"test F1:\",F1(y_test, y_test_pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
