{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kexinyu/anaconda3/envs/tensorflow/lib/python3.6/site-packages/statsmodels/compat/pandas.py:56: FutureWarning: The pandas.core.datetools module is deprecated and will be removed in a future version. Please use the pandas.tseries module instead.\n",
      "  from pandas.core import datetools\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>USDT_BTC_high</th>\n",
       "      <th>USDT_BTC_low</th>\n",
       "      <th>USDT_BTC_close</th>\n",
       "      <th>USDT_BTC_open</th>\n",
       "      <th>USDT_BTC_volume</th>\n",
       "      <th>USDT_BTC_quoteVolume</th>\n",
       "      <th>USDT_BTC_weighted_mean</th>\n",
       "      <th>USDT_BTC_pctChange</th>\n",
       "      <th>USDT_ETH_high</th>\n",
       "      <th>USDT_ETH_low</th>\n",
       "      <th>...</th>\n",
       "      <th>BTC_LTC_weighted_mean</th>\n",
       "      <th>BTC_LTC_pctChange</th>\n",
       "      <th>BTC_XRP_high</th>\n",
       "      <th>BTC_XRP_low</th>\n",
       "      <th>BTC_XRP_close</th>\n",
       "      <th>BTC_XRP_open</th>\n",
       "      <th>BTC_XRP_volume</th>\n",
       "      <th>BTC_XRP_quoteVolume</th>\n",
       "      <th>BTC_XRP_weighted_mean</th>\n",
       "      <th>BTC_XRP_pctChange</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>time</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2016-01-02 12:00:00</th>\n",
       "      <td>432.5000</td>\n",
       "      <td>432.50</td>\n",
       "      <td>432.500000</td>\n",
       "      <td>432.50000</td>\n",
       "      <td>40.041239</td>\n",
       "      <td>0.092581</td>\n",
       "      <td>432.500000</td>\n",
       "      <td>2.220446e-16</td>\n",
       "      <td>0.959136</td>\n",
       "      <td>0.959136</td>\n",
       "      <td>...</td>\n",
       "      <td>0.008063</td>\n",
       "      <td>-0.002293</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.033605</td>\n",
       "      <td>2408.822942</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>-0.002859</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-01-02 13:00:00</th>\n",
       "      <td>432.5000</td>\n",
       "      <td>432.50</td>\n",
       "      <td>432.500000</td>\n",
       "      <td>432.50000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>432.986941</td>\n",
       "      <td>1.125876e-03</td>\n",
       "      <td>0.959136</td>\n",
       "      <td>0.959136</td>\n",
       "      <td>...</td>\n",
       "      <td>0.008060</td>\n",
       "      <td>-0.000333</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.004704</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-01-02 14:00:00</th>\n",
       "      <td>437.3635</td>\n",
       "      <td>432.48</td>\n",
       "      <td>433.336667</td>\n",
       "      <td>433.52799</td>\n",
       "      <td>359.269753</td>\n",
       "      <td>0.828819</td>\n",
       "      <td>433.473883</td>\n",
       "      <td>1.124610e-03</td>\n",
       "      <td>0.959136</td>\n",
       "      <td>0.957000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.008073</td>\n",
       "      <td>0.001623</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>1.141981</td>\n",
       "      <td>81071.098773</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.004682</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-01-02 15:00:00</th>\n",
       "      <td>432.4800</td>\n",
       "      <td>432.48</td>\n",
       "      <td>432.480000</td>\n",
       "      <td>432.48000</td>\n",
       "      <td>60.859598</td>\n",
       "      <td>0.140722</td>\n",
       "      <td>432.480000</td>\n",
       "      <td>-2.292832e-03</td>\n",
       "      <td>0.957000</td>\n",
       "      <td>0.957000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.008089</td>\n",
       "      <td>0.002002</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>2.120423</td>\n",
       "      <td>150622.792769</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>-0.000492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-01-02 16:00:00</th>\n",
       "      <td>432.4800</td>\n",
       "      <td>432.48</td>\n",
       "      <td>432.480000</td>\n",
       "      <td>432.48000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>432.350000</td>\n",
       "      <td>-3.005919e-04</td>\n",
       "      <td>0.957000</td>\n",
       "      <td>0.957000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.008079</td>\n",
       "      <td>-0.001224</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.491516</td>\n",
       "      <td>35178.793196</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>-0.007526</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 56 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                     USDT_BTC_high  USDT_BTC_low  USDT_BTC_close  \\\n",
       "time                                                               \n",
       "2016-01-02 12:00:00       432.5000        432.50      432.500000   \n",
       "2016-01-02 13:00:00       432.5000        432.50      432.500000   \n",
       "2016-01-02 14:00:00       437.3635        432.48      433.336667   \n",
       "2016-01-02 15:00:00       432.4800        432.48      432.480000   \n",
       "2016-01-02 16:00:00       432.4800        432.48      432.480000   \n",
       "\n",
       "                     USDT_BTC_open  USDT_BTC_volume  USDT_BTC_quoteVolume  \\\n",
       "time                                                                        \n",
       "2016-01-02 12:00:00      432.50000        40.041239              0.092581   \n",
       "2016-01-02 13:00:00      432.50000         0.000000              0.000000   \n",
       "2016-01-02 14:00:00      433.52799       359.269753              0.828819   \n",
       "2016-01-02 15:00:00      432.48000        60.859598              0.140722   \n",
       "2016-01-02 16:00:00      432.48000         0.000000              0.000000   \n",
       "\n",
       "                     USDT_BTC_weighted_mean  USDT_BTC_pctChange  \\\n",
       "time                                                              \n",
       "2016-01-02 12:00:00              432.500000        2.220446e-16   \n",
       "2016-01-02 13:00:00              432.986941        1.125876e-03   \n",
       "2016-01-02 14:00:00              433.473883        1.124610e-03   \n",
       "2016-01-02 15:00:00              432.480000       -2.292832e-03   \n",
       "2016-01-02 16:00:00              432.350000       -3.005919e-04   \n",
       "\n",
       "                     USDT_ETH_high  USDT_ETH_low        ...          \\\n",
       "time                                                    ...           \n",
       "2016-01-02 12:00:00       0.959136      0.959136        ...           \n",
       "2016-01-02 13:00:00       0.959136      0.959136        ...           \n",
       "2016-01-02 14:00:00       0.959136      0.957000        ...           \n",
       "2016-01-02 15:00:00       0.957000      0.957000        ...           \n",
       "2016-01-02 16:00:00       0.957000      0.957000        ...           \n",
       "\n",
       "                     BTC_LTC_weighted_mean  BTC_LTC_pctChange  BTC_XRP_high  \\\n",
       "time                                                                          \n",
       "2016-01-02 12:00:00               0.008063          -0.002293      0.000014   \n",
       "2016-01-02 13:00:00               0.008060          -0.000333      0.000014   \n",
       "2016-01-02 14:00:00               0.008073           0.001623      0.000014   \n",
       "2016-01-02 15:00:00               0.008089           0.002002      0.000014   \n",
       "2016-01-02 16:00:00               0.008079          -0.001224      0.000014   \n",
       "\n",
       "                     BTC_XRP_low  BTC_XRP_close  BTC_XRP_open  BTC_XRP_volume  \\\n",
       "time                                                                            \n",
       "2016-01-02 12:00:00     0.000014       0.000014      0.000014        0.033605   \n",
       "2016-01-02 13:00:00     0.000014       0.000014      0.000014        0.000000   \n",
       "2016-01-02 14:00:00     0.000014       0.000014      0.000014        1.141981   \n",
       "2016-01-02 15:00:00     0.000014       0.000014      0.000014        2.120423   \n",
       "2016-01-02 16:00:00     0.000014       0.000014      0.000014        0.491516   \n",
       "\n",
       "                     BTC_XRP_quoteVolume  BTC_XRP_weighted_mean  \\\n",
       "time                                                              \n",
       "2016-01-02 12:00:00          2408.822942               0.000014   \n",
       "2016-01-02 13:00:00             0.000000               0.000014   \n",
       "2016-01-02 14:00:00         81071.098773               0.000014   \n",
       "2016-01-02 15:00:00        150622.792769               0.000014   \n",
       "2016-01-02 16:00:00         35178.793196               0.000014   \n",
       "\n",
       "                     BTC_XRP_pctChange  \n",
       "time                                    \n",
       "2016-01-02 12:00:00          -0.002859  \n",
       "2016-01-02 13:00:00           0.004704  \n",
       "2016-01-02 14:00:00           0.004682  \n",
       "2016-01-02 15:00:00          -0.000492  \n",
       "2016-01-02 16:00:00          -0.007526  \n",
       "\n",
       "[5 rows x 56 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import statsmodels.api as sm\n",
    "import random\n",
    "from sklearn import preprocessing\n",
    "import tensorflow as tf\n",
    "import os\n",
    "\n",
    "dir_path = './data/'\n",
    "df = pd.read_pickle(dir_path+'df_hourly_poloniex.pickle')\n",
    "df = df.dropna()\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_pairs = 7 # USDT_BTC, USDT_ETH, USDT_LTC, USDT_XRP, BTC_ETH, BTC_LTC, BTC_XRP\n",
    "n_channels = 8 # high, low, close, open, volume, quoteVolume, weighted mean, pctChange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler_features = preprocessing.MinMaxScaler(feature_range=(0.1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to create train, validation, test data given sequence length\n",
    "def load_data(df, seq_len, test_size=-1):\n",
    "    # prepare one-hot labels\n",
    "    labels = df['USDT_BTC_pctChange'].as_matrix().reshape([-1,1])\n",
    "    labels = np.concatenate([(labels > 3e-3)*1, ((3e-3 > labels)&(labels > -3e-3))*1, (labels < -3e-3)*1],1)\n",
    "    labels = labels[seq_len-1:] # so labels and data has same length\n",
    "    \n",
    "    feature_set = [x for x in range(56)] #[0,1,2,3,4,6,7]\n",
    "    \n",
    "    data_raw = df.as_matrix() # convert to numpy array\n",
    "    # fit scaler\n",
    "    data_raw = scaler_features.fit_transform(data_raw[:, feature_set])\n",
    "    \n",
    "    # reshape\n",
    "    data_raw = data_raw.reshape((-1, n_pairs, n_channels))\n",
    "    \n",
    "    data = []\n",
    "    \n",
    "    # create all possible sequences of length seq_len\n",
    "    for index in range(len(data_raw) - seq_len + 1): \n",
    "        data.append(data_raw[index: index + seq_len, :])\n",
    "    \n",
    "    data = np.array(data)\n",
    "    \n",
    "    if test_size == -1: # split the old way\n",
    "        n_train_valid_pairs = 3\n",
    "        each_train_set_size_pct = 25\n",
    "        each_valid_set_size_pct = 5\n",
    "\n",
    "        each_train_set_size = round(each_train_set_size_pct/100*data.shape[0])\n",
    "        each_valid_set_size = round(each_valid_set_size_pct/100*data.shape[0])\n",
    "\n",
    "        x_train_sets = []\n",
    "        y_train_sets = []\n",
    "        x_valid_sets = []\n",
    "        y_valid_sets = []\n",
    "        used = 0\n",
    "\n",
    "        for i in range(n_train_valid_pairs):\n",
    "            x_train_sets.append(data[used : used + each_train_set_size,:-1,:]) # cannot see last day, which we aim to predict\n",
    "            y_train_sets.append(labels[used : used + each_train_set_size, :])\n",
    "            used += each_train_set_size\n",
    "\n",
    "            x_valid_sets.append(data[used : used + each_valid_set_size,:-1,:])\n",
    "            y_valid_sets.append(labels[used : used + each_valid_set_size, :])\n",
    "            used += each_valid_set_size\n",
    "\n",
    "        x_test = data[used : , :-1, :]\n",
    "        y_test = labels[used : , :]\n",
    "\n",
    "        x_train = np.concatenate(x_train_sets, axis=0)\n",
    "        y_train = np.concatenate(y_train_sets, axis=0)\n",
    "        x_valid = np.concatenate(x_valid_sets, axis=0)\n",
    "        y_valid = np.concatenate(y_valid_sets, axis=0)\n",
    "    \n",
    "    else:\n",
    "        x_test = data[-test_size : , :-1, :]\n",
    "        y_test = labels[-test_size : , :]\n",
    "        \n",
    "        valid_start = data.shape[0] - test_size - int(test_size/2)\n",
    "        x_valid = data[valid_start:-test_size, :-1, :]\n",
    "        y_valid = labels[valid_start:-test_size, :]\n",
    "        \n",
    "        x_train = data[:valid_start, :-1, :]\n",
    "        y_train = labels[:valid_start, :]\n",
    "    \n",
    "    return [x_train, y_train, x_valid, y_valid, x_test, y_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train.shape =  (16981, 9, 7, 8)\n",
      "y_train.shape =  (16981, 3)\n",
      "x_valid.shape =  (998, 9, 7, 8)\n",
      "y_valid.shape =  (998, 3)\n",
      "x_test.shape =  (1996, 9, 7, 8)\n",
      "y_test.shape =  (1996, 3)\n"
     ]
    }
   ],
   "source": [
    "# create train, test data\n",
    "seq_len = 10 # choose sequence length\n",
    "x_train, y_train, x_valid, y_valid, x_test, y_test = load_data(df, seq_len, test_size=1996)\n",
    "# y_train = y_train.reshape([-1,1])\n",
    "# y_valid = y_valid.reshape([-1,1])\n",
    "# y_test = y_test.reshape([-1,1])\n",
    "print('x_train.shape = ',x_train.shape)\n",
    "print('y_train.shape = ', y_train.shape)\n",
    "print('x_valid.shape = ',x_valid.shape)\n",
    "print('y_valid.shape = ', y_valid.shape)\n",
    "print('x_test.shape = ', x_test.shape)\n",
    "print('y_test.shape = ',y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "baseline dev_F1= 0.39618856569709127\n",
      "baseline test_F1= 0.3712424849699399\n"
     ]
    }
   ],
   "source": [
    "y_pred = np.roll(y_valid,1, axis=0)\n",
    "print('baseline dev_F1=',f1_score(y_valid[1:], y_pred[1:], average='micro'))  \n",
    "y_pred = np.roll(y_test,1, axis=0)\n",
    "y_pred[0] = y_valid[-1] # be careful here\n",
    "print('baseline test_F1=',f1_score(y_test, y_pred, average='micro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_in_epoch = 0;\n",
    "perm_array  = np.arange(x_train.shape[0])\n",
    "np.random.shuffle(perm_array)\n",
    "\n",
    "# function to get the next batch\n",
    "def get_next_batch(batch_size):\n",
    "    global index_in_epoch, x_train, perm_array   \n",
    "    start = index_in_epoch\n",
    "    index_in_epoch += batch_size\n",
    "    \n",
    "    if index_in_epoch > x_train.shape[0]:\n",
    "        np.random.shuffle(perm_array) # shuffle permutation array\n",
    "        start = 0 # start next epoch\n",
    "        index_in_epoch = batch_size\n",
    "        \n",
    "    end = index_in_epoch\n",
    "    return x_train[perm_array[start:end]], y_train[perm_array[start:end]]\n",
    "\n",
    "x_1000_train, y_1000_train = get_next_batch(1000) # special batch of 1000 records in training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class vanilla_CNN(object):\n",
    "    def __init__(self, n_pairs, seq_len, n_channels):\n",
    "        # parameters\n",
    "        self.n_steps = seq_len-1\n",
    "        self.n_pairs = n_pairs\n",
    "        self.n_channels = n_channels\n",
    "        self.n_bins = 3\n",
    "        self.keep_prob = tf.placeholder(tf.float32, [])\n",
    "        self.batch_size = 100\n",
    "        self.n_epochs = 0 # 0 means to train indefinitely\n",
    "        self.train_set_size = x_train.shape[0]\n",
    "        self.test_set_size = x_test.shape[0]\n",
    "        self.keep_prob = tf.placeholder(tf.float32, [])\n",
    "        self.max_gradient_norm = 5\n",
    "        \n",
    "        with tf.variable_scope(\"vanilla_CNN\", initializer=tf.contrib.layers.xavier_initializer()):\n",
    "            self.X = tf.placeholder(tf.float32, [None, self.n_steps, self.n_pairs, self.n_channels])\n",
    "            self.y = tf.placeholder(tf.float32, [None, self.n_bins])\n",
    "            \n",
    "            # Convolutional Layer #1\n",
    "            conv1 = tf.layers.conv2d(inputs=self.X,\n",
    "                                     filters=64,\n",
    "                                     kernel_size=[4, 1],\n",
    "                                     padding=\"same\",\n",
    "                                     activation=tf.nn.relu)\n",
    "            \n",
    "            # Pooling Layer #1\n",
    "            pool1 = tf.layers.max_pooling2d(inputs=conv1, pool_size=[2, 1], strides=[2, 1])\n",
    "            \n",
    "            # Convolutional Layer #2\n",
    "            conv2 = tf.layers.conv2d(inputs=pool1,\n",
    "                                     filters=32,\n",
    "                                     kernel_size=[4, 1],\n",
    "                                     padding=\"same\",\n",
    "                                     activation=tf.nn.relu)\n",
    "            \n",
    "            # Pooling Layer #2\n",
    "            pool2 = tf.layers.max_pooling2d(inputs=conv2, pool_size=[2, 2], strides=[2, 2])\n",
    "            \n",
    "            # Convolutional Layer #3\n",
    "            conv3 = tf.layers.conv2d(inputs=pool2,\n",
    "                                     filters=8,\n",
    "                                     kernel_size=[4, 7],\n",
    "                                     padding=\"same\",\n",
    "                                     activation=tf.nn.relu)\n",
    "            # Pooling Layer #3\n",
    "            pool3 = tf.layers.max_pooling2d(inputs=conv3, pool_size=[2, 2], strides=[2, 2])\n",
    "            \n",
    "            pool3_flat = tf.reshape(pool3, [-1, 8])\n",
    "            \n",
    "            fc1 = tf.layers.dense(inputs=pool3_flat, units=512, activation=tf.nn.relu)\n",
    "            \n",
    "            dp1 = tf.nn.dropout(fc1, self.keep_prob)\n",
    "            \n",
    "            fc2 = tf.layers.dense(inputs=dp1, units=256)\n",
    "            \n",
    "            dp2 = tf.nn.dropout(fc2, self.keep_prob)\n",
    "            \n",
    "            fc3 = tf.layers.dense(inputs=dp1, units=self.n_bins)\n",
    "            \n",
    "            self.final_logits = tf.layers.dense(inputs=fc3, units=self.n_bins)\n",
    "            \n",
    "            self.indices = tf.argmax(self.final_logits, axis=-1) # (batchsize, 1)\n",
    "            self.preds = tf.one_hot(self.indices, depth=self.n_bins)\n",
    "            \n",
    "            self.each_loss = tf.nn.softmax_cross_entropy_with_logits(logits=self.final_logits, labels=self.y)\n",
    "            self.loss = tf.reduce_mean(self.each_loss) \n",
    "\n",
    "            params = tf.trainable_variables()\n",
    "            gradients = tf.gradients(self.loss, params)\n",
    "            self.gradient_norm = tf.global_norm(gradients)\n",
    "            clipped_gradients, _ = tf.clip_by_global_norm(gradients, self.max_gradient_norm)\n",
    "            clipped_norm = tf.global_norm(clipped_gradients)\n",
    "            self.param_norm = tf.global_norm(params)\n",
    "            self.learning_rate_placeholder = tf.placeholder(tf.float32, [], name='learning_rate')\n",
    "            optimizer = tf.train.RMSPropOptimizer(learning_rate=self.learning_rate_placeholder) \n",
    "            # training_op = optimizer.minimize(loss)\n",
    "            self.training_op = optimizer.apply_gradients(zip(clipped_gradients, params))\n",
    "\n",
    "            # initialize parameters\n",
    "#             sess = tf.Session()\n",
    "            self.global_step = tf.Variable(0, name='global_step', trainable=False)\n",
    "            self.saver = tf.train.Saver(max_to_keep=2)\n",
    "            self.bestmodel_saver = tf.train.Saver(max_to_keep=2)\n",
    "\n",
    "\n",
    "    def train(self, session, experiment_name, keep_prob_val):\n",
    "        \n",
    "        bestmodel_dir = experiment_name+'/best_ckpt'\n",
    "        bestmodel_ckpt_path = bestmodel_dir+'/best.ckpt'\n",
    "        best_valid_f1 = None\n",
    "        # Make bestmodel dir if necessary\n",
    "        if not os.path.exists(bestmodel_dir):\n",
    "            os.makedirs(bestmodel_dir)\n",
    "        \n",
    "        ckpt = tf.train.get_checkpoint_state(experiment_name)\n",
    "        v2_path = ckpt.model_checkpoint_path + \".index\" if ckpt else \"\"\n",
    "        if ckpt and (tf.gfile.Exists(ckpt.model_checkpoint_path) or tf.gfile.Exists(v2_path)):\n",
    "            self.saver.restore(sess, ckpt.model_checkpoint_path)\n",
    "            iteration = self.global_step.eval(session=sess) # get last global_step\n",
    "            print(\"Start from iteration:\", iteration)\n",
    "            lr = 1e-3\n",
    "        else:\n",
    "            print('There is not saved parameters. Creating model with fresh parameters.')\n",
    "            sess.run(tf.global_variables_initializer())\n",
    "            iteration = 0\n",
    "            lr = 1e-2 # should fix this...\n",
    "\n",
    "        old_loss = 1000\n",
    "        \n",
    "        while self.n_epochs == 0 or iteration*self.batch_size/self.train_set_size < self.n_epochs:\n",
    "            iteration = iteration + 1\n",
    "            x_batch, y_batch = get_next_batch(self.batch_size) # fetch the next training batch \n",
    "\n",
    "            # train on this batch\n",
    "            sess.run(self.training_op, feed_dict={self.X: x_batch, self.y: y_batch, self.learning_rate_placeholder:lr, \\\n",
    "                                                 self.keep_prob: keep_prob_val})\n",
    "\n",
    "            if iteration % 50 == 0:\n",
    "                y_1000_train_pred, loss_val, param_norm_val, grad_norm_val = \\\n",
    "                    sess.run([self.preds, self.loss, self.param_norm, self.gradient_norm],\\\n",
    "                            feed_dict={self.X: x_1000_train, self.y:y_1000_train, \\\n",
    "                                        self.learning_rate_placeholder:lr, self.keep_prob: keep_prob_val})\n",
    "                    \n",
    "                if loss_val > old_loss * 1.2:\n",
    "                    lr /= 2\n",
    "                old_loss = loss_val\n",
    "\n",
    "                y_valid_pred = sess.run(self.preds, feed_dict={self.X: x_valid, self.keep_prob: keep_prob_val})\n",
    "                \n",
    "                valid_f1 = f1_score(y_valid, y_valid_pred, average='micro')\n",
    "                print('%.2f epochs, iter %d: train_loss = %.9f, param_norm = %.3f, grad_norm = %.3f, train_F1/valid_F1 = %.6f/%.6f' \\\n",
    "                      %(iteration*self.batch_size/self.train_set_size, iteration, loss_val, param_norm_val, grad_norm_val, \\\n",
    "                        f1_score(y_1000_train, y_1000_train_pred, average='micro'), \\\n",
    "                        valid_f1))\n",
    "\n",
    "                if best_valid_f1 is None or valid_f1 > best_valid_f1:\n",
    "                    best_valid_f1 = valid_f1\n",
    "                    print(\"======New best valid F1. Saving to %s...\" % bestmodel_ckpt_path)\n",
    "                    self.bestmodel_saver.save(sess, bestmodel_ckpt_path, global_step=self.global_step)\n",
    "                \n",
    "            if iteration % 100 == 0:\n",
    "                self.global_step.assign(iteration).eval(session=sess) # set and update(eval) global_step with index, i\n",
    "                save_path = self.saver.save(sess, \"./\"+experiment_name+\"/model.ckpt\", global_step=self.global_step)\n",
    "                print('Saved parameters to %s' % save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_name=\"vanilla_CNN\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/kexinyu/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/base.py:198: retry (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use the retry module or similar alternatives.\n",
      "WARNING:tensorflow:From <ipython-input-9-b3df50960798>:67: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See tf.nn.softmax_cross_entropy_with_logits_v2.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "cnn_model = vanilla_CNN(n_pairs=n_pairs,seq_len=seq_len,n_channels=n_channels)\n",
    "sess = tf.Session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There is not saved parameters. Creating model with fresh parameters.\n",
      "0.29 epochs, iter 50: train_loss = 1.045646429, param_norm = 20.831, grad_norm = 0.068, train_F1/valid_F1 = 0.484000/0.186373\n",
      "======New best valid F1. Saving to vanilla_CNN/best_ckpt/best.ckpt...\n",
      "0.59 epochs, iter 100: train_loss = 1.045559168, param_norm = 20.834, grad_norm = 0.110, train_F1/valid_F1 = 0.484000/0.194389\n",
      "======New best valid F1. Saving to vanilla_CNN/best_ckpt/best.ckpt...\n",
      "Saved parameters to ./vanilla_CNN/model.ckpt-100\n",
      "0.88 epochs, iter 150: train_loss = 1.047325253, param_norm = 20.811, grad_norm = 0.035, train_F1/valid_F1 = 0.484000/0.186373\n",
      "1.18 epochs, iter 200: train_loss = 1.054908156, param_norm = 20.957, grad_norm = 0.117, train_F1/valid_F1 = 0.484000/0.186373\n",
      "Saved parameters to ./vanilla_CNN/model.ckpt-200\n",
      "1.47 epochs, iter 250: train_loss = 1.030577183, param_norm = 21.124, grad_norm = 0.142, train_F1/valid_F1 = 0.483000/0.342685\n",
      "======New best valid F1. Saving to vanilla_CNN/best_ckpt/best.ckpt...\n",
      "1.77 epochs, iter 300: train_loss = 1.047845483, param_norm = 21.196, grad_norm = 0.280, train_F1/valid_F1 = 0.484000/0.377756\n",
      "======New best valid F1. Saving to vanilla_CNN/best_ckpt/best.ckpt...\n",
      "Saved parameters to ./vanilla_CNN/model.ckpt-300\n",
      "2.06 epochs, iter 350: train_loss = 1.025185227, param_norm = 21.248, grad_norm = 0.095, train_F1/valid_F1 = 0.489000/0.412826\n",
      "======New best valid F1. Saving to vanilla_CNN/best_ckpt/best.ckpt...\n",
      "2.36 epochs, iter 400: train_loss = 1.030449033, param_norm = 21.327, grad_norm = 0.229, train_F1/valid_F1 = 0.492000/0.396794\n",
      "Saved parameters to ./vanilla_CNN/model.ckpt-400\n",
      "2.65 epochs, iter 450: train_loss = 1.022644520, param_norm = 21.364, grad_norm = 0.117, train_F1/valid_F1 = 0.486000/0.339679\n",
      "2.94 epochs, iter 500: train_loss = 1.015034199, param_norm = 21.407, grad_norm = 0.098, train_F1/valid_F1 = 0.514000/0.407816\n",
      "Saved parameters to ./vanilla_CNN/model.ckpt-500\n",
      "3.24 epochs, iter 550: train_loss = 1.049146295, param_norm = 21.444, grad_norm = 0.386, train_F1/valid_F1 = 0.492000/0.418838\n",
      "======New best valid F1. Saving to vanilla_CNN/best_ckpt/best.ckpt...\n",
      "3.53 epochs, iter 600: train_loss = 1.024212837, param_norm = 21.474, grad_norm = 0.225, train_F1/valid_F1 = 0.494000/0.418838\n",
      "Saved parameters to ./vanilla_CNN/model.ckpt-600\n",
      "3.83 epochs, iter 650: train_loss = 1.030365705, param_norm = 21.483, grad_norm = 0.183, train_F1/valid_F1 = 0.491000/0.386774\n",
      "4.12 epochs, iter 700: train_loss = 1.018618822, param_norm = 21.563, grad_norm = 0.067, train_F1/valid_F1 = 0.489000/0.367735\n",
      "Saved parameters to ./vanilla_CNN/model.ckpt-700\n",
      "4.42 epochs, iter 750: train_loss = 1.016305208, param_norm = 21.628, grad_norm = 0.153, train_F1/valid_F1 = 0.509000/0.420842\n",
      "======New best valid F1. Saving to vanilla_CNN/best_ckpt/best.ckpt...\n",
      "4.71 epochs, iter 800: train_loss = 1.012094617, param_norm = 21.662, grad_norm = 0.082, train_F1/valid_F1 = 0.506000/0.418838\n",
      "Saved parameters to ./vanilla_CNN/model.ckpt-800\n",
      "5.01 epochs, iter 850: train_loss = 1.023293495, param_norm = 21.705, grad_norm = 0.182, train_F1/valid_F1 = 0.511000/0.419840\n",
      "5.30 epochs, iter 900: train_loss = 1.020834088, param_norm = 21.759, grad_norm = 0.199, train_F1/valid_F1 = 0.506000/0.413828\n",
      "Saved parameters to ./vanilla_CNN/model.ckpt-900\n",
      "5.59 epochs, iter 950: train_loss = 1.019030333, param_norm = 21.818, grad_norm = 0.264, train_F1/valid_F1 = 0.503000/0.411824\n",
      "5.89 epochs, iter 1000: train_loss = 1.006360650, param_norm = 21.892, grad_norm = 0.077, train_F1/valid_F1 = 0.501000/0.408818\n",
      "Saved parameters to ./vanilla_CNN/model.ckpt-1000\n",
      "6.18 epochs, iter 1050: train_loss = 1.011424184, param_norm = 21.961, grad_norm = 0.163, train_F1/valid_F1 = 0.506000/0.415832\n",
      "6.48 epochs, iter 1100: train_loss = 1.005989909, param_norm = 22.007, grad_norm = 0.195, train_F1/valid_F1 = 0.521000/0.419840\n",
      "Saved parameters to ./vanilla_CNN/model.ckpt-1100\n",
      "6.77 epochs, iter 1150: train_loss = 1.001317143, param_norm = 22.074, grad_norm = 0.098, train_F1/valid_F1 = 0.505000/0.408818\n",
      "7.07 epochs, iter 1200: train_loss = 0.994973660, param_norm = 22.183, grad_norm = 0.102, train_F1/valid_F1 = 0.512000/0.419840\n",
      "Saved parameters to ./vanilla_CNN/model.ckpt-1200\n",
      "7.36 epochs, iter 1250: train_loss = 1.000779510, param_norm = 22.217, grad_norm = 0.137, train_F1/valid_F1 = 0.504000/0.391784\n",
      "7.66 epochs, iter 1300: train_loss = 0.999051154, param_norm = 22.296, grad_norm = 0.255, train_F1/valid_F1 = 0.514000/0.414830\n",
      "Saved parameters to ./vanilla_CNN/model.ckpt-1300\n",
      "7.95 epochs, iter 1350: train_loss = 0.995218515, param_norm = 22.376, grad_norm = 0.173, train_F1/valid_F1 = 0.509000/0.419840\n",
      "8.24 epochs, iter 1400: train_loss = 1.001426935, param_norm = 22.406, grad_norm = 0.232, train_F1/valid_F1 = 0.498000/0.398798\n",
      "Saved parameters to ./vanilla_CNN/model.ckpt-1400\n",
      "8.54 epochs, iter 1450: train_loss = 0.991573691, param_norm = 22.519, grad_norm = 0.217, train_F1/valid_F1 = 0.518000/0.419840\n",
      "8.83 epochs, iter 1500: train_loss = 1.000887156, param_norm = 22.577, grad_norm = 0.332, train_F1/valid_F1 = 0.521000/0.419840\n",
      "Saved parameters to ./vanilla_CNN/model.ckpt-1500\n",
      "9.13 epochs, iter 1550: train_loss = 0.986346543, param_norm = 22.565, grad_norm = 0.186, train_F1/valid_F1 = 0.517000/0.418838\n",
      "9.42 epochs, iter 1600: train_loss = 0.987361431, param_norm = 22.633, grad_norm = 0.181, train_F1/valid_F1 = 0.522000/0.417836\n",
      "Saved parameters to ./vanilla_CNN/model.ckpt-1600\n",
      "9.72 epochs, iter 1650: train_loss = 1.004896879, param_norm = 22.744, grad_norm = 0.322, train_F1/valid_F1 = 0.519000/0.403808\n",
      "10.01 epochs, iter 1700: train_loss = 1.005727410, param_norm = 22.794, grad_norm = 0.271, train_F1/valid_F1 = 0.520000/0.425852\n",
      "======New best valid F1. Saving to vanilla_CNN/best_ckpt/best.ckpt...\n",
      "Saved parameters to ./vanilla_CNN/model.ckpt-1700\n",
      "10.31 epochs, iter 1750: train_loss = 0.981789351, param_norm = 22.880, grad_norm = 0.141, train_F1/valid_F1 = 0.520000/0.416834\n",
      "10.60 epochs, iter 1800: train_loss = 1.011007547, param_norm = 22.895, grad_norm = 0.379, train_F1/valid_F1 = 0.508000/0.389780\n",
      "Saved parameters to ./vanilla_CNN/model.ckpt-1800\n",
      "10.89 epochs, iter 1850: train_loss = 0.989324987, param_norm = 22.993, grad_norm = 0.124, train_F1/valid_F1 = 0.518000/0.399800\n",
      "11.19 epochs, iter 1900: train_loss = 0.986087561, param_norm = 23.085, grad_norm = 0.167, train_F1/valid_F1 = 0.518000/0.402806\n",
      "Saved parameters to ./vanilla_CNN/model.ckpt-1900\n",
      "11.48 epochs, iter 1950: train_loss = 0.993757546, param_norm = 23.142, grad_norm = 0.292, train_F1/valid_F1 = 0.513000/0.407816\n",
      "11.78 epochs, iter 2000: train_loss = 1.011795640, param_norm = 23.168, grad_norm = 0.643, train_F1/valid_F1 = 0.510000/0.350701\n",
      "Saved parameters to ./vanilla_CNN/model.ckpt-2000\n",
      "12.07 epochs, iter 2050: train_loss = 0.986062407, param_norm = 23.276, grad_norm = 0.239, train_F1/valid_F1 = 0.533000/0.410822\n",
      "12.37 epochs, iter 2100: train_loss = 0.987279892, param_norm = 23.307, grad_norm = 0.176, train_F1/valid_F1 = 0.508000/0.375752\n",
      "Saved parameters to ./vanilla_CNN/model.ckpt-2100\n",
      "12.66 epochs, iter 2150: train_loss = 0.985445917, param_norm = 23.365, grad_norm = 0.197, train_F1/valid_F1 = 0.509000/0.411824\n",
      "12.96 epochs, iter 2200: train_loss = 0.989095867, param_norm = 23.416, grad_norm = 0.286, train_F1/valid_F1 = 0.524000/0.410822\n",
      "Saved parameters to ./vanilla_CNN/model.ckpt-2200\n",
      "13.25 epochs, iter 2250: train_loss = 0.993013561, param_norm = 23.488, grad_norm = 0.272, train_F1/valid_F1 = 0.524000/0.400802\n",
      "13.54 epochs, iter 2300: train_loss = 1.004233599, param_norm = 23.579, grad_norm = 0.403, train_F1/valid_F1 = 0.506000/0.410822\n",
      "Saved parameters to ./vanilla_CNN/model.ckpt-2300\n",
      "13.84 epochs, iter 2350: train_loss = 0.980786979, param_norm = 23.603, grad_norm = 0.082, train_F1/valid_F1 = 0.522000/0.390782\n",
      "14.13 epochs, iter 2400: train_loss = 0.997306347, param_norm = 23.662, grad_norm = 0.569, train_F1/valid_F1 = 0.502000/0.402806\n",
      "Saved parameters to ./vanilla_CNN/model.ckpt-2400\n",
      "14.43 epochs, iter 2450: train_loss = 0.983156860, param_norm = 23.692, grad_norm = 0.203, train_F1/valid_F1 = 0.513000/0.331663\n",
      "14.72 epochs, iter 2500: train_loss = 0.980619848, param_norm = 23.813, grad_norm = 0.102, train_F1/valid_F1 = 0.506000/0.364729\n",
      "Saved parameters to ./vanilla_CNN/model.ckpt-2500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15.02 epochs, iter 2550: train_loss = 0.991187394, param_norm = 23.882, grad_norm = 0.346, train_F1/valid_F1 = 0.500000/0.331663\n",
      "15.31 epochs, iter 2600: train_loss = 0.991090178, param_norm = 23.898, grad_norm = 0.337, train_F1/valid_F1 = 0.476000/0.385772\n",
      "Saved parameters to ./vanilla_CNN/model.ckpt-2600\n",
      "15.61 epochs, iter 2650: train_loss = 1.000569105, param_norm = 23.924, grad_norm = 0.371, train_F1/valid_F1 = 0.505000/0.342685\n",
      "15.90 epochs, iter 2700: train_loss = 0.980285406, param_norm = 24.001, grad_norm = 0.104, train_F1/valid_F1 = 0.521000/0.396794\n",
      "Saved parameters to ./vanilla_CNN/model.ckpt-2700\n",
      "16.19 epochs, iter 2750: train_loss = 0.987045169, param_norm = 24.072, grad_norm = 0.408, train_F1/valid_F1 = 0.493000/0.387776\n",
      "16.49 epochs, iter 2800: train_loss = 0.993848324, param_norm = 24.070, grad_norm = 0.304, train_F1/valid_F1 = 0.490000/0.302605\n",
      "Saved parameters to ./vanilla_CNN/model.ckpt-2800\n",
      "16.78 epochs, iter 2850: train_loss = 0.981218159, param_norm = 24.132, grad_norm = 0.179, train_F1/valid_F1 = 0.524000/0.396794\n",
      "17.08 epochs, iter 2900: train_loss = 1.001437306, param_norm = 24.141, grad_norm = 0.578, train_F1/valid_F1 = 0.504000/0.410822\n",
      "Saved parameters to ./vanilla_CNN/model.ckpt-2900\n",
      "17.37 epochs, iter 2950: train_loss = 1.005432606, param_norm = 24.154, grad_norm = 0.410, train_F1/valid_F1 = 0.510000/0.404810\n",
      "17.67 epochs, iter 3000: train_loss = 0.988290668, param_norm = 24.210, grad_norm = 0.177, train_F1/valid_F1 = 0.520000/0.419840\n",
      "Saved parameters to ./vanilla_CNN/model.ckpt-3000\n",
      "17.96 epochs, iter 3050: train_loss = 0.994014084, param_norm = 24.234, grad_norm = 0.161, train_F1/valid_F1 = 0.505000/0.353707\n",
      "18.26 epochs, iter 3100: train_loss = 0.985897601, param_norm = 24.336, grad_norm = 0.232, train_F1/valid_F1 = 0.514000/0.415832\n",
      "Saved parameters to ./vanilla_CNN/model.ckpt-3100\n",
      "18.55 epochs, iter 3150: train_loss = 0.992708564, param_norm = 24.359, grad_norm = 0.250, train_F1/valid_F1 = 0.494000/0.354709\n",
      "18.84 epochs, iter 3200: train_loss = 1.008270741, param_norm = 24.376, grad_norm = 0.358, train_F1/valid_F1 = 0.499000/0.392786\n",
      "Saved parameters to ./vanilla_CNN/model.ckpt-3200\n",
      "19.14 epochs, iter 3250: train_loss = 0.987924099, param_norm = 24.433, grad_norm = 0.333, train_F1/valid_F1 = 0.495000/0.313627\n",
      "19.43 epochs, iter 3300: train_loss = 1.036248922, param_norm = 24.456, grad_norm = 1.046, train_F1/valid_F1 = 0.508000/0.313627\n",
      "Saved parameters to ./vanilla_CNN/model.ckpt-3300\n",
      "19.73 epochs, iter 3350: train_loss = 0.994825602, param_norm = 24.490, grad_norm = 0.445, train_F1/valid_F1 = 0.484000/0.382766\n",
      "20.02 epochs, iter 3400: train_loss = 0.999538064, param_norm = 24.583, grad_norm = 0.405, train_F1/valid_F1 = 0.504000/0.282565\n",
      "Saved parameters to ./vanilla_CNN/model.ckpt-3400\n",
      "20.32 epochs, iter 3450: train_loss = 0.977088451, param_norm = 24.586, grad_norm = 0.161, train_F1/valid_F1 = 0.525000/0.406814\n",
      "20.61 epochs, iter 3500: train_loss = 0.988849759, param_norm = 24.640, grad_norm = 0.307, train_F1/valid_F1 = 0.503000/0.381764\n",
      "Saved parameters to ./vanilla_CNN/model.ckpt-3500\n",
      "20.91 epochs, iter 3550: train_loss = 0.999408424, param_norm = 24.667, grad_norm = 0.297, train_F1/valid_F1 = 0.498000/0.212425\n",
      "21.20 epochs, iter 3600: train_loss = 0.998381138, param_norm = 24.724, grad_norm = 0.476, train_F1/valid_F1 = 0.496000/0.336673\n",
      "Saved parameters to ./vanilla_CNN/model.ckpt-3600\n",
      "21.49 epochs, iter 3650: train_loss = 0.998931766, param_norm = 24.748, grad_norm = 0.409, train_F1/valid_F1 = 0.515000/0.374749\n",
      "21.79 epochs, iter 3700: train_loss = 0.989609122, param_norm = 24.763, grad_norm = 0.292, train_F1/valid_F1 = 0.515000/0.368737\n",
      "Saved parameters to ./vanilla_CNN/model.ckpt-3700\n",
      "22.08 epochs, iter 3750: train_loss = 0.986036837, param_norm = 24.810, grad_norm = 0.335, train_F1/valid_F1 = 0.510000/0.379760\n",
      "22.38 epochs, iter 3800: train_loss = 0.994168222, param_norm = 24.817, grad_norm = 0.466, train_F1/valid_F1 = 0.529000/0.415832\n",
      "Saved parameters to ./vanilla_CNN/model.ckpt-3800\n",
      "22.67 epochs, iter 3850: train_loss = 0.989666343, param_norm = 24.808, grad_norm = 0.264, train_F1/valid_F1 = 0.508000/0.357715\n",
      "22.97 epochs, iter 3900: train_loss = 0.987148702, param_norm = 24.862, grad_norm = 0.370, train_F1/valid_F1 = 0.495000/0.397796\n",
      "Saved parameters to ./vanilla_CNN/model.ckpt-3900\n",
      "23.26 epochs, iter 3950: train_loss = 0.983995318, param_norm = 24.975, grad_norm = 0.212, train_F1/valid_F1 = 0.520000/0.419840\n",
      "23.56 epochs, iter 4000: train_loss = 0.988135636, param_norm = 25.027, grad_norm = 0.224, train_F1/valid_F1 = 0.515000/0.404810\n",
      "Saved parameters to ./vanilla_CNN/model.ckpt-4000\n",
      "23.85 epochs, iter 4050: train_loss = 0.991063476, param_norm = 25.020, grad_norm = 0.353, train_F1/valid_F1 = 0.509000/0.414830\n",
      "24.14 epochs, iter 4100: train_loss = 0.987652361, param_norm = 25.063, grad_norm = 0.382, train_F1/valid_F1 = 0.514000/0.340681\n",
      "Saved parameters to ./vanilla_CNN/model.ckpt-4100\n",
      "24.44 epochs, iter 4150: train_loss = 0.994958520, param_norm = 25.104, grad_norm = 0.491, train_F1/valid_F1 = 0.517000/0.408818\n",
      "24.73 epochs, iter 4200: train_loss = 0.980951726, param_norm = 25.070, grad_norm = 0.130, train_F1/valid_F1 = 0.519000/0.412826\n",
      "Saved parameters to ./vanilla_CNN/model.ckpt-4200\n",
      "25.03 epochs, iter 4250: train_loss = 0.982186794, param_norm = 25.085, grad_norm = 0.257, train_F1/valid_F1 = 0.513000/0.402806\n",
      "25.32 epochs, iter 4300: train_loss = 1.012393951, param_norm = 25.118, grad_norm = 0.719, train_F1/valid_F1 = 0.511000/0.414830\n",
      "Saved parameters to ./vanilla_CNN/model.ckpt-4300\n",
      "25.62 epochs, iter 4350: train_loss = 1.010650039, param_norm = 25.130, grad_norm = 0.627, train_F1/valid_F1 = 0.501000/0.281563\n",
      "25.91 epochs, iter 4400: train_loss = 0.982940137, param_norm = 25.157, grad_norm = 0.291, train_F1/valid_F1 = 0.518000/0.382766\n",
      "Saved parameters to ./vanilla_CNN/model.ckpt-4400\n",
      "26.21 epochs, iter 4450: train_loss = 0.996593118, param_norm = 25.198, grad_norm = 0.263, train_F1/valid_F1 = 0.502000/0.340681\n",
      "26.50 epochs, iter 4500: train_loss = 0.987224221, param_norm = 25.246, grad_norm = 0.521, train_F1/valid_F1 = 0.495000/0.386774\n",
      "Saved parameters to ./vanilla_CNN/model.ckpt-4500\n",
      "26.79 epochs, iter 4550: train_loss = 1.008320928, param_norm = 25.279, grad_norm = 0.679, train_F1/valid_F1 = 0.518000/0.421844\n",
      "27.09 epochs, iter 4600: train_loss = 0.978332996, param_norm = 25.270, grad_norm = 0.201, train_F1/valid_F1 = 0.510000/0.377756\n",
      "Saved parameters to ./vanilla_CNN/model.ckpt-4600\n",
      "27.38 epochs, iter 4650: train_loss = 0.986752927, param_norm = 25.327, grad_norm = 0.191, train_F1/valid_F1 = 0.505000/0.335671\n",
      "27.68 epochs, iter 4700: train_loss = 1.001883507, param_norm = 25.312, grad_norm = 0.618, train_F1/valid_F1 = 0.505000/0.382766\n",
      "Saved parameters to ./vanilla_CNN/model.ckpt-4700\n",
      "27.97 epochs, iter 4750: train_loss = 0.979882717, param_norm = 25.322, grad_norm = 0.210, train_F1/valid_F1 = 0.519000/0.280561\n",
      "28.27 epochs, iter 4800: train_loss = 0.975964546, param_norm = 25.340, grad_norm = 0.218, train_F1/valid_F1 = 0.520000/0.415832\n",
      "Saved parameters to ./vanilla_CNN/model.ckpt-4800\n",
      "28.56 epochs, iter 4850: train_loss = 0.981624126, param_norm = 25.374, grad_norm = 0.226, train_F1/valid_F1 = 0.514000/0.319639\n",
      "28.86 epochs, iter 4900: train_loss = 0.985060692, param_norm = 25.424, grad_norm = 0.227, train_F1/valid_F1 = 0.512000/0.279559\n",
      "Saved parameters to ./vanilla_CNN/model.ckpt-4900\n",
      "29.15 epochs, iter 4950: train_loss = 0.972831905, param_norm = 25.496, grad_norm = 0.103, train_F1/valid_F1 = 0.512000/0.314629\n",
      "29.44 epochs, iter 5000: train_loss = 0.988195181, param_norm = 25.494, grad_norm = 0.215, train_F1/valid_F1 = 0.524000/0.347695\n",
      "Saved parameters to ./vanilla_CNN/model.ckpt-5000\n",
      "29.74 epochs, iter 5050: train_loss = 1.000408769, param_norm = 25.541, grad_norm = 0.337, train_F1/valid_F1 = 0.515000/0.401804\n",
      "30.03 epochs, iter 5100: train_loss = 0.975663841, param_norm = 25.557, grad_norm = 0.120, train_F1/valid_F1 = 0.513000/0.303607\n",
      "Saved parameters to ./vanilla_CNN/model.ckpt-5100\n",
      "30.33 epochs, iter 5150: train_loss = 0.997576892, param_norm = 25.584, grad_norm = 0.512, train_F1/valid_F1 = 0.518000/0.322645\n",
      "30.62 epochs, iter 5200: train_loss = 0.987511814, param_norm = 25.564, grad_norm = 0.403, train_F1/valid_F1 = 0.515000/0.373747\n",
      "Saved parameters to ./vanilla_CNN/model.ckpt-5200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30.92 epochs, iter 5250: train_loss = 0.992329955, param_norm = 25.550, grad_norm = 0.200, train_F1/valid_F1 = 0.497000/0.212425\n",
      "31.21 epochs, iter 5300: train_loss = 1.017305374, param_norm = 25.605, grad_norm = 0.814, train_F1/valid_F1 = 0.509000/0.230461\n",
      "Saved parameters to ./vanilla_CNN/model.ckpt-5300\n",
      "31.51 epochs, iter 5350: train_loss = 0.981570423, param_norm = 25.588, grad_norm = 0.230, train_F1/valid_F1 = 0.518000/0.263527\n",
      "31.80 epochs, iter 5400: train_loss = 0.977464259, param_norm = 25.654, grad_norm = 0.130, train_F1/valid_F1 = 0.519000/0.269539\n",
      "Saved parameters to ./vanilla_CNN/model.ckpt-5400\n",
      "32.09 epochs, iter 5450: train_loss = 1.010827065, param_norm = 25.644, grad_norm = 0.469, train_F1/valid_F1 = 0.498000/0.367735\n",
      "32.39 epochs, iter 5500: train_loss = 0.987664700, param_norm = 25.697, grad_norm = 0.326, train_F1/valid_F1 = 0.520000/0.314629\n",
      "Saved parameters to ./vanilla_CNN/model.ckpt-5500\n",
      "32.68 epochs, iter 5550: train_loss = 0.981081426, param_norm = 25.731, grad_norm = 0.219, train_F1/valid_F1 = 0.523000/0.268537\n",
      "32.98 epochs, iter 5600: train_loss = 1.011023521, param_norm = 25.783, grad_norm = 0.619, train_F1/valid_F1 = 0.494000/0.194389\n",
      "Saved parameters to ./vanilla_CNN/model.ckpt-5600\n",
      "33.27 epochs, iter 5650: train_loss = 0.986225903, param_norm = 25.766, grad_norm = 0.151, train_F1/valid_F1 = 0.514000/0.214429\n",
      "33.57 epochs, iter 5700: train_loss = 1.018153071, param_norm = 25.840, grad_norm = 0.859, train_F1/valid_F1 = 0.492000/0.357715\n",
      "Saved parameters to ./vanilla_CNN/model.ckpt-5700\n",
      "33.86 epochs, iter 5750: train_loss = 0.992264152, param_norm = 25.871, grad_norm = 0.261, train_F1/valid_F1 = 0.501000/0.336673\n",
      "34.16 epochs, iter 5800: train_loss = 0.974129498, param_norm = 25.881, grad_norm = 0.089, train_F1/valid_F1 = 0.525000/0.364729\n",
      "Saved parameters to ./vanilla_CNN/model.ckpt-5800\n",
      "34.45 epochs, iter 5850: train_loss = 0.979148805, param_norm = 25.915, grad_norm = 0.268, train_F1/valid_F1 = 0.521000/0.355711\n",
      "34.74 epochs, iter 5900: train_loss = 0.995681882, param_norm = 25.923, grad_norm = 0.472, train_F1/valid_F1 = 0.511000/0.365731\n",
      "Saved parameters to ./vanilla_CNN/model.ckpt-5900\n",
      "35.04 epochs, iter 5950: train_loss = 0.979359269, param_norm = 25.950, grad_norm = 0.159, train_F1/valid_F1 = 0.526000/0.403808\n",
      "35.33 epochs, iter 6000: train_loss = 0.987430155, param_norm = 25.982, grad_norm = 0.248, train_F1/valid_F1 = 0.510000/0.372745\n",
      "Saved parameters to ./vanilla_CNN/model.ckpt-6000\n",
      "35.63 epochs, iter 6050: train_loss = 0.975119054, param_norm = 26.029, grad_norm = 0.134, train_F1/valid_F1 = 0.526000/0.411824\n",
      "35.92 epochs, iter 6100: train_loss = 0.992119372, param_norm = 26.069, grad_norm = 0.245, train_F1/valid_F1 = 0.504000/0.292585\n",
      "Saved parameters to ./vanilla_CNN/model.ckpt-6100\n",
      "36.22 epochs, iter 6150: train_loss = 0.981876612, param_norm = 26.076, grad_norm = 0.246, train_F1/valid_F1 = 0.515000/0.347695\n",
      "36.51 epochs, iter 6200: train_loss = 0.980094731, param_norm = 26.133, grad_norm = 0.371, train_F1/valid_F1 = 0.529000/0.391784\n",
      "Saved parameters to ./vanilla_CNN/model.ckpt-6200\n",
      "36.81 epochs, iter 6250: train_loss = 0.976717651, param_norm = 26.121, grad_norm = 0.176, train_F1/valid_F1 = 0.528000/0.417836\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-7ab6967265b9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcnn_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexperiment_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mexperiment_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeep_prob_val\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.6\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;31m# IMPORTANT:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# when you think F1 is not going to improve anymore, wait another 10 epochs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# if you see any better iteration that has not appeared before, keep waiting.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-9-b3df50960798>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, session, experiment_name, keep_prob_val)\u001b[0m\n\u001b[1;32m    124\u001b[0m                 \u001b[0mold_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_val\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 126\u001b[0;31m                 \u001b[0my_valid_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mx_valid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeep_prob\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mkeep_prob_val\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    127\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m                 \u001b[0mvalid_f1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf1_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_valid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_valid_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maverage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'micro'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    903\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    904\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 905\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    906\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    907\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1138\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1139\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1140\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1141\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1142\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1319\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1320\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1321\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1322\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1323\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1325\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1326\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1327\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1328\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1329\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1310\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1311\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1312\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1313\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1314\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1418\u001b[0m         return tf_session.TF_Run(\n\u001b[1;32m   1419\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1420\u001b[0;31m             status, run_metadata)\n\u001b[0m\u001b[1;32m   1421\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1422\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "cnn_model.train(session=sess, experiment_name=experiment_name, keep_prob_val=0.6)\n",
    "# IMPORTANT:\n",
    "# when you think F1 is not going to improve anymore, wait another 10 epochs. \n",
    "# if you see any better iteration that has not appeared before, keep waiting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from vanilla_CNN/best_ckpt/best.ckpt-1600\n",
      "train F1: 0.514\n",
      "dev F1: 0.42084168336673344\n",
      "test F1: 0.38927855711422843\n"
     ]
    }
   ],
   "source": [
    "# load best checkpoint (based on dev f1) and evaluate\n",
    "ckpt = tf.train.get_checkpoint_state(experiment_name+'/best_ckpt')\n",
    "v2_path = ckpt.model_checkpoint_path + \".index\" if ckpt else \"\"\n",
    "if ckpt and (tf.gfile.Exists(ckpt.model_checkpoint_path) or tf.gfile.Exists(v2_path)):\n",
    "    cnn_model.saver.restore(sess, ckpt.model_checkpoint_path)\n",
    "else:\n",
    "    raise ValueError('What? you dont have a best checkpoint?')\n",
    "\n",
    "y_1000_train_pred = sess.run(cnn_model.preds, feed_dict={cnn_model.X: x_1000_train, cnn_model.keep_prob: 0.8})\n",
    "print(\"train F1:\",f1_score(y_1000_train, y_1000_train_pred, average='micro'))\n",
    "y_valid_pred = sess.run(cnn_model.preds, feed_dict={cnn_model.X: x_valid, cnn_model.keep_prob: 0.8})\n",
    "print(\"dev F1:\",f1_score(y_valid, y_valid_pred, average='micro'))\n",
    "y_test_pred = sess.run(cnn_model.preds, feed_dict={cnn_model.X: x_test, cnn_model.keep_prob: 0.8})\n",
    "print(\"test F1:\",f1_score(y_test, y_test_pred, average='micro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([Dimension(100), Dimension(3)])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Code for checking dimension\n",
    "\n",
    "seq_len = 10\n",
    "n_steps = seq_len - 1\n",
    "n_pairs = 7\n",
    "n_channels = 8\n",
    "batch_size = 100\n",
    "\n",
    "x = tf.Variable(tf.random_normal([batch_size, n_steps, n_pairs, n_channels]))\n",
    "\n",
    "conv1 = tf.layers.conv2d(inputs=x,\n",
    "                                     filters=128,\n",
    "                                     kernel_size=[4, 1],\n",
    "                                     padding=\"same\",\n",
    "                                     activation=tf.nn.relu) # [100, 9, 7, 128]\n",
    "\n",
    "pool1 = tf.layers.max_pooling2d(inputs=conv1, pool_size=[2, 1], strides=[2, 1]) # [100, 4, 7, 128]\n",
    "\n",
    "\n",
    "conv2 = tf.layers.conv2d(inputs=pool1,\n",
    "                                     filters=64,\n",
    "                                     kernel_size=[4, 1],\n",
    "                                     padding=\"same\",\n",
    "                                     activation=tf.nn.relu) # [100, 4, 7, 64]\n",
    "\n",
    "pool2 = tf.layers.max_pooling2d(inputs=conv2, pool_size=[2, 2], strides=[2, 2]) # [100, 2, 3, 64]\n",
    "\n",
    "conv3 = tf.layers.conv2d(inputs=pool2,\n",
    "                                     filters=32, \n",
    "                                     kernel_size=[4, 7],\n",
    "                                     padding=\"same\",\n",
    "                                     activation=tf.nn.relu) # [100, 2, 3, 32]\n",
    "\n",
    "pool3 = tf.layers.max_pooling2d(inputs=conv3, pool_size=[2, 2], strides=[2, 2]) # [100, 1, 1, 32]\n",
    "\n",
    "pool3_flat = tf.reshape(pool3, [-1, 32]) # [100,32]\n",
    "\n",
    "fc1 = tf.layers.dense(inputs=pool3_flat, units=512, activation=tf.nn.relu)\n",
    "\n",
    "dropout = tf.nn.dropout(fc1, 0.8)\n",
    "\n",
    "fc2 = tf.layers.dense(inputs=dropout, units=3) # [100,32]\n",
    "\n",
    "fc2.shape "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
