{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>USDT_BTC_high</th>\n",
       "      <th>USDT_BTC_low</th>\n",
       "      <th>USDT_BTC_close</th>\n",
       "      <th>USDT_BTC_open</th>\n",
       "      <th>USDT_BTC_volume</th>\n",
       "      <th>USDT_BTC_quoteVolume</th>\n",
       "      <th>USDT_BTC_weighted_mean</th>\n",
       "      <th>USDT_BTC_pctChange</th>\n",
       "      <th>USDT_ETH_high</th>\n",
       "      <th>USDT_ETH_low</th>\n",
       "      <th>...</th>\n",
       "      <th>BTC_LTC_weighted_mean</th>\n",
       "      <th>BTC_LTC_pctChange</th>\n",
       "      <th>BTC_XRP_high</th>\n",
       "      <th>BTC_XRP_low</th>\n",
       "      <th>BTC_XRP_close</th>\n",
       "      <th>BTC_XRP_open</th>\n",
       "      <th>BTC_XRP_volume</th>\n",
       "      <th>BTC_XRP_quoteVolume</th>\n",
       "      <th>BTC_XRP_weighted_mean</th>\n",
       "      <th>BTC_XRP_pctChange</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>time</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2016-01-02 12:00:00</th>\n",
       "      <td>432.5000</td>\n",
       "      <td>432.50</td>\n",
       "      <td>432.500000</td>\n",
       "      <td>432.50000</td>\n",
       "      <td>40.041239</td>\n",
       "      <td>0.092581</td>\n",
       "      <td>432.500000</td>\n",
       "      <td>2.220446e-16</td>\n",
       "      <td>0.959136</td>\n",
       "      <td>0.959136</td>\n",
       "      <td>...</td>\n",
       "      <td>0.008063</td>\n",
       "      <td>-0.002293</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.033605</td>\n",
       "      <td>2408.822942</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>-0.002859</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-01-02 13:00:00</th>\n",
       "      <td>432.5000</td>\n",
       "      <td>432.50</td>\n",
       "      <td>432.500000</td>\n",
       "      <td>432.50000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>432.986941</td>\n",
       "      <td>1.125876e-03</td>\n",
       "      <td>0.959136</td>\n",
       "      <td>0.959136</td>\n",
       "      <td>...</td>\n",
       "      <td>0.008060</td>\n",
       "      <td>-0.000333</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.004704</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-01-02 14:00:00</th>\n",
       "      <td>437.3635</td>\n",
       "      <td>432.48</td>\n",
       "      <td>433.336667</td>\n",
       "      <td>433.52799</td>\n",
       "      <td>359.269753</td>\n",
       "      <td>0.828819</td>\n",
       "      <td>433.473883</td>\n",
       "      <td>1.124610e-03</td>\n",
       "      <td>0.959136</td>\n",
       "      <td>0.957000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.008073</td>\n",
       "      <td>0.001623</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>1.141981</td>\n",
       "      <td>81071.098773</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.004682</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-01-02 15:00:00</th>\n",
       "      <td>432.4800</td>\n",
       "      <td>432.48</td>\n",
       "      <td>432.480000</td>\n",
       "      <td>432.48000</td>\n",
       "      <td>60.859598</td>\n",
       "      <td>0.140722</td>\n",
       "      <td>432.480000</td>\n",
       "      <td>-2.292832e-03</td>\n",
       "      <td>0.957000</td>\n",
       "      <td>0.957000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.008089</td>\n",
       "      <td>0.002002</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>2.120423</td>\n",
       "      <td>150622.792769</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>-0.000492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-01-02 16:00:00</th>\n",
       "      <td>432.4800</td>\n",
       "      <td>432.48</td>\n",
       "      <td>432.480000</td>\n",
       "      <td>432.48000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>432.350000</td>\n",
       "      <td>-3.005919e-04</td>\n",
       "      <td>0.957000</td>\n",
       "      <td>0.957000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.008079</td>\n",
       "      <td>-0.001224</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.491516</td>\n",
       "      <td>35178.793196</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>-0.007526</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 56 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                     USDT_BTC_high  USDT_BTC_low  USDT_BTC_close  \\\n",
       "time                                                               \n",
       "2016-01-02 12:00:00       432.5000        432.50      432.500000   \n",
       "2016-01-02 13:00:00       432.5000        432.50      432.500000   \n",
       "2016-01-02 14:00:00       437.3635        432.48      433.336667   \n",
       "2016-01-02 15:00:00       432.4800        432.48      432.480000   \n",
       "2016-01-02 16:00:00       432.4800        432.48      432.480000   \n",
       "\n",
       "                     USDT_BTC_open  USDT_BTC_volume  USDT_BTC_quoteVolume  \\\n",
       "time                                                                        \n",
       "2016-01-02 12:00:00      432.50000        40.041239              0.092581   \n",
       "2016-01-02 13:00:00      432.50000         0.000000              0.000000   \n",
       "2016-01-02 14:00:00      433.52799       359.269753              0.828819   \n",
       "2016-01-02 15:00:00      432.48000        60.859598              0.140722   \n",
       "2016-01-02 16:00:00      432.48000         0.000000              0.000000   \n",
       "\n",
       "                     USDT_BTC_weighted_mean  USDT_BTC_pctChange  \\\n",
       "time                                                              \n",
       "2016-01-02 12:00:00              432.500000        2.220446e-16   \n",
       "2016-01-02 13:00:00              432.986941        1.125876e-03   \n",
       "2016-01-02 14:00:00              433.473883        1.124610e-03   \n",
       "2016-01-02 15:00:00              432.480000       -2.292832e-03   \n",
       "2016-01-02 16:00:00              432.350000       -3.005919e-04   \n",
       "\n",
       "                     USDT_ETH_high  USDT_ETH_low        ...          \\\n",
       "time                                                    ...           \n",
       "2016-01-02 12:00:00       0.959136      0.959136        ...           \n",
       "2016-01-02 13:00:00       0.959136      0.959136        ...           \n",
       "2016-01-02 14:00:00       0.959136      0.957000        ...           \n",
       "2016-01-02 15:00:00       0.957000      0.957000        ...           \n",
       "2016-01-02 16:00:00       0.957000      0.957000        ...           \n",
       "\n",
       "                     BTC_LTC_weighted_mean  BTC_LTC_pctChange  BTC_XRP_high  \\\n",
       "time                                                                          \n",
       "2016-01-02 12:00:00               0.008063          -0.002293      0.000014   \n",
       "2016-01-02 13:00:00               0.008060          -0.000333      0.000014   \n",
       "2016-01-02 14:00:00               0.008073           0.001623      0.000014   \n",
       "2016-01-02 15:00:00               0.008089           0.002002      0.000014   \n",
       "2016-01-02 16:00:00               0.008079          -0.001224      0.000014   \n",
       "\n",
       "                     BTC_XRP_low  BTC_XRP_close  BTC_XRP_open  BTC_XRP_volume  \\\n",
       "time                                                                            \n",
       "2016-01-02 12:00:00     0.000014       0.000014      0.000014        0.033605   \n",
       "2016-01-02 13:00:00     0.000014       0.000014      0.000014        0.000000   \n",
       "2016-01-02 14:00:00     0.000014       0.000014      0.000014        1.141981   \n",
       "2016-01-02 15:00:00     0.000014       0.000014      0.000014        2.120423   \n",
       "2016-01-02 16:00:00     0.000014       0.000014      0.000014        0.491516   \n",
       "\n",
       "                     BTC_XRP_quoteVolume  BTC_XRP_weighted_mean  \\\n",
       "time                                                              \n",
       "2016-01-02 12:00:00          2408.822942               0.000014   \n",
       "2016-01-02 13:00:00             0.000000               0.000014   \n",
       "2016-01-02 14:00:00         81071.098773               0.000014   \n",
       "2016-01-02 15:00:00        150622.792769               0.000014   \n",
       "2016-01-02 16:00:00         35178.793196               0.000014   \n",
       "\n",
       "                     BTC_XRP_pctChange  \n",
       "time                                    \n",
       "2016-01-02 12:00:00          -0.002859  \n",
       "2016-01-02 13:00:00           0.004704  \n",
       "2016-01-02 14:00:00           0.004682  \n",
       "2016-01-02 15:00:00          -0.000492  \n",
       "2016-01-02 16:00:00          -0.007526  \n",
       "\n",
       "[5 rows x 56 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "__author__ = \"Yicheng Li\"\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import statsmodels.api as sm\n",
    "import random\n",
    "from sklearn import preprocessing\n",
    "import tensorflow as tf\n",
    "import os\n",
    "\n",
    "dir_path = ''\n",
    "df = pd.read_pickle(dir_path+'df_hourly_poloniex.pickle')\n",
    "df = df.dropna()\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler_features = preprocessing.MinMaxScaler(feature_range=(0.1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to create train, validation, test data given sequence length\n",
    "def load_data(df, seq_len, test_size=-1):\n",
    "    # prepare one-hot labels\n",
    "    labels = df['USDT_BTC_pctChange'].as_matrix().reshape([-1,1])\n",
    "    labels = np.concatenate([(labels > 3e-3)*1, ((3e-3 > labels)&(labels > -3e-3))*1, (labels < -3e-3)*1],1)\n",
    "    labels = labels[seq_len-1:] # so labels and data has same length\n",
    "    \n",
    "    feature_set = [x for x in range(df.shape[-1])] #[0,1,2,3,4,6,7]\n",
    "    \n",
    "    data_raw = df.as_matrix() # convert to numpy array\n",
    "    # fit scaler\n",
    "    data_raw = scaler_features.fit_transform(data_raw[:, feature_set])\n",
    "    data = []\n",
    "    \n",
    "    # create all possible sequences of length seq_len\n",
    "    for index in range(len(data_raw) - seq_len + 1): \n",
    "        data.append(data_raw[index: index + seq_len, :])\n",
    "    \n",
    "    data = np.array(data)\n",
    "    \n",
    "    if test_size == -1: # split the old way\n",
    "        n_train_valid_pairs = 3\n",
    "        each_train_set_size_pct = 25\n",
    "        each_valid_set_size_pct = 5\n",
    "\n",
    "        each_train_set_size = round(each_train_set_size_pct/100*data.shape[0])\n",
    "        each_valid_set_size = round(each_valid_set_size_pct/100*data.shape[0])\n",
    "\n",
    "        x_train_sets = []\n",
    "        y_train_sets = []\n",
    "        x_valid_sets = []\n",
    "        y_valid_sets = []\n",
    "        used = 0\n",
    "\n",
    "        for i in range(n_train_valid_pairs):\n",
    "            x_train_sets.append(data[used : used + each_train_set_size,:-1,:]) # cannot see last day, which we aim to predict\n",
    "            y_train_sets.append(labels[used : used + each_train_set_size, :])\n",
    "            used += each_train_set_size\n",
    "\n",
    "            x_valid_sets.append(data[used : used + each_valid_set_size,:-1,:])\n",
    "            y_valid_sets.append(labels[used : used + each_valid_set_size, :])\n",
    "            used += each_valid_set_size\n",
    "\n",
    "        x_test = data[used : , :-1, :]\n",
    "        y_test = labels[used : , :]\n",
    "\n",
    "        x_train = np.concatenate(x_train_sets, axis=0)\n",
    "        y_train = np.concatenate(y_train_sets, axis=0)\n",
    "        x_valid = np.concatenate(x_valid_sets, axis=0)\n",
    "        y_valid = np.concatenate(y_valid_sets, axis=0)\n",
    "    \n",
    "    else:\n",
    "        x_test = data[-test_size : , :-1, :]\n",
    "        y_test = labels[-test_size : , :]\n",
    "        \n",
    "        valid_start = data.shape[0] - test_size - int(test_size/2)\n",
    "        x_valid = data[valid_start:-test_size, :-1, :]\n",
    "        y_valid = labels[valid_start:-test_size, :]\n",
    "        \n",
    "        x_train = data[:valid_start, :-1, :]\n",
    "        y_train = labels[:valid_start, :]\n",
    "    \n",
    "    return [x_train, y_train, x_valid, y_valid, x_test, y_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train.shape =  (16981, 9, 56)\n",
      "y_train.shape =  (16981, 3)\n",
      "x_valid.shape =  (998, 9, 56)\n",
      "y_valid.shape =  (998, 3)\n",
      "x_test.shape =  (1996, 9, 56)\n",
      "y_test.shape =  (1996, 3)\n"
     ]
    }
   ],
   "source": [
    "# create train, test data\n",
    "seq_len = 10 # choose sequence length\n",
    "x_train, y_train, x_valid, y_valid, x_test, y_test = load_data(df, seq_len, test_size=1996)\n",
    "# y_train = y_train.reshape([-1,1])\n",
    "# y_valid = y_valid.reshape([-1,1])\n",
    "# y_test = y_test.reshape([-1,1])\n",
    "print('x_train.shape = ',x_train.shape)\n",
    "print('y_train.shape = ', y_train.shape)\n",
    "print('x_valid.shape = ',x_valid.shape)\n",
    "print('y_valid.shape = ', y_valid.shape)\n",
    "print('x_test.shape = ', x_test.shape)\n",
    "print('y_test.shape = ',y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f1_score_single(y_true, y_pred):\n",
    "    TP = y_true.dot(y_pred) # zero or one\n",
    "    FP = np.sum(y_pred > y_true) # sum over all k classes, zero or one\n",
    "    FN = np.sum(y_pred < y_true) # sum over all k classes, zero or one\n",
    "    \n",
    "    if TP == 0: return 0.\n",
    "    p = 1. * TP / (TP + FP)\n",
    "    r = 1. * TP / (TP + FN)\n",
    "    return 2 * p * r / (p + r)\n",
    "    \n",
    "def F1(y_true, y_pred):\n",
    "    return np.mean([f1_score_single(x, y) for x, y in zip(y_true, y_pred)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "baseline dev_F1= 0.39618856569709127\n",
      "baseline test_F1= 0.3712424849699399\n"
     ]
    }
   ],
   "source": [
    "y_pred = np.roll(y_valid,1, axis=0)\n",
    "print('baseline dev_F1=',F1(y_valid[1:], y_pred[1:]))\n",
    "y_pred = np.roll(y_test,1, axis=0)\n",
    "y_pred[0] = y_valid[-1] # be careful here\n",
    "print('baseline test_F1=',F1(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_in_epoch = 0;\n",
    "perm_array  = np.arange(x_train.shape[0])\n",
    "np.random.shuffle(perm_array)\n",
    "\n",
    "# function to get the next batch\n",
    "def get_next_batch(batch_size):\n",
    "    global index_in_epoch, x_train, perm_array   \n",
    "    start = index_in_epoch\n",
    "    index_in_epoch += batch_size\n",
    "    \n",
    "    if index_in_epoch > x_train.shape[0]:\n",
    "        np.random.shuffle(perm_array) # shuffle permutation array\n",
    "        start = 0 # start next epoch\n",
    "        index_in_epoch = batch_size\n",
    "        \n",
    "    end = index_in_epoch\n",
    "    return x_train[perm_array[start:end]], y_train[perm_array[start:end]]\n",
    "\n",
    "x_1000_train, y_1000_train = get_next_batch(1000) # special batch of 1000 records in training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM_Model(object):\n",
    "    def __init__(self, seq_len, use_batch_normalization):\n",
    "        # parameters\n",
    "        self.n_steps = seq_len-1 \n",
    "        self.n_inputs = x_train.shape[-1]\n",
    "        self.n_neurons = 200  # cell.state_size\n",
    "        self.n_bins = 3 # be careful if you want to change this\n",
    "        self.n_layers = 2\n",
    "        self.batch_size = 100\n",
    "        self.n_epochs = 0 # 0 means to train indefinitely\n",
    "        self.train_set_size = x_train.shape[0]\n",
    "        self.test_set_size = x_test.shape[0]\n",
    "        self.keep_prob = tf.placeholder(tf.float32, [])\n",
    "        self.beta = tf.placeholder(tf.float32, []) # to control l2 regularization\n",
    "        self.is_training = tf.placeholder(tf.bool)\n",
    "        self.max_gradient_norm = 5\n",
    "        with tf.variable_scope(\"LSTM_Model\", initializer=tf.contrib.layers.xavier_initializer()):\n",
    "            self.X = tf.placeholder(tf.float32, [None, self.n_steps, self.n_inputs])\n",
    "            self.y = tf.placeholder(tf.float32, [None, self.n_bins])\n",
    "\n",
    "            layers = [tf.contrib.rnn.LSTMCell(num_units=self.n_neurons, \\\n",
    "                                              initializer=tf.contrib.layers.xavier_initializer(), \\\n",
    "                                              activation=tf.nn.elu)\n",
    "                     for layer in range(self.n_layers)]\n",
    "            \n",
    "#             layers = [tf.contrib.rnn.BasicRNNCell(num_units=self.n_neurons, \\\n",
    "#                                               activation=tf.nn.elu)\n",
    "#                      for layer in range(self.n_layers)]\n",
    "            \n",
    "            multi_layer_cell = tf.contrib.rnn.MultiRNNCell(layers)\n",
    "\n",
    "            outputs, states = tf.nn.dynamic_rnn(multi_layer_cell, self.X, dtype=tf.float32)\n",
    "            outputs = tf.nn.dropout(outputs, self.keep_prob) # dropout after LSTM\n",
    "            # 'outputs' is a tensor of shape [batch_size, n_steps, n_neurons(cell.state_size)]\n",
    "        \n",
    "#             directly output\n",
    "#             logits = outputs[:,self.n_steps-1,:] # keep only last output of sequence\n",
    "            \n",
    "            # ======== attn layer ===================\n",
    "#             sim_mat = tf.matmul(outputs, tf.transpose(outputs, perm=[0,2,1]))\n",
    "#             attn_dist = tf.nn.softmax(sim_mat, 2)\n",
    "#             # (batchsize, n_steps, n_steps)\n",
    "#             attn_outputs = tf.matmul(attn_dist, outputs)\n",
    "#             # (batchsize, n_steps, n_bins)\n",
    "            # ========================================\n",
    "            \n",
    "            stacked_outputs = outputs[:, -1, :] # take last timestep\n",
    "#             stacked_outputs = tf.reshape(outputs, [-1, self.n_neurons]) \n",
    "            stacked_outputs = tf.layers.dense(stacked_outputs, 20)  # first FC\n",
    "            # batch normalization\n",
    "            if use_batch_normalization:\n",
    "                stacked_outputs = tf.contrib.layers.batch_norm(stacked_outputs, center=True, \\\n",
    "                                                               scale=False, is_training=self.is_training, scope='bn')\n",
    "            stacked_outputs = tf.nn.dropout(stacked_outputs, self.keep_prob)\n",
    "            \n",
    "            stacked_outputs = tf.layers.dense(stacked_outputs, self.n_bins)  # second FC\n",
    "            self.final_logits = tf.reshape(stacked_outputs, [-1, self.n_bins])\n",
    "            \n",
    "#             self.final_logits = final_outputs[:, -1, :] # last timestep\n",
    "            # (batchsize, n_bins), this is logits, not probs!!\n",
    "            self.final_logits = tf.nn.dropout(self.final_logits, self.keep_prob) # dropout\n",
    "            \n",
    "            self.indices = tf.argmax(self.final_logits, axis=-1) # (batchsize, 1)\n",
    "            self.preds = tf.one_hot(self.indices, depth=self.n_bins)\n",
    "            \n",
    "            self.each_loss = tf.nn.softmax_cross_entropy_with_logits(logits=self.final_logits, labels=self.y)\n",
    "            self.loss = tf.reduce_mean(self.each_loss) \n",
    "           \n",
    "            params = tf.trainable_variables()\n",
    "            \n",
    "            self.loss += self.beta * tf.add_n([ tf.nn.l2_loss(v) for v in params ])\n",
    "            \n",
    "            gradients = tf.gradients(self.loss, params)\n",
    "            self.gradient_norm = tf.global_norm(gradients)\n",
    "            clipped_gradients, _ = tf.clip_by_global_norm(gradients, self.max_gradient_norm)\n",
    "            clipped_norm = tf.global_norm(clipped_gradients)\n",
    "            self.param_norm = tf.global_norm(params)\n",
    "            self.learning_rate_placeholder = tf.placeholder(tf.float32, [], name='learning_rate')\n",
    "            optimizer = tf.train.RMSPropOptimizer(learning_rate=self.learning_rate_placeholder) \n",
    "            # training_op = optimizer.minimize(loss)\n",
    "            self.training_op = optimizer.apply_gradients(zip(clipped_gradients, params))\n",
    "\n",
    "            # initialize parameters\n",
    "#             sess = tf.Session()\n",
    "            self.global_step = tf.Variable(0, name='global_step', trainable=False)\n",
    "            self.saver = tf.train.Saver(max_to_keep=2)\n",
    "            self.bestmodel_saver = tf.train.Saver(max_to_keep=2)\n",
    "    \n",
    "    def train(self, session, experiment_name, keep_prob_val, beta):\n",
    "        \n",
    "        bestmodel_dir = experiment_name+'/best_ckpt'\n",
    "        bestmodel_ckpt_path = bestmodel_dir+'/best.ckpt'\n",
    "        best_valid_f1 = None\n",
    "        # Make bestmodel dir if necessary\n",
    "        if not os.path.exists(bestmodel_dir):\n",
    "            os.makedirs(bestmodel_dir)\n",
    "        \n",
    "        ckpt = tf.train.get_checkpoint_state(experiment_name)\n",
    "        v2_path = ckpt.model_checkpoint_path + \".index\" if ckpt else \"\"\n",
    "        if ckpt and (tf.gfile.Exists(ckpt.model_checkpoint_path) or tf.gfile.Exists(v2_path)):\n",
    "            self.saver.restore(sess, ckpt.model_checkpoint_path)\n",
    "            iteration = self.global_step.eval(session=sess) # get last global_step\n",
    "            print(\"Start from iteration:\", iteration)\n",
    "            lr = 1e-3\n",
    "        else:\n",
    "            print('There is not saved parameters. Creating model with fresh parameters.')\n",
    "            sess.run(tf.global_variables_initializer())\n",
    "            iteration = 0\n",
    "            lr = 1e-2 # should fix this...\n",
    "\n",
    "        old_loss = 1000\n",
    "        \n",
    "        while self.n_epochs == 0 or iteration*self.batch_size/self.train_set_size < self.n_epochs:\n",
    "            iteration = iteration + 1\n",
    "            x_batch, y_batch = get_next_batch(self.batch_size) # fetch the next training batch \n",
    "\n",
    "            # train on this batch\n",
    "            sess.run(self.training_op, feed_dict={self.X: x_batch, self.y: y_batch, self.learning_rate_placeholder:lr, \\\n",
    "                                                 self.keep_prob: keep_prob_val, self.beta: beta, self.is_training: True})\n",
    "\n",
    "            if iteration % 200 == 0:\n",
    "                y_1000_train_pred, loss_val, param_norm_val, grad_norm_val = \\\n",
    "                    sess.run([self.preds, self.loss, self.param_norm, self.gradient_norm],\\\n",
    "                            feed_dict={self.X: x_1000_train, self.y:y_1000_train, \\\n",
    "                                        self.learning_rate_placeholder:lr, self.keep_prob: keep_prob_val, \\\n",
    "                                        self.beta: beta, self.is_training: False})\n",
    "                    \n",
    "                if loss_val > old_loss * 1.2:\n",
    "                    lr /= 2\n",
    "                old_loss = loss_val\n",
    "\n",
    "                y_valid_pred = sess.run(self.preds, feed_dict={self.X: x_valid, self.keep_prob: keep_prob_val,\\\n",
    "                                                              self.is_training: False})\n",
    "                \n",
    "                valid_f1 = F1(y_valid, y_valid_pred)\n",
    "                print('%.2f epochs, iter %d: train_loss = %.9f, param_norm = %.3f, grad_norm = %.3f, train_F1/valid_F1 = %.6f/%.6f' \\\n",
    "                      %(iteration*self.batch_size/self.train_set_size, iteration, loss_val, param_norm_val, grad_norm_val, \\\n",
    "                        F1(y_1000_train, y_1000_train_pred), \\\n",
    "                        valid_f1))\n",
    "\n",
    "                if best_valid_f1 is None or valid_f1 > best_valid_f1:\n",
    "                    best_valid_f1 = valid_f1\n",
    "                    self.global_step.assign(iteration).eval(session=sess)\n",
    "                    print(\"======New best valid F1. Saving to %s...\" % bestmodel_ckpt_path)\n",
    "                    self.bestmodel_saver.save(sess, bestmodel_ckpt_path, global_step=self.global_step)\n",
    "                \n",
    "            if iteration % 1000 == 0:\n",
    "                self.global_step.assign(iteration).eval(session=sess) # set and update(eval) global_step with index, i\n",
    "                save_path = self.saver.save(sess, \"./\"+experiment_name+\"/model.ckpt\", global_step=self.global_step)\n",
    "                print('Saved parameters to %s' % save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## run experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_name=\"04-28-02\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "lstm_model = LSTM_Model(seq_len=seq_len, use_batch_normalization=False)\n",
    "sess = tf.Session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.1730167\n",
      "2.0823033\n",
      "iter 50: train_loss = 1.011497855, train_F1= 0.540000\n",
      "iter 100: train_loss = 0.979385853, train_F1= 0.530000\n",
      "iter 150: train_loss = 0.881866217, train_F1= 0.620000\n",
      "iter 200: train_loss = 0.752146482, train_F1= 0.640000\n",
      "iter 250: train_loss = 0.716808558, train_F1= 0.670000\n",
      "iter 300: train_loss = 0.612567961, train_F1= 0.720000\n",
      "iter 350: train_loss = 0.499484569, train_F1= 0.820000\n",
      "iter 400: train_loss = 0.410808474, train_F1= 0.870000\n",
      "iter 450: train_loss = 0.369383961, train_F1= 0.920000\n",
      "iter 500: train_loss = 0.305401087, train_F1= 0.920000\n",
      "iter 550: train_loss = 0.294736117, train_F1= 0.900000\n",
      "iter 600: train_loss = 0.288513750, train_F1= 0.920000\n",
      "iter 650: train_loss = 0.241855398, train_F1= 0.930000\n",
      "iter 700: train_loss = 0.519630015, train_F1= 0.790000\n",
      "iter 750: train_loss = 0.143681929, train_F1= 0.970000\n",
      "iter 800: train_loss = 0.374593824, train_F1= 0.880000\n",
      "iter 850: train_loss = 0.166960984, train_F1= 0.950000\n",
      "iter 900: train_loss = 0.129060283, train_F1= 0.960000\n",
      "iter 950: train_loss = 0.074580029, train_F1= 0.990000\n",
      "iter 1000: train_loss = 0.138810188, train_F1= 0.970000\n",
      "iter 1050: train_loss = 0.023935737, train_F1= 1.000000\n"
     ]
    }
   ],
   "source": [
    "# sanity check\n",
    "sess.run(tf.global_variables_initializer())\n",
    "# first check random loss\n",
    "sanity_loss = sess.run(lstm_model.loss, feed_dict={lstm_model.X: x_1000_train, lstm_model.y:y_1000_train, \\\n",
    "                                        lstm_model.learning_rate_placeholder:1e-3, lstm_model.keep_prob: 1, \\\n",
    "                                                   lstm_model.beta: 0, lstm_model.is_training: False})\n",
    "print(sanity_loss) # should be around 1.09\n",
    "\n",
    "sanity_loss = sess.run(lstm_model.loss, feed_dict={lstm_model.X: x_1000_train, lstm_model.y:y_1000_train, \\\n",
    "                                        lstm_model.learning_rate_placeholder:1e-3, lstm_model.keep_prob: 1, \\\n",
    "                                                   lstm_model.beta: 0.005, lstm_model.is_training: False})\n",
    "print(sanity_loss) # should be around 1.09\n",
    "\n",
    "x_100_train, y_100_train = get_next_batch(100)\n",
    "iter_ = 0\n",
    "while True:\n",
    "    iter_ += 1\n",
    "    _, y_100_train_pred, loss_val = sess.run([lstm_model.training_op, lstm_model.preds, lstm_model.loss], \\\n",
    "                                             feed_dict={lstm_model.X: x_100_train, lstm_model.y:y_100_train, \\\n",
    "                                        lstm_model.learning_rate_placeholder:1e-3, lstm_model.keep_prob: 1, \\\n",
    "                                                        lstm_model.beta: 0, lstm_model.is_training: True})\n",
    "    if iter_ % 50 == 0:\n",
    "        print('iter %d: train_loss = %.9f, train_F1= %.6f' \\\n",
    "                      %(iter_, loss_val, F1(y_100_train, y_100_train_pred)))\n",
    "    if loss_val < 0.01:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There is not saved parameters. Creating model with fresh parameters.\n",
      "1.18 epochs, iter 200: train_loss = 1.180256248, param_norm = 7.305, grad_norm = 0.049, train_F1/valid_F1 = 0.500000/0.189379\n",
      "======New best valid F1. Saving to 04-28-02/best_ckpt/best.ckpt...\n",
      "2.36 epochs, iter 400: train_loss = 1.698128700, param_norm = 16.019, grad_norm = 0.132, train_F1/valid_F1 = 0.490000/0.194389\n",
      "======New best valid F1. Saving to 04-28-02/best_ckpt/best.ckpt...\n",
      "3.53 epochs, iter 600: train_loss = 1.037311196, param_norm = 2.120, grad_norm = 0.067, train_F1/valid_F1 = 0.496000/0.390782\n",
      "======New best valid F1. Saving to 04-28-02/best_ckpt/best.ckpt...\n",
      "4.71 epochs, iter 800: train_loss = 1.030741096, param_norm = 1.992, grad_norm = 0.020, train_F1/valid_F1 = 0.500000/0.412826\n",
      "======New best valid F1. Saving to 04-28-02/best_ckpt/best.ckpt...\n",
      "5.89 epochs, iter 1000: train_loss = 1.040804386, param_norm = 2.058, grad_norm = 0.136, train_F1/valid_F1 = 0.495000/0.379760\n",
      "Saved parameters to ./04-28-02/model.ckpt-1000\n",
      "7.07 epochs, iter 1200: train_loss = 1.105531216, param_norm = 2.560, grad_norm = 0.812, train_F1/valid_F1 = 0.496000/0.420842\n",
      "======New best valid F1. Saving to 04-28-02/best_ckpt/best.ckpt...\n",
      "8.24 epochs, iter 1400: train_loss = 1.041728616, param_norm = 1.937, grad_norm = 0.110, train_F1/valid_F1 = 0.491000/0.330661\n",
      "9.42 epochs, iter 1600: train_loss = 1.038511634, param_norm = 2.148, grad_norm = 0.098, train_F1/valid_F1 = 0.501000/0.408818\n",
      "10.60 epochs, iter 1800: train_loss = 1.045428514, param_norm = 2.946, grad_norm = 0.043, train_F1/valid_F1 = 0.484000/0.417836\n",
      "11.78 epochs, iter 2000: train_loss = 1.061080694, param_norm = 3.193, grad_norm = 0.158, train_F1/valid_F1 = 0.498000/0.369739\n",
      "Saved parameters to ./04-28-02/model.ckpt-2000\n",
      "12.96 epochs, iter 2200: train_loss = 1.040649295, param_norm = 2.603, grad_norm = 0.059, train_F1/valid_F1 = 0.493000/0.419840\n",
      "14.13 epochs, iter 2400: train_loss = 1.033723116, param_norm = 2.112, grad_norm = 0.039, train_F1/valid_F1 = 0.502000/0.413828\n",
      "15.31 epochs, iter 2600: train_loss = 1.042426348, param_norm = 2.068, grad_norm = 0.145, train_F1/valid_F1 = 0.497000/0.384770\n",
      "16.49 epochs, iter 2800: train_loss = 1.036459327, param_norm = 2.291, grad_norm = 0.069, train_F1/valid_F1 = 0.507000/0.395792\n",
      "17.67 epochs, iter 3000: train_loss = 1.053726315, param_norm = 2.675, grad_norm = 0.204, train_F1/valid_F1 = 0.486000/0.420842\n",
      "Saved parameters to ./04-28-02/model.ckpt-3000\n",
      "18.84 epochs, iter 3200: train_loss = 1.035033226, param_norm = 2.243, grad_norm = 0.052, train_F1/valid_F1 = 0.493000/0.413828\n",
      "20.02 epochs, iter 3400: train_loss = 1.045175910, param_norm = 2.504, grad_norm = 0.219, train_F1/valid_F1 = 0.503000/0.402806\n",
      "21.20 epochs, iter 3600: train_loss = 1.033437252, param_norm = 2.496, grad_norm = 0.074, train_F1/valid_F1 = 0.509000/0.416834\n",
      "22.38 epochs, iter 3800: train_loss = 1.028473139, param_norm = 2.170, grad_norm = 0.076, train_F1/valid_F1 = 0.503000/0.403808\n",
      "23.56 epochs, iter 4000: train_loss = 1.042446971, param_norm = 2.495, grad_norm = 0.260, train_F1/valid_F1 = 0.507000/0.401804\n",
      "Saved parameters to ./04-28-02/model.ckpt-4000\n",
      "24.73 epochs, iter 4200: train_loss = 1.076905966, param_norm = 4.511, grad_norm = 0.047, train_F1/valid_F1 = 0.503000/0.377756\n",
      "25.91 epochs, iter 4400: train_loss = 1.027890563, param_norm = 2.281, grad_norm = 0.197, train_F1/valid_F1 = 0.515000/0.419840\n",
      "27.09 epochs, iter 4600: train_loss = 1.031271696, param_norm = 2.316, grad_norm = 0.059, train_F1/valid_F1 = 0.497000/0.412826\n",
      "28.27 epochs, iter 4800: train_loss = 1.075619698, param_norm = 4.164, grad_norm = 0.285, train_F1/valid_F1 = 0.491000/0.268537\n",
      "29.44 epochs, iter 5000: train_loss = 1.032838225, param_norm = 2.696, grad_norm = 0.120, train_F1/valid_F1 = 0.497000/0.412826\n",
      "Saved parameters to ./04-28-02/model.ckpt-5000\n",
      "30.62 epochs, iter 5200: train_loss = 1.033813000, param_norm = 2.507, grad_norm = 0.041, train_F1/valid_F1 = 0.504000/0.421844\n",
      "======New best valid F1. Saving to 04-28-02/best_ckpt/best.ckpt...\n",
      "31.80 epochs, iter 5400: train_loss = 1.028120637, param_norm = 2.461, grad_norm = 0.111, train_F1/valid_F1 = 0.511000/0.400802\n",
      "32.98 epochs, iter 5600: train_loss = 1.033191442, param_norm = 2.480, grad_norm = 0.072, train_F1/valid_F1 = 0.495000/0.379760\n",
      "34.16 epochs, iter 5800: train_loss = 1.040048122, param_norm = 2.455, grad_norm = 0.454, train_F1/valid_F1 = 0.501000/0.412826\n",
      "35.33 epochs, iter 6000: train_loss = 1.032369137, param_norm = 2.374, grad_norm = 0.128, train_F1/valid_F1 = 0.509000/0.427856\n",
      "======New best valid F1. Saving to 04-28-02/best_ckpt/best.ckpt...\n",
      "Saved parameters to ./04-28-02/model.ckpt-6000\n",
      "36.51 epochs, iter 6200: train_loss = 1.029187918, param_norm = 2.336, grad_norm = 0.126, train_F1/valid_F1 = 0.506000/0.413828\n",
      "37.69 epochs, iter 6400: train_loss = 1.239961863, param_norm = 9.161, grad_norm = 0.177, train_F1/valid_F1 = 0.494000/0.189379\n",
      "38.87 epochs, iter 6600: train_loss = 1.026012778, param_norm = 2.282, grad_norm = 0.331, train_F1/valid_F1 = 0.518000/0.411824\n",
      "40.04 epochs, iter 6800: train_loss = 1.018985391, param_norm = 2.341, grad_norm = 0.278, train_F1/valid_F1 = 0.517000/0.422846\n",
      "41.22 epochs, iter 7000: train_loss = 1.016539812, param_norm = 2.369, grad_norm = 0.109, train_F1/valid_F1 = 0.511000/0.421844\n",
      "Saved parameters to ./04-28-02/model.ckpt-7000\n",
      "42.40 epochs, iter 7200: train_loss = 1.024262428, param_norm = 2.707, grad_norm = 0.090, train_F1/valid_F1 = 0.514000/0.413828\n",
      "43.58 epochs, iter 7400: train_loss = 1.026682973, param_norm = 2.378, grad_norm = 0.589, train_F1/valid_F1 = 0.513000/0.427856\n",
      "44.76 epochs, iter 7600: train_loss = 1.024210215, param_norm = 2.352, grad_norm = 0.484, train_F1/valid_F1 = 0.503000/0.419840\n",
      "45.93 epochs, iter 7800: train_loss = 1.022555709, param_norm = 2.461, grad_norm = 0.458, train_F1/valid_F1 = 0.507000/0.420842\n",
      "47.11 epochs, iter 8000: train_loss = 1.034962416, param_norm = 2.436, grad_norm = 0.910, train_F1/valid_F1 = 0.499000/0.422846\n",
      "Saved parameters to ./04-28-02/model.ckpt-8000\n",
      "48.29 epochs, iter 8200: train_loss = 1.017866850, param_norm = 2.371, grad_norm = 0.066, train_F1/valid_F1 = 0.500000/0.406814\n",
      "49.47 epochs, iter 8400: train_loss = 1.024116635, param_norm = 2.403, grad_norm = 0.096, train_F1/valid_F1 = 0.509000/0.414830\n",
      "50.64 epochs, iter 8600: train_loss = 1.016631603, param_norm = 2.386, grad_norm = 0.573, train_F1/valid_F1 = 0.509000/0.408818\n",
      "51.82 epochs, iter 8800: train_loss = 1.029573321, param_norm = 2.558, grad_norm = 0.373, train_F1/valid_F1 = 0.503000/0.398798\n",
      "53.00 epochs, iter 9000: train_loss = 1.028026938, param_norm = 2.442, grad_norm = 0.596, train_F1/valid_F1 = 0.507000/0.422846\n",
      "Saved parameters to ./04-28-02/model.ckpt-9000\n",
      "54.18 epochs, iter 9200: train_loss = 1.053425550, param_norm = 2.452, grad_norm = 1.332, train_F1/valid_F1 = 0.507000/0.423848\n",
      "55.36 epochs, iter 9400: train_loss = 1.016753435, param_norm = 2.421, grad_norm = 0.158, train_F1/valid_F1 = 0.511000/0.414830\n",
      "56.53 epochs, iter 9600: train_loss = 1.023724437, param_norm = 2.344, grad_norm = 0.589, train_F1/valid_F1 = 0.506000/0.412826\n",
      "57.71 epochs, iter 9800: train_loss = 1.028094888, param_norm = 2.605, grad_norm = 0.488, train_F1/valid_F1 = 0.510000/0.419840\n",
      "58.89 epochs, iter 10000: train_loss = 1.040967584, param_norm = 2.827, grad_norm = 1.139, train_F1/valid_F1 = 0.497000/0.422846\n",
      "Saved parameters to ./04-28-02/model.ckpt-10000\n",
      "60.07 epochs, iter 10200: train_loss = 1.021587253, param_norm = 2.370, grad_norm = 0.442, train_F1/valid_F1 = 0.511000/0.410822\n",
      "61.24 epochs, iter 10400: train_loss = 1.031140924, param_norm = 2.497, grad_norm = 1.103, train_F1/valid_F1 = 0.505000/0.411824\n",
      "62.42 epochs, iter 10600: train_loss = 1.037484169, param_norm = 2.495, grad_norm = 1.451, train_F1/valid_F1 = 0.499000/0.392786\n",
      "63.60 epochs, iter 10800: train_loss = 1.018498302, param_norm = 2.391, grad_norm = 0.184, train_F1/valid_F1 = 0.499000/0.428858\n",
      "======New best valid F1. Saving to 04-28-02/best_ckpt/best.ckpt...\n",
      "64.78 epochs, iter 11000: train_loss = 1.026466370, param_norm = 2.472, grad_norm = 0.631, train_F1/valid_F1 = 0.505000/0.402806\n",
      "Saved parameters to ./04-28-02/model.ckpt-11000\n",
      "65.96 epochs, iter 11200: train_loss = 1.026607394, param_norm = 2.925, grad_norm = 0.102, train_F1/valid_F1 = 0.515000/0.407816\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "67.13 epochs, iter 11400: train_loss = 1.032280922, param_norm = 2.634, grad_norm = 0.662, train_F1/valid_F1 = 0.508000/0.410822\n",
      "68.31 epochs, iter 11600: train_loss = 1.022282839, param_norm = 2.415, grad_norm = 0.694, train_F1/valid_F1 = 0.510000/0.405812\n",
      "69.49 epochs, iter 11800: train_loss = 1.033065081, param_norm = 2.396, grad_norm = 0.938, train_F1/valid_F1 = 0.502000/0.425852\n",
      "70.67 epochs, iter 12000: train_loss = 1.036429644, param_norm = 3.016, grad_norm = 1.065, train_F1/valid_F1 = 0.496000/0.418838\n",
      "Saved parameters to ./04-28-02/model.ckpt-12000\n",
      "71.85 epochs, iter 12200: train_loss = 1.024908900, param_norm = 2.389, grad_norm = 0.295, train_F1/valid_F1 = 0.511000/0.431864\n",
      "======New best valid F1. Saving to 04-28-02/best_ckpt/best.ckpt...\n",
      "73.02 epochs, iter 12400: train_loss = 1.035925388, param_norm = 2.536, grad_norm = 0.919, train_F1/valid_F1 = 0.503000/0.407816\n",
      "74.20 epochs, iter 12600: train_loss = 1.021008372, param_norm = 2.482, grad_norm = 0.087, train_F1/valid_F1 = 0.509000/0.400802\n",
      "75.38 epochs, iter 12800: train_loss = 1.020767689, param_norm = 2.740, grad_norm = 0.188, train_F1/valid_F1 = 0.521000/0.413828\n",
      "76.56 epochs, iter 13000: train_loss = 1.054565907, param_norm = 3.925, grad_norm = 1.048, train_F1/valid_F1 = 0.505000/0.391784\n",
      "Saved parameters to ./04-28-02/model.ckpt-13000\n",
      "77.73 epochs, iter 13200: train_loss = 1.023381591, param_norm = 2.467, grad_norm = 0.507, train_F1/valid_F1 = 0.498000/0.413828\n",
      "78.91 epochs, iter 13400: train_loss = 1.021377206, param_norm = 2.395, grad_norm = 0.635, train_F1/valid_F1 = 0.500000/0.428858\n",
      "80.09 epochs, iter 13600: train_loss = 1.021297932, param_norm = 2.494, grad_norm = 0.322, train_F1/valid_F1 = 0.510000/0.415832\n",
      "81.27 epochs, iter 13800: train_loss = 1.024227381, param_norm = 2.482, grad_norm = 0.658, train_F1/valid_F1 = 0.506000/0.423848\n",
      "82.45 epochs, iter 14000: train_loss = 1.020039320, param_norm = 2.391, grad_norm = 0.307, train_F1/valid_F1 = 0.503000/0.416834\n",
      "Saved parameters to ./04-28-02/model.ckpt-14000\n",
      "83.62 epochs, iter 14200: train_loss = 1.045082450, param_norm = 2.430, grad_norm = 1.473, train_F1/valid_F1 = 0.508000/0.405812\n",
      "84.80 epochs, iter 14400: train_loss = 1.018323660, param_norm = 2.400, grad_norm = 0.252, train_F1/valid_F1 = 0.496000/0.428858\n",
      "85.98 epochs, iter 14600: train_loss = 1.019904017, param_norm = 2.547, grad_norm = 0.216, train_F1/valid_F1 = 0.503000/0.438878\n",
      "======New best valid F1. Saving to 04-28-02/best_ckpt/best.ckpt...\n",
      "87.16 epochs, iter 14800: train_loss = 1.018835425, param_norm = 2.406, grad_norm = 0.156, train_F1/valid_F1 = 0.525000/0.426854\n",
      "88.33 epochs, iter 15000: train_loss = 1.025322080, param_norm = 2.550, grad_norm = 0.353, train_F1/valid_F1 = 0.500000/0.408818\n",
      "Saved parameters to ./04-28-02/model.ckpt-15000\n",
      "89.51 epochs, iter 15200: train_loss = 1.018730640, param_norm = 2.403, grad_norm = 0.520, train_F1/valid_F1 = 0.511000/0.425852\n",
      "90.69 epochs, iter 15400: train_loss = 1.020026207, param_norm = 2.403, grad_norm = 0.294, train_F1/valid_F1 = 0.512000/0.421844\n",
      "91.87 epochs, iter 15600: train_loss = 1.024262786, param_norm = 2.504, grad_norm = 0.629, train_F1/valid_F1 = 0.502000/0.381764\n",
      "93.05 epochs, iter 15800: train_loss = 1.022584438, param_norm = 2.485, grad_norm = 0.792, train_F1/valid_F1 = 0.508000/0.418838\n",
      "94.22 epochs, iter 16000: train_loss = 1.018791556, param_norm = 2.455, grad_norm = 0.170, train_F1/valid_F1 = 0.511000/0.421844\n",
      "Saved parameters to ./04-28-02/model.ckpt-16000\n",
      "95.40 epochs, iter 16200: train_loss = 1.019240856, param_norm = 2.452, grad_norm = 0.091, train_F1/valid_F1 = 0.510000/0.425852\n",
      "96.58 epochs, iter 16400: train_loss = 1.022334814, param_norm = 2.471, grad_norm = 0.120, train_F1/valid_F1 = 0.516000/0.420842\n",
      "97.76 epochs, iter 16600: train_loss = 1.015172243, param_norm = 2.465, grad_norm = 0.136, train_F1/valid_F1 = 0.513000/0.413828\n",
      "98.93 epochs, iter 16800: train_loss = 1.024411559, param_norm = 2.401, grad_norm = 0.304, train_F1/valid_F1 = 0.515000/0.409820\n",
      "100.11 epochs, iter 17000: train_loss = 1.035740376, param_norm = 2.505, grad_norm = 1.328, train_F1/valid_F1 = 0.493000/0.416834\n",
      "Saved parameters to ./04-28-02/model.ckpt-17000\n",
      "101.29 epochs, iter 17200: train_loss = 1.025848150, param_norm = 2.474, grad_norm = 0.955, train_F1/valid_F1 = 0.513000/0.426854\n",
      "102.47 epochs, iter 17400: train_loss = 1.014657140, param_norm = 2.464, grad_norm = 0.223, train_F1/valid_F1 = 0.506000/0.413828\n",
      "103.65 epochs, iter 17600: train_loss = 1.025157690, param_norm = 2.582, grad_norm = 0.825, train_F1/valid_F1 = 0.502000/0.422846\n",
      "104.82 epochs, iter 17800: train_loss = 1.024298191, param_norm = 2.632, grad_norm = 0.175, train_F1/valid_F1 = 0.498000/0.426854\n",
      "106.00 epochs, iter 18000: train_loss = 1.018270135, param_norm = 2.461, grad_norm = 0.102, train_F1/valid_F1 = 0.511000/0.428858\n",
      "Saved parameters to ./04-28-02/model.ckpt-18000\n",
      "107.18 epochs, iter 18200: train_loss = 1.026690006, param_norm = 2.522, grad_norm = 0.753, train_F1/valid_F1 = 0.513000/0.412826\n",
      "108.36 epochs, iter 18400: train_loss = 1.023281574, param_norm = 2.727, grad_norm = 0.335, train_F1/valid_F1 = 0.504000/0.417836\n",
      "109.53 epochs, iter 18600: train_loss = 1.021041512, param_norm = 2.428, grad_norm = 0.136, train_F1/valid_F1 = 0.506000/0.404810\n",
      "110.71 epochs, iter 18800: train_loss = 1.047577739, param_norm = 4.003, grad_norm = 0.600, train_F1/valid_F1 = 0.506000/0.416834\n",
      "111.89 epochs, iter 19000: train_loss = 1.020217299, param_norm = 2.465, grad_norm = 0.154, train_F1/valid_F1 = 0.509000/0.414830\n",
      "Saved parameters to ./04-28-02/model.ckpt-19000\n",
      "113.07 epochs, iter 19200: train_loss = 1.012858510, param_norm = 2.436, grad_norm = 0.334, train_F1/valid_F1 = 0.511000/0.429860\n",
      "114.25 epochs, iter 19400: train_loss = 1.024429679, param_norm = 2.577, grad_norm = 0.136, train_F1/valid_F1 = 0.511000/0.411824\n",
      "115.42 epochs, iter 19600: train_loss = 1.025553346, param_norm = 2.444, grad_norm = 0.575, train_F1/valid_F1 = 0.512000/0.425852\n",
      "116.60 epochs, iter 19800: train_loss = 1.026807427, param_norm = 2.430, grad_norm = 0.801, train_F1/valid_F1 = 0.506000/0.405812\n",
      "117.78 epochs, iter 20000: train_loss = 1.027512550, param_norm = 2.424, grad_norm = 0.753, train_F1/valid_F1 = 0.498000/0.416834\n",
      "Saved parameters to ./04-28-02/model.ckpt-20000\n",
      "118.96 epochs, iter 20200: train_loss = 1.022143483, param_norm = 2.436, grad_norm = 0.391, train_F1/valid_F1 = 0.505000/0.410822\n",
      "120.13 epochs, iter 20400: train_loss = 1.010539770, param_norm = 2.404, grad_norm = 0.346, train_F1/valid_F1 = 0.508000/0.411824\n",
      "121.31 epochs, iter 20600: train_loss = 1.021653414, param_norm = 2.395, grad_norm = 0.074, train_F1/valid_F1 = 0.505000/0.421844\n",
      "122.49 epochs, iter 20800: train_loss = 1.031972766, param_norm = 2.805, grad_norm = 0.608, train_F1/valid_F1 = 0.508000/0.409820\n",
      "123.67 epochs, iter 21000: train_loss = 1.021342039, param_norm = 2.458, grad_norm = 0.541, train_F1/valid_F1 = 0.508000/0.424850\n",
      "Saved parameters to ./04-28-02/model.ckpt-21000\n",
      "124.85 epochs, iter 21200: train_loss = 1.023184180, param_norm = 2.478, grad_norm = 0.304, train_F1/valid_F1 = 0.512000/0.431864\n",
      "126.02 epochs, iter 21400: train_loss = 1.020415068, param_norm = 2.448, grad_norm = 0.647, train_F1/valid_F1 = 0.509000/0.413828\n",
      "127.20 epochs, iter 21600: train_loss = 1.021794319, param_norm = 2.501, grad_norm = 0.267, train_F1/valid_F1 = 0.502000/0.403808\n",
      "128.38 epochs, iter 21800: train_loss = 1.037206650, param_norm = 2.613, grad_norm = 1.513, train_F1/valid_F1 = 0.501000/0.419840\n",
      "129.56 epochs, iter 22000: train_loss = 1.032541156, param_norm = 2.412, grad_norm = 0.838, train_F1/valid_F1 = 0.506000/0.403808\n",
      "Saved parameters to ./04-28-02/model.ckpt-22000\n",
      "130.73 epochs, iter 22200: train_loss = 1.025681853, param_norm = 2.465, grad_norm = 0.868, train_F1/valid_F1 = 0.516000/0.425852\n",
      "131.91 epochs, iter 22400: train_loss = 1.017804265, param_norm = 2.474, grad_norm = 0.420, train_F1/valid_F1 = 0.525000/0.425852\n",
      "133.09 epochs, iter 22600: train_loss = 1.032393932, param_norm = 2.563, grad_norm = 1.171, train_F1/valid_F1 = 0.508000/0.426854\n",
      "134.27 epochs, iter 22800: train_loss = 1.022193074, param_norm = 2.627, grad_norm = 0.410, train_F1/valid_F1 = 0.513000/0.434870\n",
      "135.45 epochs, iter 23000: train_loss = 1.017528534, param_norm = 2.547, grad_norm = 0.255, train_F1/valid_F1 = 0.514000/0.419840\n",
      "Saved parameters to ./04-28-02/model.ckpt-23000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "136.62 epochs, iter 23200: train_loss = 1.021192789, param_norm = 2.401, grad_norm = 0.190, train_F1/valid_F1 = 0.503000/0.423848\n",
      "137.80 epochs, iter 23400: train_loss = 1.022510529, param_norm = 2.385, grad_norm = 0.072, train_F1/valid_F1 = 0.511000/0.428858\n",
      "138.98 epochs, iter 23600: train_loss = 1.030391932, param_norm = 2.415, grad_norm = 0.870, train_F1/valid_F1 = 0.505000/0.428858\n",
      "140.16 epochs, iter 23800: train_loss = 1.025235415, param_norm = 2.663, grad_norm = 0.752, train_F1/valid_F1 = 0.499000/0.419840\n",
      "141.33 epochs, iter 24000: train_loss = 1.044010639, param_norm = 2.752, grad_norm = 1.261, train_F1/valid_F1 = 0.491000/0.423848\n",
      "Saved parameters to ./04-28-02/model.ckpt-24000\n",
      "142.51 epochs, iter 24200: train_loss = 1.033337355, param_norm = 2.467, grad_norm = 1.149, train_F1/valid_F1 = 0.508000/0.430862\n",
      "143.69 epochs, iter 24400: train_loss = 1.025221586, param_norm = 2.516, grad_norm = 0.301, train_F1/valid_F1 = 0.510000/0.438878\n",
      "144.87 epochs, iter 24600: train_loss = 1.020261288, param_norm = 2.448, grad_norm = 0.319, train_F1/valid_F1 = 0.495000/0.421844\n",
      "146.05 epochs, iter 24800: train_loss = 1.023066878, param_norm = 2.448, grad_norm = 0.488, train_F1/valid_F1 = 0.508000/0.412826\n",
      "147.22 epochs, iter 25000: train_loss = 1.022505641, param_norm = 2.463, grad_norm = 0.904, train_F1/valid_F1 = 0.494000/0.415832\n",
      "Saved parameters to ./04-28-02/model.ckpt-25000\n",
      "148.40 epochs, iter 25200: train_loss = 1.011536598, param_norm = 2.489, grad_norm = 0.368, train_F1/valid_F1 = 0.513000/0.414830\n",
      "149.58 epochs, iter 25400: train_loss = 1.024315119, param_norm = 2.471, grad_norm = 0.241, train_F1/valid_F1 = 0.516000/0.422846\n",
      "150.76 epochs, iter 25600: train_loss = 1.024576426, param_norm = 2.588, grad_norm = 0.065, train_F1/valid_F1 = 0.515000/0.423848\n",
      "151.93 epochs, iter 25800: train_loss = 1.026772261, param_norm = 2.398, grad_norm = 0.410, train_F1/valid_F1 = 0.517000/0.410822\n",
      "153.11 epochs, iter 26000: train_loss = 1.024912119, param_norm = 2.520, grad_norm = 0.200, train_F1/valid_F1 = 0.505000/0.429860\n",
      "Saved parameters to ./04-28-02/model.ckpt-26000\n",
      "154.29 epochs, iter 26200: train_loss = 1.015745282, param_norm = 2.622, grad_norm = 0.422, train_F1/valid_F1 = 0.504000/0.402806\n",
      "155.47 epochs, iter 26400: train_loss = 1.066851377, param_norm = 2.650, grad_norm = 2.640, train_F1/valid_F1 = 0.494000/0.422846\n",
      "156.65 epochs, iter 26600: train_loss = 1.024004698, param_norm = 2.415, grad_norm = 0.212, train_F1/valid_F1 = 0.507000/0.405812\n",
      "157.82 epochs, iter 26800: train_loss = 1.012607217, param_norm = 2.402, grad_norm = 0.337, train_F1/valid_F1 = 0.513000/0.423848\n",
      "159.00 epochs, iter 27000: train_loss = 1.020989895, param_norm = 2.519, grad_norm = 0.127, train_F1/valid_F1 = 0.511000/0.409820\n",
      "Saved parameters to ./04-28-02/model.ckpt-27000\n",
      "160.18 epochs, iter 27200: train_loss = 1.032888651, param_norm = 2.764, grad_norm = 0.891, train_F1/valid_F1 = 0.519000/0.417836\n",
      "161.36 epochs, iter 27400: train_loss = 1.011947870, param_norm = 2.471, grad_norm = 0.136, train_F1/valid_F1 = 0.508000/0.415832\n",
      "162.53 epochs, iter 27600: train_loss = 1.024555922, param_norm = 2.412, grad_norm = 0.617, train_F1/valid_F1 = 0.508000/0.399800\n",
      "163.71 epochs, iter 27800: train_loss = 1.015287399, param_norm = 2.511, grad_norm = 0.123, train_F1/valid_F1 = 0.519000/0.415832\n",
      "164.89 epochs, iter 28000: train_loss = 1.031037807, param_norm = 2.429, grad_norm = 1.037, train_F1/valid_F1 = 0.511000/0.432866\n",
      "Saved parameters to ./04-28-02/model.ckpt-28000\n",
      "166.07 epochs, iter 28200: train_loss = 1.034170389, param_norm = 2.579, grad_norm = 0.834, train_F1/valid_F1 = 0.503000/0.416834\n",
      "167.25 epochs, iter 28400: train_loss = 1.029973388, param_norm = 2.483, grad_norm = 0.919, train_F1/valid_F1 = 0.505000/0.411824\n",
      "168.42 epochs, iter 28600: train_loss = 1.018034935, param_norm = 2.459, grad_norm = 0.308, train_F1/valid_F1 = 0.511000/0.408818\n",
      "169.60 epochs, iter 28800: train_loss = 1.020670056, param_norm = 2.420, grad_norm = 0.338, train_F1/valid_F1 = 0.520000/0.415832\n",
      "170.78 epochs, iter 29000: train_loss = 1.023374081, param_norm = 2.485, grad_norm = 0.089, train_F1/valid_F1 = 0.509000/0.422846\n",
      "Saved parameters to ./04-28-02/model.ckpt-29000\n",
      "171.96 epochs, iter 29200: train_loss = 1.037742615, param_norm = 2.779, grad_norm = 1.200, train_F1/valid_F1 = 0.511000/0.421844\n",
      "173.13 epochs, iter 29400: train_loss = 1.023498774, param_norm = 2.610, grad_norm = 0.086, train_F1/valid_F1 = 0.516000/0.411824\n",
      "174.31 epochs, iter 29600: train_loss = 1.020455360, param_norm = 2.444, grad_norm = 0.776, train_F1/valid_F1 = 0.514000/0.411824\n",
      "175.49 epochs, iter 29800: train_loss = 1.015950680, param_norm = 2.424, grad_norm = 0.257, train_F1/valid_F1 = 0.514000/0.422846\n",
      "176.67 epochs, iter 30000: train_loss = 1.015646815, param_norm = 2.527, grad_norm = 0.423, train_F1/valid_F1 = 0.527000/0.405812\n",
      "Saved parameters to ./04-28-02/model.ckpt-30000\n",
      "177.85 epochs, iter 30200: train_loss = 1.017254472, param_norm = 2.506, grad_norm = 0.075, train_F1/valid_F1 = 0.524000/0.424850\n",
      "179.02 epochs, iter 30400: train_loss = 1.030392766, param_norm = 2.447, grad_norm = 0.807, train_F1/valid_F1 = 0.509000/0.413828\n",
      "180.20 epochs, iter 30600: train_loss = 1.023093224, param_norm = 2.483, grad_norm = 0.428, train_F1/valid_F1 = 0.502000/0.412826\n",
      "181.38 epochs, iter 30800: train_loss = 1.017991781, param_norm = 2.393, grad_norm = 0.449, train_F1/valid_F1 = 0.510000/0.425852\n",
      "182.56 epochs, iter 31000: train_loss = 1.021686435, param_norm = 2.426, grad_norm = 0.609, train_F1/valid_F1 = 0.502000/0.422846\n",
      "Saved parameters to ./04-28-02/model.ckpt-31000\n",
      "183.73 epochs, iter 31200: train_loss = 1.015475512, param_norm = 2.463, grad_norm = 0.133, train_F1/valid_F1 = 0.505000/0.425852\n",
      "184.91 epochs, iter 31400: train_loss = 1.023759127, param_norm = 2.454, grad_norm = 0.219, train_F1/valid_F1 = 0.507000/0.406814\n",
      "186.09 epochs, iter 31600: train_loss = 1.031228065, param_norm = 2.870, grad_norm = 0.565, train_F1/valid_F1 = 0.508000/0.416834\n",
      "187.27 epochs, iter 31800: train_loss = 1.029007554, param_norm = 2.431, grad_norm = 1.051, train_F1/valid_F1 = 0.508000/0.401804\n",
      "188.45 epochs, iter 32000: train_loss = 1.031589866, param_norm = 2.535, grad_norm = 1.201, train_F1/valid_F1 = 0.506000/0.411824\n",
      "Saved parameters to ./04-28-02/model.ckpt-32000\n",
      "189.62 epochs, iter 32200: train_loss = 1.019113302, param_norm = 2.437, grad_norm = 0.656, train_F1/valid_F1 = 0.505000/0.427856\n",
      "190.80 epochs, iter 32400: train_loss = 1.023299575, param_norm = 2.476, grad_norm = 0.339, train_F1/valid_F1 = 0.500000/0.419840\n",
      "191.98 epochs, iter 32600: train_loss = 1.018411517, param_norm = 2.373, grad_norm = 0.245, train_F1/valid_F1 = 0.514000/0.405812\n",
      "193.16 epochs, iter 32800: train_loss = 1.026919723, param_norm = 2.417, grad_norm = 0.729, train_F1/valid_F1 = 0.502000/0.416834\n",
      "194.33 epochs, iter 33000: train_loss = 1.020168781, param_norm = 2.557, grad_norm = 0.380, train_F1/valid_F1 = 0.505000/0.410822\n",
      "Saved parameters to ./04-28-02/model.ckpt-33000\n",
      "195.51 epochs, iter 33200: train_loss = 1.024057627, param_norm = 2.451, grad_norm = 0.555, train_F1/valid_F1 = 0.503000/0.405812\n",
      "196.69 epochs, iter 33400: train_loss = 1.033332229, param_norm = 2.497, grad_norm = 1.184, train_F1/valid_F1 = 0.504000/0.417836\n",
      "197.87 epochs, iter 33600: train_loss = 1.028422236, param_norm = 2.513, grad_norm = 0.963, train_F1/valid_F1 = 0.495000/0.416834\n",
      "199.05 epochs, iter 33800: train_loss = 1.016943574, param_norm = 2.425, grad_norm = 0.249, train_F1/valid_F1 = 0.506000/0.422846\n",
      "200.22 epochs, iter 34000: train_loss = 1.027119756, param_norm = 2.440, grad_norm = 0.597, train_F1/valid_F1 = 0.508000/0.402806\n",
      "Saved parameters to ./04-28-02/model.ckpt-34000\n",
      "201.40 epochs, iter 34200: train_loss = 1.014022350, param_norm = 2.482, grad_norm = 0.631, train_F1/valid_F1 = 0.526000/0.424850\n"
     ]
    }
   ],
   "source": [
    "lstm_model.train(session=sess, experiment_name=experiment_name, keep_prob_val=0.9, beta=0.005)\n",
    "# IMPORTANT:\n",
    "# when you think F1 is not going to improve anymore, wait another 50-100 epochs. \n",
    "# if you see any better iteration that has not appeared before, keep waiting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from 04-28-02/best_ckpt/best.ckpt-180\n",
      "train F1: 0.264\n",
      "dev F1: 0.4198396793587174\n",
      "test F1: 0.3687374749498998\n"
     ]
    }
   ],
   "source": [
    "# load best checkpoint (based on dev f1) and evaluate\n",
    "ckpt = tf.train.get_checkpoint_state(experiment_name+'/best_ckpt')\n",
    "v2_path = ckpt.model_checkpoint_path + \".index\" if ckpt else \"\"\n",
    "if ckpt and (tf.gfile.Exists(ckpt.model_checkpoint_path) or tf.gfile.Exists(v2_path)):\n",
    "    lstm_model.saver.restore(sess, ckpt.model_checkpoint_path)\n",
    "else:\n",
    "    raise ValueError('What? you dont have a best checkpoint?')\n",
    "\n",
    "y_1000_train_pred = sess.run(lstm_model.preds, feed_dict={lstm_model.X: x_1000_train, lstm_model.keep_prob: 1, \\\n",
    "                                                          lstm_model.beta:0, lstm_model.is_training: False})\n",
    "print(\"train F1:\",F1(y_1000_train, y_1000_train_pred))\n",
    "y_valid_pred = sess.run(lstm_model.preds, feed_dict={lstm_model.X: x_valid, lstm_model.keep_prob: 1, \\\n",
    "                                                     lstm_model.beta:0, lstm_model.is_training: False})\n",
    "print(\"dev F1:\",F1(y_valid, y_valid_pred))\n",
    "y_test_pred = sess.run(lstm_model.preds, feed_dict={lstm_model.X: x_test, lstm_model.keep_prob: 1, \\\n",
    "                                                    lstm_model.beta:0, lstm_model.is_training: False})\n",
    "print(\"test F1:\",F1(y_test, y_test_pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
