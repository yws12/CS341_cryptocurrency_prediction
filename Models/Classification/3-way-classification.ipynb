{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yicheng/.local/lib/python3.5/site-packages/statsmodels/compat/pandas.py:56: FutureWarning: The pandas.core.datetools module is deprecated and will be removed in a future version. Please use the pandas.tseries module instead.\n",
      "  from pandas.core import datetools\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>USDT_BTC_high</th>\n",
       "      <th>USDT_BTC_low</th>\n",
       "      <th>USDT_BTC_close</th>\n",
       "      <th>USDT_BTC_open</th>\n",
       "      <th>USDT_BTC_volume</th>\n",
       "      <th>USDT_BTC_quoteVolume</th>\n",
       "      <th>USDT_BTC_weighted_mean</th>\n",
       "      <th>USDT_BTC_pctChange</th>\n",
       "      <th>USDT_ETH_high</th>\n",
       "      <th>USDT_ETH_low</th>\n",
       "      <th>...</th>\n",
       "      <th>BTC_LTC_weighted_mean</th>\n",
       "      <th>BTC_LTC_pctChange</th>\n",
       "      <th>BTC_XRP_high</th>\n",
       "      <th>BTC_XRP_low</th>\n",
       "      <th>BTC_XRP_close</th>\n",
       "      <th>BTC_XRP_open</th>\n",
       "      <th>BTC_XRP_volume</th>\n",
       "      <th>BTC_XRP_quoteVolume</th>\n",
       "      <th>BTC_XRP_weighted_mean</th>\n",
       "      <th>BTC_XRP_pctChange</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>time</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2016-01-02 12:00:00</th>\n",
       "      <td>432.5000</td>\n",
       "      <td>432.50</td>\n",
       "      <td>432.500000</td>\n",
       "      <td>432.50000</td>\n",
       "      <td>40.041239</td>\n",
       "      <td>0.092581</td>\n",
       "      <td>432.500000</td>\n",
       "      <td>2.220446e-16</td>\n",
       "      <td>0.959136</td>\n",
       "      <td>0.959136</td>\n",
       "      <td>...</td>\n",
       "      <td>0.008063</td>\n",
       "      <td>-0.002293</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.033605</td>\n",
       "      <td>2408.822942</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>-0.002859</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-01-02 13:00:00</th>\n",
       "      <td>432.5000</td>\n",
       "      <td>432.50</td>\n",
       "      <td>432.500000</td>\n",
       "      <td>432.50000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>432.986941</td>\n",
       "      <td>1.125876e-03</td>\n",
       "      <td>0.959136</td>\n",
       "      <td>0.959136</td>\n",
       "      <td>...</td>\n",
       "      <td>0.008060</td>\n",
       "      <td>-0.000333</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.004704</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-01-02 14:00:00</th>\n",
       "      <td>437.3635</td>\n",
       "      <td>432.48</td>\n",
       "      <td>433.336667</td>\n",
       "      <td>433.52799</td>\n",
       "      <td>359.269753</td>\n",
       "      <td>0.828819</td>\n",
       "      <td>433.473883</td>\n",
       "      <td>1.124610e-03</td>\n",
       "      <td>0.959136</td>\n",
       "      <td>0.957000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.008073</td>\n",
       "      <td>0.001623</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>1.141981</td>\n",
       "      <td>81071.098773</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.004682</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-01-02 15:00:00</th>\n",
       "      <td>432.4800</td>\n",
       "      <td>432.48</td>\n",
       "      <td>432.480000</td>\n",
       "      <td>432.48000</td>\n",
       "      <td>60.859598</td>\n",
       "      <td>0.140722</td>\n",
       "      <td>432.480000</td>\n",
       "      <td>-2.292832e-03</td>\n",
       "      <td>0.957000</td>\n",
       "      <td>0.957000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.008089</td>\n",
       "      <td>0.002002</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>2.120423</td>\n",
       "      <td>150622.792769</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>-0.000492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-01-02 16:00:00</th>\n",
       "      <td>432.4800</td>\n",
       "      <td>432.48</td>\n",
       "      <td>432.480000</td>\n",
       "      <td>432.48000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>432.350000</td>\n",
       "      <td>-3.005919e-04</td>\n",
       "      <td>0.957000</td>\n",
       "      <td>0.957000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.008079</td>\n",
       "      <td>-0.001224</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.491516</td>\n",
       "      <td>35178.793196</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>-0.007526</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 56 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                     USDT_BTC_high  USDT_BTC_low  USDT_BTC_close  \\\n",
       "time                                                               \n",
       "2016-01-02 12:00:00       432.5000        432.50      432.500000   \n",
       "2016-01-02 13:00:00       432.5000        432.50      432.500000   \n",
       "2016-01-02 14:00:00       437.3635        432.48      433.336667   \n",
       "2016-01-02 15:00:00       432.4800        432.48      432.480000   \n",
       "2016-01-02 16:00:00       432.4800        432.48      432.480000   \n",
       "\n",
       "                     USDT_BTC_open  USDT_BTC_volume  USDT_BTC_quoteVolume  \\\n",
       "time                                                                        \n",
       "2016-01-02 12:00:00      432.50000        40.041239              0.092581   \n",
       "2016-01-02 13:00:00      432.50000         0.000000              0.000000   \n",
       "2016-01-02 14:00:00      433.52799       359.269753              0.828819   \n",
       "2016-01-02 15:00:00      432.48000        60.859598              0.140722   \n",
       "2016-01-02 16:00:00      432.48000         0.000000              0.000000   \n",
       "\n",
       "                     USDT_BTC_weighted_mean  USDT_BTC_pctChange  \\\n",
       "time                                                              \n",
       "2016-01-02 12:00:00              432.500000        2.220446e-16   \n",
       "2016-01-02 13:00:00              432.986941        1.125876e-03   \n",
       "2016-01-02 14:00:00              433.473883        1.124610e-03   \n",
       "2016-01-02 15:00:00              432.480000       -2.292832e-03   \n",
       "2016-01-02 16:00:00              432.350000       -3.005919e-04   \n",
       "\n",
       "                     USDT_ETH_high  USDT_ETH_low        ...          \\\n",
       "time                                                    ...           \n",
       "2016-01-02 12:00:00       0.959136      0.959136        ...           \n",
       "2016-01-02 13:00:00       0.959136      0.959136        ...           \n",
       "2016-01-02 14:00:00       0.959136      0.957000        ...           \n",
       "2016-01-02 15:00:00       0.957000      0.957000        ...           \n",
       "2016-01-02 16:00:00       0.957000      0.957000        ...           \n",
       "\n",
       "                     BTC_LTC_weighted_mean  BTC_LTC_pctChange  BTC_XRP_high  \\\n",
       "time                                                                          \n",
       "2016-01-02 12:00:00               0.008063          -0.002293      0.000014   \n",
       "2016-01-02 13:00:00               0.008060          -0.000333      0.000014   \n",
       "2016-01-02 14:00:00               0.008073           0.001623      0.000014   \n",
       "2016-01-02 15:00:00               0.008089           0.002002      0.000014   \n",
       "2016-01-02 16:00:00               0.008079          -0.001224      0.000014   \n",
       "\n",
       "                     BTC_XRP_low  BTC_XRP_close  BTC_XRP_open  BTC_XRP_volume  \\\n",
       "time                                                                            \n",
       "2016-01-02 12:00:00     0.000014       0.000014      0.000014        0.033605   \n",
       "2016-01-02 13:00:00     0.000014       0.000014      0.000014        0.000000   \n",
       "2016-01-02 14:00:00     0.000014       0.000014      0.000014        1.141981   \n",
       "2016-01-02 15:00:00     0.000014       0.000014      0.000014        2.120423   \n",
       "2016-01-02 16:00:00     0.000014       0.000014      0.000014        0.491516   \n",
       "\n",
       "                     BTC_XRP_quoteVolume  BTC_XRP_weighted_mean  \\\n",
       "time                                                              \n",
       "2016-01-02 12:00:00          2408.822942               0.000014   \n",
       "2016-01-02 13:00:00             0.000000               0.000014   \n",
       "2016-01-02 14:00:00         81071.098773               0.000014   \n",
       "2016-01-02 15:00:00        150622.792769               0.000014   \n",
       "2016-01-02 16:00:00         35178.793196               0.000014   \n",
       "\n",
       "                     BTC_XRP_pctChange  \n",
       "time                                    \n",
       "2016-01-02 12:00:00          -0.002859  \n",
       "2016-01-02 13:00:00           0.004704  \n",
       "2016-01-02 14:00:00           0.004682  \n",
       "2016-01-02 15:00:00          -0.000492  \n",
       "2016-01-02 16:00:00          -0.007526  \n",
       "\n",
       "[5 rows x 56 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "__author__ = \"Yicheng Li\"\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import statsmodels.api as sm\n",
    "import random\n",
    "from sklearn import preprocessing\n",
    "import tensorflow as tf\n",
    "\n",
    "dir_path = 'CS341-repo/Data/'\n",
    "df = pd.read_pickle(dir_path+'df_hourly_poloniex.pickle')\n",
    "df = df.dropna()\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler_features = preprocessing.MinMaxScaler(feature_range=(0.1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to create train, validation, test data given sequence length\n",
    "def load_data(df, seq_len):\n",
    "    # prepare one-hot labels\n",
    "    labels = df['USDT_BTC_pctChange'].as_matrix().reshape([-1,1])\n",
    "    labels = np.concatenate([(labels > 3e-3)*1, ((3e-3 > labels)&(labels > -3e-3))*1, (labels < -3e-3)*1],1)\n",
    "    feature_set = [x for x in range(56)] #[0,1,2,3,4,6,7]\n",
    "    \n",
    "    data_raw = df.as_matrix() # convert to numpy array\n",
    "    # fit scaler\n",
    "    data_raw = scaler_features.fit_transform(data_raw[:, feature_set])\n",
    "    data = []\n",
    "    \n",
    "    # create all possible sequences of length seq_len\n",
    "    for index in range(len(data_raw) - seq_len + 1): \n",
    "        data.append(data_raw[index: index + seq_len, :])\n",
    "    \n",
    "    data = np.array(data)\n",
    "    \n",
    "    n_train_valid_pairs = 3\n",
    "    each_train_set_size_pct = 25\n",
    "    each_valid_set_size_pct = 5\n",
    "    \n",
    "    each_train_set_size = round(each_train_set_size_pct/100*data.shape[0])\n",
    "    each_valid_set_size = round(each_valid_set_size_pct/100*data.shape[0])\n",
    "    \n",
    "    x_train_sets = []\n",
    "    y_train_sets = []\n",
    "    x_valid_sets = []\n",
    "    y_valid_sets = []\n",
    "    used = 0\n",
    "    \n",
    "    for i in range(n_train_valid_pairs):\n",
    "        x_train_sets.append(data[used : used + each_train_set_size,:-1,:]) # cannot see last day, which we aim to predict\n",
    "        y_train_sets.append(labels[used + seq_len-1 : used + each_train_set_size + seq_len-1, :])\n",
    "        used += each_train_set_size\n",
    "    \n",
    "        x_valid_sets.append(data[used : used + each_valid_set_size,:-1,:])\n",
    "        y_valid_sets.append(labels[used + seq_len-1 : used + each_valid_set_size + seq_len-1, :])\n",
    "        used += each_valid_set_size\n",
    "    \n",
    "    x_test = data[used : , :-1, :]\n",
    "    y_test = labels[seq_len-1 + used : , :]\n",
    "    \n",
    "    x_train = np.concatenate(x_train_sets, axis=0)\n",
    "    y_train = np.concatenate(y_train_sets, axis=0)\n",
    "    x_valid = np.concatenate(x_valid_sets, axis=0)\n",
    "    y_valid = np.concatenate(y_valid_sets, axis=0)\n",
    "    \n",
    "    return [x_train, y_train, x_valid, y_valid, x_test, y_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train.shape =  (14982, 9, 56)\n",
      "y_train.shape =  (14982, 3)\n",
      "x_valid.shape =  (2997, 9, 56)\n",
      "y_valid.shape =  (2997, 3)\n",
      "x_test.shape =  (1996, 9, 56)\n",
      "y_test.shape =  (1996, 3)\n"
     ]
    }
   ],
   "source": [
    "# create train, test data\n",
    "seq_len = 10 # choose sequence length\n",
    "x_train, y_train, x_valid, y_valid, x_test, y_test = load_data(df, seq_len)\n",
    "# y_train = y_train.reshape([-1,1])\n",
    "# y_valid = y_valid.reshape([-1,1])\n",
    "# y_test = y_test.reshape([-1,1])\n",
    "print('x_train.shape = ',x_train.shape)\n",
    "print('y_train.shape = ', y_train.shape)\n",
    "print('x_valid.shape = ',x_valid.shape)\n",
    "print('y_valid.shape = ', y_valid.shape)\n",
    "print('x_test.shape = ', x_test.shape)\n",
    "print('y_test.shape = ',y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f1_score_single(y_true, y_pred):\n",
    "    TP = y_true.dot(y_pred) # zero or one\n",
    "    FP = np.sum(y_pred > y_true) # sum over all k classes, zero or one\n",
    "    FN = np.sum(y_pred < y_true) # sum over all k classes, zero or one\n",
    "    \n",
    "    if TP == 0: return 0.\n",
    "    p = 1. * TP / (TP + FP)\n",
    "    r = 1. * TP / (TP + FN)\n",
    "    return 2 * p * r / (p + r)\n",
    "    \n",
    "def F1(y_true, y_pred):\n",
    "    return np.mean([f1_score_single(x, y) for x, y in zip(y_true, y_pred)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "baseline dev_F1= 0.49345857094934586\n",
      "baseline test_F1= 0.3708542713567839\n"
     ]
    }
   ],
   "source": [
    "y_pred = np.roll(y_valid,1, axis=0)\n",
    "print('baseline dev_F1=',F1(y_valid[1:], y_pred[1:]))\n",
    "y_pred = np.roll(y_test,1, axis=0)\n",
    "y_pred[0] = y_valid[-1] # be careful here\n",
    "print('baseline test_F1=',F1(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_in_epoch = 0;\n",
    "perm_array  = np.arange(x_train.shape[0])\n",
    "np.random.shuffle(perm_array)\n",
    "\n",
    "# function to get the next batch\n",
    "def get_next_batch(batch_size):\n",
    "    global index_in_epoch, x_train, perm_array   \n",
    "    start = index_in_epoch\n",
    "    index_in_epoch += batch_size\n",
    "    \n",
    "    if index_in_epoch > x_train.shape[0]:\n",
    "        np.random.shuffle(perm_array) # shuffle permutation array\n",
    "        start = 0 # start next epoch\n",
    "        index_in_epoch = batch_size\n",
    "        \n",
    "    end = index_in_epoch\n",
    "    return x_train[perm_array[start:end]], y_train[perm_array[start:end]]\n",
    "\n",
    "x_1000_train, y_1000_train = get_next_batch(1000) # special batch of 1000 records in training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM_Model(object):\n",
    "    def __init__(self, seq_len):\n",
    "        # parameters\n",
    "        self.n_steps = seq_len-1 \n",
    "        self.n_inputs = x_train.shape[-1]\n",
    "        self.n_neurons = 100  # cell.state_size\n",
    "        self.n_bins = 3 # be careful if you want to change this\n",
    "        self.n_layers = 2\n",
    "        self.batch_size = 100\n",
    "        self.n_epochs = 0 # 0 means to train indefinitely\n",
    "        self.train_set_size = x_train.shape[0]\n",
    "        self.test_set_size = x_test.shape[0]\n",
    "        self.keep_prob = 0.9\n",
    "        self.max_gradient_norm = 5\n",
    "        with tf.variable_scope(\"LSTM_Model\", initializer=tf.contrib.layers.xavier_initializer()):\n",
    "            self.X = tf.placeholder(tf.float32, [None, self.n_steps, self.n_inputs])\n",
    "            self.y = tf.placeholder(tf.float32, [None, self.n_bins])\n",
    "\n",
    "            layers = [tf.contrib.rnn.LSTMCell(num_units=self.n_neurons, \\\n",
    "                                              initializer=tf.contrib.layers.xavier_initializer(), \\\n",
    "                                              activation=tf.nn.elu)\n",
    "                     for layer in range(self.n_layers)]\n",
    "            \n",
    "#             layers = [tf.contrib.rnn.BasicRNNCell(num_units=self.n_neurons, \\\n",
    "#                                               activation=tf.nn.elu)\n",
    "#                      for layer in range(self.n_layers)]\n",
    "            \n",
    "            multi_layer_cell = tf.contrib.rnn.MultiRNNCell(layers)\n",
    "\n",
    "            outputs, states = tf.nn.dynamic_rnn(multi_layer_cell, self.X, dtype=tf.float32)\n",
    "            outputs = tf.nn.dropout(outputs, self.keep_prob)\n",
    "            # 'outputs' is a tensor of shape [batch_size, n_steps, n_neurons(cell.state_size)]\n",
    "        \n",
    "#             directly output\n",
    "#             logits = outputs[:,self.n_steps-1,:] # keep only last output of sequence\n",
    "            \n",
    "            # ======== attn layer ===================\n",
    "#             sim_mat = tf.matmul(outputs, tf.transpose(outputs, perm=[0,2,1]))\n",
    "#             attn_dist = tf.nn.softmax(sim_mat, 2)\n",
    "#             # (batchsize, n_steps, n_steps)\n",
    "#             attn_outputs = tf.matmul(attn_dist, outputs)\n",
    "#             # (batchsize, n_steps, n_bins)\n",
    "            # ========================================\n",
    "            \n",
    "            stacked_outputs = tf.reshape(outputs, [-1, self.n_neurons]) \n",
    "            stacked_outputs = tf.layers.dense(stacked_outputs, self.n_bins)\n",
    "            final_outputs = tf.reshape(stacked_outputs, [-1, self.n_steps, self.n_bins])\n",
    "            \n",
    "            self.final_logits = final_outputs[:, -1, :] # last timestep\n",
    "            # (batchsize, n_bins), this is logits, not probs!!\n",
    "            self.indices = tf.argmax(self.final_logits, axis=-1) # (batchsize, 1)\n",
    "            self.preds = tf.one_hot(self.indices, depth=self.n_bins)\n",
    "            \n",
    "            self.each_loss = tf.nn.softmax_cross_entropy_with_logits(logits=self.final_logits, labels=self.y)\n",
    "            self.loss = tf.reduce_mean(self.each_loss) \n",
    "\n",
    "            params = tf.trainable_variables()\n",
    "            gradients = tf.gradients(self.loss, params)\n",
    "            self.gradient_norm = tf.global_norm(gradients)\n",
    "            clipped_gradients, _ = tf.clip_by_global_norm(gradients, self.max_gradient_norm)\n",
    "            clipped_norm = tf.global_norm(clipped_gradients)\n",
    "            self.param_norm = tf.global_norm(params)\n",
    "            self.learning_rate_placeholder = tf.placeholder(tf.float32, [], name='learning_rate')\n",
    "            optimizer = tf.train.RMSPropOptimizer(learning_rate=self.learning_rate_placeholder) \n",
    "            # training_op = optimizer.minimize(loss)\n",
    "            self.training_op = optimizer.apply_gradients(zip(clipped_gradients, params))\n",
    "\n",
    "            # initialize parameters\n",
    "#             sess = tf.Session()\n",
    "            self.global_step = tf.Variable(0, name='global_step', trainable=False)\n",
    "            self.saver = tf.train.Saver(max_to_keep=2)\n",
    "    \n",
    "    def train(self, session, experiment_name):\n",
    "        ckpt = tf.train.get_checkpoint_state(experiment_name)\n",
    "        v2_path = ckpt.model_checkpoint_path + \".index\" if ckpt else \"\"\n",
    "        if ckpt and (tf.gfile.Exists(ckpt.model_checkpoint_path) or tf.gfile.Exists(v2_path)):\n",
    "            self.saver.restore(sess, ckpt.model_checkpoint_path)\n",
    "            iteration = self.global_step.eval(session=sess) # get last global_step\n",
    "            print(\"Start from iteration:\", iteration)\n",
    "            lr = 1e-3\n",
    "        else:\n",
    "            print('There is not saved parameters. Creating model with fresh parameters.')\n",
    "            sess.run(tf.global_variables_initializer())\n",
    "            iteration = 0\n",
    "            lr = 1e-2 # should fix this...\n",
    "\n",
    "        old_loss = 1000\n",
    "        \n",
    "        while self.n_epochs == 0 or iteration*self.batch_size/self.train_set_size < self.n_epochs:\n",
    "            iteration = iteration + 1\n",
    "            x_batch, y_batch = get_next_batch(self.batch_size) # fetch the next training batch \n",
    "\n",
    "            # train on this batch\n",
    "            sess.run(self.training_op, feed_dict={self.X: x_batch, self.y: y_batch, self.learning_rate_placeholder:lr})\n",
    "\n",
    "            if iteration % 20 == 0:\n",
    "                y_1000_train_pred, loss_val, param_norm_val, grad_norm_val = sess.run([self.preds, self.loss, self.param_norm, self.gradient_norm],\\\n",
    "                                                                        feed_dict={self.X: x_1000_train, self.y:y_1000_train, \\\n",
    "                                                                                  self.learning_rate_placeholder:lr})\n",
    "                if loss_val > old_loss * 1.2:\n",
    "                    lr /= 2\n",
    "                old_loss = loss_val\n",
    "\n",
    "                y_valid_pred = sess.run(self.preds, feed_dict={self.X: x_valid})\n",
    "                print('%.2f epochs, iter %d: train_loss = %.9f, param_norm = %.3f, grad_norm = %.3f, train_F1/valid_F1 = %.6f/%.6f' \\\n",
    "                      %(iteration*self.batch_size/self.train_set_size, iteration, loss_val, param_norm_val, grad_norm_val, \\\n",
    "                        F1(y_1000_train, y_1000_train_pred), \\\n",
    "                        F1(y_valid, y_valid_pred)))\n",
    "        #             print('%.2f epochs: loss train/valid = %.6f/%.6f'%(\n",
    "        #                 iteration*batch_size/train_set_size, loss_train, loss_valid))\n",
    "            if iteration % 100 == 0:\n",
    "                self.global_step.assign(iteration).eval(session=sess) # set and update(eval) global_step with index, i\n",
    "                save_path = self.saver.save(sess, \"./\"+experiment_name+\"/model.ckpt\", global_step=self.global_step)\n",
    "                print('Saved parameters to %s' % save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## run experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_name=\"all_features_10_seq_100hidden_2layer\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "lstm_model = LSTM_Model(seq_len=seq_len)\n",
    "sess = tf.Session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from all_features_10_seq_100hidden_2layer/model.ckpt-12700\n",
      "Start from iteration: 12700\n",
      "84.90 epochs, iter 12720: train_loss = 0.893858790, param_norm = 150.472, grad_norm = 0.085, train_F1/valid_F1 = 0.585000/0.523190\n",
      "85.04 epochs, iter 12740: train_loss = 0.897749186, param_norm = 150.496, grad_norm = 0.088, train_F1/valid_F1 = 0.575000/0.518519\n",
      "85.17 epochs, iter 12760: train_loss = 0.904023051, param_norm = 150.479, grad_norm = 0.073, train_F1/valid_F1 = 0.567000/0.518852\n",
      "85.30 epochs, iter 12780: train_loss = 0.896291614, param_norm = 150.487, grad_norm = 0.072, train_F1/valid_F1 = 0.572000/0.535202\n",
      "85.44 epochs, iter 12800: train_loss = 0.899986863, param_norm = 150.506, grad_norm = 0.083, train_F1/valid_F1 = 0.566000/0.523524\n",
      "Saved parameters to ./all_features_10_seq_100hidden_2layer/model.ckpt-12800\n",
      "85.57 epochs, iter 12820: train_loss = 0.896343887, param_norm = 150.542, grad_norm = 0.082, train_F1/valid_F1 = 0.575000/0.526527\n",
      "85.70 epochs, iter 12840: train_loss = 0.897523463, param_norm = 150.543, grad_norm = 0.085, train_F1/valid_F1 = 0.572000/0.532533\n",
      "85.84 epochs, iter 12860: train_loss = 0.896019578, param_norm = 150.555, grad_norm = 0.089, train_F1/valid_F1 = 0.575000/0.534535\n",
      "85.97 epochs, iter 12880: train_loss = 0.896073878, param_norm = 150.561, grad_norm = 0.089, train_F1/valid_F1 = 0.570000/0.523524\n",
      "86.10 epochs, iter 12900: train_loss = 0.892472386, param_norm = 150.569, grad_norm = 0.089, train_F1/valid_F1 = 0.570000/0.524191\n",
      "Saved parameters to ./all_features_10_seq_100hidden_2layer/model.ckpt-12900\n",
      "86.24 epochs, iter 12920: train_loss = 0.893338144, param_norm = 150.591, grad_norm = 0.086, train_F1/valid_F1 = 0.579000/0.515182\n",
      "86.37 epochs, iter 12940: train_loss = 0.928623319, param_norm = 150.594, grad_norm = 4.049, train_F1/valid_F1 = 0.574000/0.514181\n",
      "86.50 epochs, iter 12960: train_loss = 0.894952178, param_norm = 150.609, grad_norm = 0.087, train_F1/valid_F1 = 0.575000/0.519186\n",
      "86.64 epochs, iter 12980: train_loss = 0.896568954, param_norm = 150.617, grad_norm = 0.097, train_F1/valid_F1 = 0.578000/0.524191\n",
      "86.77 epochs, iter 13000: train_loss = 0.901921093, param_norm = 150.616, grad_norm = 0.095, train_F1/valid_F1 = 0.568000/0.513847\n",
      "Saved parameters to ./all_features_10_seq_100hidden_2layer/model.ckpt-13000\n",
      "86.90 epochs, iter 13020: train_loss = 0.896878660, param_norm = 150.636, grad_norm = 0.102, train_F1/valid_F1 = 0.575000/0.530197\n",
      "87.04 epochs, iter 13040: train_loss = 0.898377419, param_norm = 150.649, grad_norm = 0.106, train_F1/valid_F1 = 0.575000/0.527194\n",
      "87.17 epochs, iter 13060: train_loss = 0.891652286, param_norm = 150.677, grad_norm = 0.078, train_F1/valid_F1 = 0.581000/0.520187\n",
      "87.30 epochs, iter 13080: train_loss = 0.900058091, param_norm = 150.698, grad_norm = 0.102, train_F1/valid_F1 = 0.571000/0.522856\n",
      "87.44 epochs, iter 13100: train_loss = 0.894281387, param_norm = 150.730, grad_norm = 0.105, train_F1/valid_F1 = 0.574000/0.522856\n",
      "Saved parameters to ./all_features_10_seq_100hidden_2layer/model.ckpt-13100\n",
      "87.57 epochs, iter 13120: train_loss = 0.893184423, param_norm = 150.754, grad_norm = 0.117, train_F1/valid_F1 = 0.578000/0.516850\n",
      "87.71 epochs, iter 13140: train_loss = 0.897583723, param_norm = 150.752, grad_norm = 0.184, train_F1/valid_F1 = 0.582000/0.522523\n",
      "87.84 epochs, iter 13160: train_loss = 0.900354624, param_norm = 150.762, grad_norm = 0.372, train_F1/valid_F1 = 0.571000/0.524525\n",
      "87.97 epochs, iter 13180: train_loss = 0.902975440, param_norm = 150.745, grad_norm = 3.397, train_F1/valid_F1 = 0.576000/0.530531\n",
      "88.11 epochs, iter 13200: train_loss = 0.898761690, param_norm = 150.765, grad_norm = 0.113, train_F1/valid_F1 = 0.578000/0.510511\n",
      "Saved parameters to ./all_features_10_seq_100hidden_2layer/model.ckpt-13200\n",
      "88.24 epochs, iter 13220: train_loss = 0.896999061, param_norm = 150.777, grad_norm = 0.092, train_F1/valid_F1 = 0.577000/0.522856\n",
      "88.37 epochs, iter 13240: train_loss = 0.904053807, param_norm = 150.794, grad_norm = 0.306, train_F1/valid_F1 = 0.565000/0.524858\n",
      "88.51 epochs, iter 13260: train_loss = 0.902053475, param_norm = 150.799, grad_norm = 0.569, train_F1/valid_F1 = 0.566000/0.528195\n",
      "88.64 epochs, iter 13280: train_loss = 0.897820830, param_norm = 150.808, grad_norm = 0.599, train_F1/valid_F1 = 0.571000/0.518185\n",
      "88.77 epochs, iter 13300: train_loss = 0.893748820, param_norm = 150.826, grad_norm = 0.128, train_F1/valid_F1 = 0.590000/0.518519\n",
      "Saved parameters to ./all_features_10_seq_100hidden_2layer/model.ckpt-13300\n",
      "88.91 epochs, iter 13320: train_loss = 0.890742540, param_norm = 150.844, grad_norm = 0.298, train_F1/valid_F1 = 0.579000/0.516183\n",
      "89.04 epochs, iter 13340: train_loss = 0.900113404, param_norm = 150.852, grad_norm = 0.360, train_F1/valid_F1 = 0.576000/0.515516\n",
      "89.17 epochs, iter 13360: train_loss = 0.900354505, param_norm = 150.871, grad_norm = 0.473, train_F1/valid_F1 = 0.572000/0.528195\n",
      "89.31 epochs, iter 13380: train_loss = 0.894038320, param_norm = 150.887, grad_norm = 0.121, train_F1/valid_F1 = 0.582000/0.524191\n",
      "89.44 epochs, iter 13400: train_loss = 0.891880155, param_norm = 150.906, grad_norm = 0.083, train_F1/valid_F1 = 0.585000/0.519853\n",
      "Saved parameters to ./all_features_10_seq_100hidden_2layer/model.ckpt-13400\n",
      "89.57 epochs, iter 13420: train_loss = 0.897337973, param_norm = 150.906, grad_norm = 0.087, train_F1/valid_F1 = 0.576000/0.518185\n",
      "89.71 epochs, iter 13440: train_loss = 0.900811434, param_norm = 150.913, grad_norm = 0.976, train_F1/valid_F1 = 0.566000/0.528195\n",
      "89.84 epochs, iter 13460: train_loss = 0.905552804, param_norm = 150.909, grad_norm = 0.449, train_F1/valid_F1 = 0.570000/0.520854\n",
      "89.97 epochs, iter 13480: train_loss = 0.893430173, param_norm = 150.925, grad_norm = 0.092, train_F1/valid_F1 = 0.573000/0.515849\n",
      "90.11 epochs, iter 13500: train_loss = 0.895585299, param_norm = 150.950, grad_norm = 0.309, train_F1/valid_F1 = 0.573000/0.515516\n",
      "Saved parameters to ./all_features_10_seq_100hidden_2layer/model.ckpt-13500\n",
      "90.24 epochs, iter 13520: train_loss = 0.899634480, param_norm = 150.955, grad_norm = 0.307, train_F1/valid_F1 = 0.566000/0.531865\n",
      "90.38 epochs, iter 13540: train_loss = 0.897396624, param_norm = 150.963, grad_norm = 0.523, train_F1/valid_F1 = 0.576000/0.525526\n",
      "90.51 epochs, iter 13560: train_loss = 0.896473289, param_norm = 150.988, grad_norm = 0.080, train_F1/valid_F1 = 0.573000/0.523857\n",
      "90.64 epochs, iter 13580: train_loss = 0.900322497, param_norm = 150.984, grad_norm = 0.253, train_F1/valid_F1 = 0.574000/0.535869\n",
      "90.78 epochs, iter 13600: train_loss = 0.892324567, param_norm = 151.012, grad_norm = 0.620, train_F1/valid_F1 = 0.584000/0.514181\n",
      "Saved parameters to ./all_features_10_seq_100hidden_2layer/model.ckpt-13600\n",
      "90.91 epochs, iter 13620: train_loss = 0.892518818, param_norm = 151.007, grad_norm = 0.129, train_F1/valid_F1 = 0.589000/0.522856\n",
      "91.04 epochs, iter 13640: train_loss = 0.901305079, param_norm = 151.022, grad_norm = 0.172, train_F1/valid_F1 = 0.573000/0.515849\n",
      "91.18 epochs, iter 13660: train_loss = 0.895175397, param_norm = 151.004, grad_norm = 0.113, train_F1/valid_F1 = 0.582000/0.531532\n",
      "91.31 epochs, iter 13680: train_loss = 0.892109156, param_norm = 151.021, grad_norm = 0.081, train_F1/valid_F1 = 0.574000/0.529530\n",
      "91.44 epochs, iter 13700: train_loss = 0.900526106, param_norm = 151.022, grad_norm = 0.203, train_F1/valid_F1 = 0.583000/0.530864\n",
      "Saved parameters to ./all_features_10_seq_100hidden_2layer/model.ckpt-13700\n",
      "91.58 epochs, iter 13720: train_loss = 0.893840492, param_norm = 151.029, grad_norm = 0.095, train_F1/valid_F1 = 0.575000/0.521522\n",
      "91.71 epochs, iter 13740: train_loss = 0.895978391, param_norm = 151.054, grad_norm = 0.106, train_F1/valid_F1 = 0.573000/0.519520\n",
      "91.84 epochs, iter 13760: train_loss = 0.897801161, param_norm = 151.047, grad_norm = 0.095, train_F1/valid_F1 = 0.564000/0.520521\n",
      "91.98 epochs, iter 13780: train_loss = 0.899635851, param_norm = 151.053, grad_norm = 0.221, train_F1/valid_F1 = 0.571000/0.529196\n",
      "92.11 epochs, iter 13800: train_loss = 0.898813188, param_norm = 151.075, grad_norm = 0.150, train_F1/valid_F1 = 0.576000/0.528862\n",
      "Saved parameters to ./all_features_10_seq_100hidden_2layer/model.ckpt-13800\n",
      "92.24 epochs, iter 13820: train_loss = 0.896306992, param_norm = 151.090, grad_norm = 0.158, train_F1/valid_F1 = 0.573000/0.521855\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "92.38 epochs, iter 13840: train_loss = 0.899423361, param_norm = 151.113, grad_norm = 0.094, train_F1/valid_F1 = 0.573000/0.529530\n",
      "92.51 epochs, iter 13860: train_loss = 0.907877386, param_norm = 151.127, grad_norm = 0.444, train_F1/valid_F1 = 0.581000/0.518185\n",
      "92.64 epochs, iter 13880: train_loss = 0.893178105, param_norm = 151.133, grad_norm = 0.126, train_F1/valid_F1 = 0.580000/0.528195\n",
      "92.78 epochs, iter 13900: train_loss = 0.896203220, param_norm = 151.129, grad_norm = 0.276, train_F1/valid_F1 = 0.573000/0.522856\n",
      "Saved parameters to ./all_features_10_seq_100hidden_2layer/model.ckpt-13900\n",
      "92.91 epochs, iter 13920: train_loss = 0.900009394, param_norm = 151.147, grad_norm = 0.096, train_F1/valid_F1 = 0.568000/0.519520\n",
      "93.04 epochs, iter 13940: train_loss = 0.894051433, param_norm = 151.171, grad_norm = 0.123, train_F1/valid_F1 = 0.577000/0.519520\n",
      "93.18 epochs, iter 13960: train_loss = 0.900637329, param_norm = 151.192, grad_norm = 0.332, train_F1/valid_F1 = 0.578000/0.526527\n",
      "93.31 epochs, iter 13980: train_loss = 0.895261228, param_norm = 151.191, grad_norm = 0.114, train_F1/valid_F1 = 0.578000/0.523190\n",
      "93.45 epochs, iter 14000: train_loss = 0.900723636, param_norm = 151.210, grad_norm = 0.122, train_F1/valid_F1 = 0.571000/0.534535\n",
      "Saved parameters to ./all_features_10_seq_100hidden_2layer/model.ckpt-14000\n",
      "93.58 epochs, iter 14020: train_loss = 0.888451576, param_norm = 151.237, grad_norm = 0.084, train_F1/valid_F1 = 0.579000/0.533867\n",
      "93.71 epochs, iter 14040: train_loss = 0.896950901, param_norm = 151.240, grad_norm = 0.134, train_F1/valid_F1 = 0.581000/0.514181\n",
      "93.85 epochs, iter 14060: train_loss = 0.895530283, param_norm = 151.244, grad_norm = 0.285, train_F1/valid_F1 = 0.582000/0.516517\n",
      "93.98 epochs, iter 14080: train_loss = 0.897610605, param_norm = 151.247, grad_norm = 0.404, train_F1/valid_F1 = 0.577000/0.511845\n",
      "94.11 epochs, iter 14100: train_loss = 0.901748121, param_norm = 151.279, grad_norm = 0.509, train_F1/valid_F1 = 0.576000/0.524858\n",
      "Saved parameters to ./all_features_10_seq_100hidden_2layer/model.ckpt-14100\n",
      "94.25 epochs, iter 14120: train_loss = 0.890911520, param_norm = 151.292, grad_norm = 0.101, train_F1/valid_F1 = 0.584000/0.523524\n",
      "94.38 epochs, iter 14140: train_loss = 0.887042582, param_norm = 151.309, grad_norm = 0.101, train_F1/valid_F1 = 0.587000/0.524191\n",
      "94.51 epochs, iter 14160: train_loss = 0.888394773, param_norm = 151.308, grad_norm = 0.113, train_F1/valid_F1 = 0.577000/0.515516\n",
      "94.65 epochs, iter 14180: train_loss = 0.889171004, param_norm = 151.307, grad_norm = 0.098, train_F1/valid_F1 = 0.585000/0.530531\n",
      "94.78 epochs, iter 14200: train_loss = 0.892751575, param_norm = 151.324, grad_norm = 0.173, train_F1/valid_F1 = 0.582000/0.526193\n",
      "Saved parameters to ./all_features_10_seq_100hidden_2layer/model.ckpt-14200\n",
      "94.91 epochs, iter 14220: train_loss = 0.890948236, param_norm = 151.337, grad_norm = 0.120, train_F1/valid_F1 = 0.577000/0.514515\n",
      "95.05 epochs, iter 14240: train_loss = 0.891972125, param_norm = 151.366, grad_norm = 0.088, train_F1/valid_F1 = 0.581000/0.523524\n",
      "95.18 epochs, iter 14260: train_loss = 0.888683856, param_norm = 151.386, grad_norm = 0.100, train_F1/valid_F1 = 0.587000/0.523524\n",
      "95.31 epochs, iter 14280: train_loss = 0.893681645, param_norm = 151.408, grad_norm = 0.140, train_F1/valid_F1 = 0.573000/0.523857\n",
      "95.45 epochs, iter 14300: train_loss = 0.891481459, param_norm = 151.403, grad_norm = 0.111, train_F1/valid_F1 = 0.581000/0.522523\n",
      "Saved parameters to ./all_features_10_seq_100hidden_2layer/model.ckpt-14300\n",
      "95.58 epochs, iter 14320: train_loss = 0.897155881, param_norm = 151.420, grad_norm = 0.112, train_F1/valid_F1 = 0.582000/0.527194\n",
      "95.71 epochs, iter 14340: train_loss = 0.895516574, param_norm = 151.430, grad_norm = 0.185, train_F1/valid_F1 = 0.576000/0.518185\n",
      "95.85 epochs, iter 14360: train_loss = 0.895083129, param_norm = 151.440, grad_norm = 0.240, train_F1/valid_F1 = 0.574000/0.526860\n",
      "95.98 epochs, iter 14380: train_loss = 0.892550707, param_norm = 151.450, grad_norm = 0.128, train_F1/valid_F1 = 0.576000/0.523190\n",
      "96.12 epochs, iter 14400: train_loss = 0.891212285, param_norm = 151.458, grad_norm = 0.120, train_F1/valid_F1 = 0.586000/0.520187\n",
      "Saved parameters to ./all_features_10_seq_100hidden_2layer/model.ckpt-14400\n",
      "96.25 epochs, iter 14420: train_loss = 0.893165767, param_norm = 151.459, grad_norm = 0.127, train_F1/valid_F1 = 0.572000/0.521188\n",
      "96.38 epochs, iter 14440: train_loss = 0.893650174, param_norm = 151.480, grad_norm = 0.121, train_F1/valid_F1 = 0.588000/0.518185\n",
      "96.52 epochs, iter 14460: train_loss = 0.888276219, param_norm = 151.467, grad_norm = 0.118, train_F1/valid_F1 = 0.585000/0.515849\n",
      "96.65 epochs, iter 14480: train_loss = 0.890716672, param_norm = 151.480, grad_norm = 0.106, train_F1/valid_F1 = 0.577000/0.514181\n",
      "96.78 epochs, iter 14500: train_loss = 0.889816523, param_norm = 151.487, grad_norm = 0.106, train_F1/valid_F1 = 0.587000/0.514181\n",
      "Saved parameters to ./all_features_10_seq_100hidden_2layer/model.ckpt-14500\n",
      "96.92 epochs, iter 14520: train_loss = 0.890847087, param_norm = 151.501, grad_norm = 0.141, train_F1/valid_F1 = 0.582000/0.517518\n",
      "97.05 epochs, iter 14540: train_loss = 0.887409925, param_norm = 151.503, grad_norm = 0.106, train_F1/valid_F1 = 0.588000/0.515182\n",
      "97.18 epochs, iter 14560: train_loss = 0.900921702, param_norm = 151.531, grad_norm = 0.137, train_F1/valid_F1 = 0.574000/0.523524\n",
      "97.32 epochs, iter 14580: train_loss = 0.898218513, param_norm = 151.540, grad_norm = 0.296, train_F1/valid_F1 = 0.569000/0.525526\n",
      "97.45 epochs, iter 14600: train_loss = 0.901234508, param_norm = 151.566, grad_norm = 0.608, train_F1/valid_F1 = 0.578000/0.527528\n",
      "Saved parameters to ./all_features_10_seq_100hidden_2layer/model.ckpt-14600\n",
      "97.58 epochs, iter 14620: train_loss = 0.891529500, param_norm = 151.565, grad_norm = 0.131, train_F1/valid_F1 = 0.573000/0.519186\n",
      "97.72 epochs, iter 14640: train_loss = 0.895222843, param_norm = 151.563, grad_norm = 0.141, train_F1/valid_F1 = 0.572000/0.520521\n",
      "97.85 epochs, iter 14660: train_loss = 0.889177024, param_norm = 151.574, grad_norm = 0.143, train_F1/valid_F1 = 0.580000/0.520854\n",
      "97.98 epochs, iter 14680: train_loss = 0.890282691, param_norm = 151.598, grad_norm = 0.230, train_F1/valid_F1 = 0.584000/0.522189\n",
      "98.12 epochs, iter 14700: train_loss = 0.892740667, param_norm = 151.612, grad_norm = 0.362, train_F1/valid_F1 = 0.574000/0.521522\n",
      "Saved parameters to ./all_features_10_seq_100hidden_2layer/model.ckpt-14700\n",
      "98.25 epochs, iter 14720: train_loss = 0.884956956, param_norm = 151.634, grad_norm = 1.644, train_F1/valid_F1 = 0.580000/0.518852\n",
      "98.38 epochs, iter 14740: train_loss = 0.884161353, param_norm = 151.656, grad_norm = 0.242, train_F1/valid_F1 = 0.579000/0.510177\n",
      "98.52 epochs, iter 14760: train_loss = 0.924331069, param_norm = 151.669, grad_norm = 4.409, train_F1/valid_F1 = 0.582000/0.505839\n",
      "98.65 epochs, iter 14780: train_loss = 0.888997734, param_norm = 151.659, grad_norm = 0.107, train_F1/valid_F1 = 0.580000/0.520521\n",
      "98.79 epochs, iter 14800: train_loss = 0.895017326, param_norm = 151.665, grad_norm = 0.140, train_F1/valid_F1 = 0.570000/0.525859\n",
      "Saved parameters to ./all_features_10_seq_100hidden_2layer/model.ckpt-14800\n",
      "98.92 epochs, iter 14820: train_loss = 0.889590979, param_norm = 151.666, grad_norm = 0.147, train_F1/valid_F1 = 0.574000/0.525192\n",
      "99.05 epochs, iter 14840: train_loss = 0.887160659, param_norm = 151.670, grad_norm = 0.149, train_F1/valid_F1 = 0.574000/0.511845\n",
      "99.19 epochs, iter 14860: train_loss = 0.896599412, param_norm = 151.688, grad_norm = 0.124, train_F1/valid_F1 = 0.575000/0.511845\n",
      "99.32 epochs, iter 14880: train_loss = 0.892405093, param_norm = 151.701, grad_norm = 0.247, train_F1/valid_F1 = 0.577000/0.530864\n",
      "99.45 epochs, iter 14900: train_loss = 0.886477530, param_norm = 151.713, grad_norm = 0.117, train_F1/valid_F1 = 0.580000/0.520854\n",
      "Saved parameters to ./all_features_10_seq_100hidden_2layer/model.ckpt-14900\n",
      "99.59 epochs, iter 14920: train_loss = 0.891503572, param_norm = 151.740, grad_norm = 0.144, train_F1/valid_F1 = 0.574000/0.517184\n",
      "99.72 epochs, iter 14940: train_loss = 0.891775727, param_norm = 151.732, grad_norm = 0.114, train_F1/valid_F1 = 0.574000/0.512513\n",
      "99.85 epochs, iter 14960: train_loss = 0.892516673, param_norm = 151.753, grad_norm = 0.287, train_F1/valid_F1 = 0.579000/0.516850\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99.99 epochs, iter 14980: train_loss = 0.890790284, param_norm = 151.770, grad_norm = 0.106, train_F1/valid_F1 = 0.576000/0.521522\n",
      "100.12 epochs, iter 15000: train_loss = 0.897155821, param_norm = 151.778, grad_norm = 0.777, train_F1/valid_F1 = 0.580000/0.521522\n",
      "Saved parameters to ./all_features_10_seq_100hidden_2layer/model.ckpt-15000\n",
      "100.25 epochs, iter 15020: train_loss = 0.898068488, param_norm = 151.780, grad_norm = 0.216, train_F1/valid_F1 = 0.572000/0.519520\n",
      "100.39 epochs, iter 15040: train_loss = 0.890522659, param_norm = 151.798, grad_norm = 0.137, train_F1/valid_F1 = 0.572000/0.520854\n",
      "100.52 epochs, iter 15060: train_loss = 0.886653364, param_norm = 151.808, grad_norm = 0.097, train_F1/valid_F1 = 0.585000/0.511845\n",
      "100.65 epochs, iter 15080: train_loss = 0.890757263, param_norm = 151.814, grad_norm = 0.129, train_F1/valid_F1 = 0.578000/0.518519\n",
      "100.79 epochs, iter 15100: train_loss = 0.890950918, param_norm = 151.861, grad_norm = 0.195, train_F1/valid_F1 = 0.581000/0.519520\n",
      "Saved parameters to ./all_features_10_seq_100hidden_2layer/model.ckpt-15100\n",
      "100.92 epochs, iter 15120: train_loss = 0.884665310, param_norm = 151.873, grad_norm = 0.161, train_F1/valid_F1 = 0.575000/0.519520\n",
      "101.05 epochs, iter 15140: train_loss = 0.885057867, param_norm = 151.901, grad_norm = 0.145, train_F1/valid_F1 = 0.584000/0.512179\n",
      "101.19 epochs, iter 15160: train_loss = 0.900661886, param_norm = 151.890, grad_norm = 0.384, train_F1/valid_F1 = 0.579000/0.518185\n",
      "101.32 epochs, iter 15180: train_loss = 0.887953222, param_norm = 151.908, grad_norm = 0.248, train_F1/valid_F1 = 0.585000/0.510511\n",
      "101.46 epochs, iter 15200: train_loss = 0.884606004, param_norm = 151.910, grad_norm = 0.123, train_F1/valid_F1 = 0.586000/0.521188\n",
      "Saved parameters to ./all_features_10_seq_100hidden_2layer/model.ckpt-15200\n",
      "101.59 epochs, iter 15220: train_loss = 0.886438131, param_norm = 151.930, grad_norm = 0.152, train_F1/valid_F1 = 0.584000/0.519520\n",
      "101.72 epochs, iter 15240: train_loss = 0.886283219, param_norm = 151.951, grad_norm = 0.128, train_F1/valid_F1 = 0.583000/0.522856\n",
      "101.86 epochs, iter 15260: train_loss = 0.884001970, param_norm = 151.982, grad_norm = 0.155, train_F1/valid_F1 = 0.585000/0.519520\n",
      "101.99 epochs, iter 15280: train_loss = 0.884284914, param_norm = 151.978, grad_norm = 0.115, train_F1/valid_F1 = 0.583000/0.520521\n",
      "102.12 epochs, iter 15300: train_loss = 0.887796521, param_norm = 151.991, grad_norm = 0.114, train_F1/valid_F1 = 0.582000/0.523524\n",
      "Saved parameters to ./all_features_10_seq_100hidden_2layer/model.ckpt-15300\n",
      "102.26 epochs, iter 15320: train_loss = 0.880508542, param_norm = 152.009, grad_norm = 0.172, train_F1/valid_F1 = 0.594000/0.514848\n",
      "102.39 epochs, iter 15340: train_loss = 0.884246707, param_norm = 152.013, grad_norm = 0.365, train_F1/valid_F1 = 0.586000/0.518852\n",
      "102.52 epochs, iter 15360: train_loss = 0.887739122, param_norm = 152.023, grad_norm = 0.166, train_F1/valid_F1 = 0.581000/0.524191\n",
      "102.66 epochs, iter 15380: train_loss = 0.892179132, param_norm = 152.044, grad_norm = 0.326, train_F1/valid_F1 = 0.580000/0.521855\n",
      "102.79 epochs, iter 15400: train_loss = 0.886726439, param_norm = 152.045, grad_norm = 0.311, train_F1/valid_F1 = 0.565000/0.526527\n",
      "Saved parameters to ./all_features_10_seq_100hidden_2layer/model.ckpt-15400\n",
      "102.92 epochs, iter 15420: train_loss = 0.885891616, param_norm = 152.070, grad_norm = 0.404, train_F1/valid_F1 = 0.580000/0.513847\n",
      "103.06 epochs, iter 15440: train_loss = 0.888430297, param_norm = 152.075, grad_norm = 0.242, train_F1/valid_F1 = 0.574000/0.519853\n",
      "103.19 epochs, iter 15460: train_loss = 0.879736185, param_norm = 152.114, grad_norm = 0.102, train_F1/valid_F1 = 0.592000/0.519853\n",
      "103.32 epochs, iter 15480: train_loss = 0.891281068, param_norm = 152.100, grad_norm = 0.227, train_F1/valid_F1 = 0.584000/0.514181\n",
      "103.46 epochs, iter 15500: train_loss = 0.881002009, param_norm = 152.117, grad_norm = 0.119, train_F1/valid_F1 = 0.585000/0.524858\n",
      "Saved parameters to ./all_features_10_seq_100hidden_2layer/model.ckpt-15500\n",
      "103.59 epochs, iter 15520: train_loss = 0.881505847, param_norm = 152.120, grad_norm = 0.122, train_F1/valid_F1 = 0.583000/0.513514\n",
      "103.72 epochs, iter 15540: train_loss = 0.885616958, param_norm = 152.140, grad_norm = 0.118, train_F1/valid_F1 = 0.590000/0.508842\n",
      "103.86 epochs, iter 15560: train_loss = 0.887348890, param_norm = 152.137, grad_norm = 0.151, train_F1/valid_F1 = 0.573000/0.507508\n",
      "103.99 epochs, iter 15580: train_loss = 0.887649179, param_norm = 152.170, grad_norm = 0.465, train_F1/valid_F1 = 0.588000/0.510177\n",
      "104.12 epochs, iter 15600: train_loss = 0.883311391, param_norm = 152.200, grad_norm = 0.145, train_F1/valid_F1 = 0.589000/0.514848\n",
      "Saved parameters to ./all_features_10_seq_100hidden_2layer/model.ckpt-15600\n",
      "104.26 epochs, iter 15620: train_loss = 0.881889522, param_norm = 152.215, grad_norm = 0.224, train_F1/valid_F1 = 0.589000/0.518185\n",
      "104.39 epochs, iter 15640: train_loss = 0.883547604, param_norm = 152.235, grad_norm = 0.339, train_F1/valid_F1 = 0.578000/0.519520\n",
      "104.53 epochs, iter 15660: train_loss = 0.885231197, param_norm = 152.226, grad_norm = 0.460, train_F1/valid_F1 = 0.575000/0.519853\n",
      "104.66 epochs, iter 15680: train_loss = 0.879070580, param_norm = 152.266, grad_norm = 0.136, train_F1/valid_F1 = 0.587000/0.516850\n",
      "104.79 epochs, iter 15700: train_loss = 0.888851345, param_norm = 152.287, grad_norm = 0.122, train_F1/valid_F1 = 0.575000/0.516850\n",
      "Saved parameters to ./all_features_10_seq_100hidden_2layer/model.ckpt-15700\n",
      "104.93 epochs, iter 15720: train_loss = 0.892008781, param_norm = 152.279, grad_norm = 0.172, train_F1/valid_F1 = 0.582000/0.525526\n",
      "105.06 epochs, iter 15740: train_loss = 0.884996951, param_norm = 152.310, grad_norm = 0.155, train_F1/valid_F1 = 0.579000/0.513180\n",
      "105.19 epochs, iter 15760: train_loss = 0.896495759, param_norm = 152.318, grad_norm = 0.967, train_F1/valid_F1 = 0.586000/0.522523\n",
      "105.33 epochs, iter 15780: train_loss = 0.883468688, param_norm = 152.333, grad_norm = 0.208, train_F1/valid_F1 = 0.579000/0.515516\n",
      "105.46 epochs, iter 15800: train_loss = 0.881759703, param_norm = 152.339, grad_norm = 0.715, train_F1/valid_F1 = 0.594000/0.513180\n",
      "Saved parameters to ./all_features_10_seq_100hidden_2layer/model.ckpt-15800\n",
      "105.59 epochs, iter 15820: train_loss = 0.889877319, param_norm = 152.340, grad_norm = 0.197, train_F1/valid_F1 = 0.586000/0.520187\n",
      "105.73 epochs, iter 15840: train_loss = 0.887241483, param_norm = 152.376, grad_norm = 0.256, train_F1/valid_F1 = 0.590000/0.508842\n",
      "105.86 epochs, iter 15860: train_loss = 0.884643018, param_norm = 152.379, grad_norm = 0.153, train_F1/valid_F1 = 0.578000/0.518519\n",
      "105.99 epochs, iter 15880: train_loss = 0.887573242, param_norm = 152.398, grad_norm = 0.151, train_F1/valid_F1 = 0.579000/0.514515\n",
      "106.13 epochs, iter 15900: train_loss = 0.885886371, param_norm = 152.439, grad_norm = 0.123, train_F1/valid_F1 = 0.583000/0.517184\n",
      "Saved parameters to ./all_features_10_seq_100hidden_2layer/model.ckpt-15900\n",
      "106.26 epochs, iter 15920: train_loss = 0.889593720, param_norm = 152.439, grad_norm = 0.670, train_F1/valid_F1 = 0.572000/0.517851\n",
      "106.39 epochs, iter 15940: train_loss = 0.883799434, param_norm = 152.446, grad_norm = 0.120, train_F1/valid_F1 = 0.582000/0.517184\n",
      "106.53 epochs, iter 15960: train_loss = 0.881833076, param_norm = 152.463, grad_norm = 0.164, train_F1/valid_F1 = 0.577000/0.526860\n",
      "106.66 epochs, iter 15980: train_loss = 0.870466590, param_norm = 152.477, grad_norm = 0.150, train_F1/valid_F1 = 0.588000/0.519520\n",
      "106.79 epochs, iter 16000: train_loss = 0.883896053, param_norm = 152.497, grad_norm = 0.138, train_F1/valid_F1 = 0.587000/0.515182\n",
      "Saved parameters to ./all_features_10_seq_100hidden_2layer/model.ckpt-16000\n",
      "106.93 epochs, iter 16020: train_loss = 0.882990599, param_norm = 152.513, grad_norm = 0.167, train_F1/valid_F1 = 0.578000/0.527528\n",
      "107.06 epochs, iter 16040: train_loss = 0.882958114, param_norm = 152.526, grad_norm = 0.164, train_F1/valid_F1 = 0.585000/0.516517\n",
      "107.20 epochs, iter 16060: train_loss = 0.883676171, param_norm = 152.547, grad_norm = 0.200, train_F1/valid_F1 = 0.589000/0.504171\n",
      "107.33 epochs, iter 16080: train_loss = 0.879295349, param_norm = 152.561, grad_norm = 0.146, train_F1/valid_F1 = 0.588000/0.510511\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "107.46 epochs, iter 16100: train_loss = 0.882169902, param_norm = 152.577, grad_norm = 0.243, train_F1/valid_F1 = 0.583000/0.510177\n",
      "Saved parameters to ./all_features_10_seq_100hidden_2layer/model.ckpt-16100\n",
      "107.60 epochs, iter 16120: train_loss = 0.884269655, param_norm = 152.581, grad_norm = 0.234, train_F1/valid_F1 = 0.584000/0.531198\n",
      "107.73 epochs, iter 16140: train_loss = 0.886056662, param_norm = 152.592, grad_norm = 0.326, train_F1/valid_F1 = 0.576000/0.521188\n",
      "107.86 epochs, iter 16160: train_loss = 0.881670654, param_norm = 152.601, grad_norm = 0.489, train_F1/valid_F1 = 0.593000/0.536203\n",
      "108.00 epochs, iter 16180: train_loss = 0.883905232, param_norm = 152.602, grad_norm = 0.168, train_F1/valid_F1 = 0.589000/0.509843\n",
      "108.13 epochs, iter 16200: train_loss = 0.881518424, param_norm = 152.615, grad_norm = 0.143, train_F1/valid_F1 = 0.590000/0.505506\n",
      "Saved parameters to ./all_features_10_seq_100hidden_2layer/model.ckpt-16200\n",
      "108.26 epochs, iter 16220: train_loss = 0.889402747, param_norm = 152.622, grad_norm = 0.169, train_F1/valid_F1 = 0.581000/0.523190\n",
      "108.40 epochs, iter 16240: train_loss = 0.883039057, param_norm = 152.643, grad_norm = 0.160, train_F1/valid_F1 = 0.581000/0.515516\n",
      "108.53 epochs, iter 16260: train_loss = 0.874383569, param_norm = 152.680, grad_norm = 0.155, train_F1/valid_F1 = 0.591000/0.517184\n",
      "108.66 epochs, iter 16280: train_loss = 0.879433215, param_norm = 152.701, grad_norm = 0.612, train_F1/valid_F1 = 0.590000/0.524858\n",
      "108.80 epochs, iter 16300: train_loss = 0.881419778, param_norm = 152.681, grad_norm = 0.152, train_F1/valid_F1 = 0.584000/0.515849\n",
      "Saved parameters to ./all_features_10_seq_100hidden_2layer/model.ckpt-16300\n",
      "108.93 epochs, iter 16320: train_loss = 0.878965437, param_norm = 152.712, grad_norm = 0.201, train_F1/valid_F1 = 0.588000/0.515516\n",
      "109.06 epochs, iter 16340: train_loss = 0.873879910, param_norm = 152.724, grad_norm = 0.093, train_F1/valid_F1 = 0.582000/0.511845\n",
      "109.20 epochs, iter 16360: train_loss = 0.877469063, param_norm = 152.750, grad_norm = 0.224, train_F1/valid_F1 = 0.577000/0.509176\n",
      "109.33 epochs, iter 16380: train_loss = 0.889182866, param_norm = 152.785, grad_norm = 0.248, train_F1/valid_F1 = 0.574000/0.511512\n",
      "109.46 epochs, iter 16400: train_loss = 0.885999382, param_norm = 152.799, grad_norm = 3.176, train_F1/valid_F1 = 0.581000/0.520854\n",
      "Saved parameters to ./all_features_10_seq_100hidden_2layer/model.ckpt-16400\n",
      "109.60 epochs, iter 16420: train_loss = 0.878761411, param_norm = 152.791, grad_norm = 0.168, train_F1/valid_F1 = 0.581000/0.517518\n",
      "109.73 epochs, iter 16440: train_loss = 0.879074454, param_norm = 152.803, grad_norm = 0.244, train_F1/valid_F1 = 0.581000/0.520854\n",
      "109.87 epochs, iter 16460: train_loss = 0.883242846, param_norm = 152.813, grad_norm = 0.159, train_F1/valid_F1 = 0.584000/0.512179\n",
      "110.00 epochs, iter 16480: train_loss = 0.879642844, param_norm = 152.842, grad_norm = 0.190, train_F1/valid_F1 = 0.586000/0.516850\n",
      "110.13 epochs, iter 16500: train_loss = 0.880707026, param_norm = 152.859, grad_norm = 0.514, train_F1/valid_F1 = 0.588000/0.511512\n",
      "Saved parameters to ./all_features_10_seq_100hidden_2layer/model.ckpt-16500\n",
      "110.27 epochs, iter 16520: train_loss = 0.874176145, param_norm = 152.869, grad_norm = 0.181, train_F1/valid_F1 = 0.585000/0.510511\n",
      "110.40 epochs, iter 16540: train_loss = 0.872072995, param_norm = 152.867, grad_norm = 0.131, train_F1/valid_F1 = 0.593000/0.515516\n",
      "110.53 epochs, iter 16560: train_loss = 0.887665749, param_norm = 152.884, grad_norm = 0.444, train_F1/valid_F1 = 0.576000/0.510511\n",
      "110.67 epochs, iter 16580: train_loss = 0.877524316, param_norm = 152.908, grad_norm = 0.160, train_F1/valid_F1 = 0.588000/0.514848\n",
      "110.80 epochs, iter 16600: train_loss = 0.876462996, param_norm = 152.923, grad_norm = 0.215, train_F1/valid_F1 = 0.591000/0.507508\n",
      "Saved parameters to ./all_features_10_seq_100hidden_2layer/model.ckpt-16600\n",
      "110.93 epochs, iter 16620: train_loss = 0.883025169, param_norm = 152.911, grad_norm = 0.283, train_F1/valid_F1 = 0.584000/0.515516\n",
      "111.07 epochs, iter 16640: train_loss = 0.877763450, param_norm = 152.921, grad_norm = 0.281, train_F1/valid_F1 = 0.584000/0.503837\n",
      "111.20 epochs, iter 16660: train_loss = 0.883380830, param_norm = 152.918, grad_norm = 0.284, train_F1/valid_F1 = 0.570000/0.515516\n",
      "111.33 epochs, iter 16680: train_loss = 0.876802623, param_norm = 152.942, grad_norm = 0.120, train_F1/valid_F1 = 0.591000/0.513180\n",
      "111.47 epochs, iter 16700: train_loss = 0.878119051, param_norm = 152.970, grad_norm = 0.163, train_F1/valid_F1 = 0.592000/0.512846\n",
      "Saved parameters to ./all_features_10_seq_100hidden_2layer/model.ckpt-16700\n",
      "111.60 epochs, iter 16720: train_loss = 0.879183352, param_norm = 152.981, grad_norm = 0.203, train_F1/valid_F1 = 0.582000/0.514848\n",
      "111.73 epochs, iter 16740: train_loss = 0.879846215, param_norm = 153.022, grad_norm = 0.157, train_F1/valid_F1 = 0.591000/0.517518\n",
      "111.87 epochs, iter 16760: train_loss = 0.880523324, param_norm = 153.039, grad_norm = 0.146, train_F1/valid_F1 = 0.588000/0.511512\n",
      "112.00 epochs, iter 16780: train_loss = 0.870291889, param_norm = 153.045, grad_norm = 0.213, train_F1/valid_F1 = 0.590000/0.516183\n",
      "112.13 epochs, iter 16800: train_loss = 0.888653934, param_norm = 153.057, grad_norm = 0.176, train_F1/valid_F1 = 0.585000/0.524858\n",
      "Saved parameters to ./all_features_10_seq_100hidden_2layer/model.ckpt-16800\n",
      "112.27 epochs, iter 16820: train_loss = 0.886604726, param_norm = 153.079, grad_norm = 0.142, train_F1/valid_F1 = 0.587000/0.514181\n",
      "112.40 epochs, iter 16840: train_loss = 0.877608716, param_norm = 153.090, grad_norm = 0.203, train_F1/valid_F1 = 0.592000/0.509510\n",
      "112.54 epochs, iter 16860: train_loss = 0.874277711, param_norm = 153.103, grad_norm = 0.166, train_F1/valid_F1 = 0.591000/0.507508\n",
      "112.67 epochs, iter 16880: train_loss = 0.885704815, param_norm = 153.123, grad_norm = 0.209, train_F1/valid_F1 = 0.595000/0.508509\n",
      "112.80 epochs, iter 16900: train_loss = 0.871786237, param_norm = 153.135, grad_norm = 0.120, train_F1/valid_F1 = 0.590000/0.516183\n",
      "Saved parameters to ./all_features_10_seq_100hidden_2layer/model.ckpt-16900\n",
      "112.94 epochs, iter 16920: train_loss = 0.882787466, param_norm = 153.137, grad_norm = 0.506, train_F1/valid_F1 = 0.578000/0.522189\n",
      "113.07 epochs, iter 16940: train_loss = 0.875342488, param_norm = 153.130, grad_norm = 0.243, train_F1/valid_F1 = 0.593000/0.510844\n",
      "113.20 epochs, iter 16960: train_loss = 0.879598141, param_norm = 153.160, grad_norm = 0.539, train_F1/valid_F1 = 0.588000/0.513514\n",
      "113.34 epochs, iter 16980: train_loss = 0.872648716, param_norm = 153.167, grad_norm = 0.118, train_F1/valid_F1 = 0.594000/0.519186\n",
      "113.47 epochs, iter 17000: train_loss = 0.889899671, param_norm = 153.176, grad_norm = 0.596, train_F1/valid_F1 = 0.591000/0.516850\n",
      "Saved parameters to ./all_features_10_seq_100hidden_2layer/model.ckpt-17000\n",
      "113.60 epochs, iter 17020: train_loss = 0.872136116, param_norm = 153.193, grad_norm = 0.132, train_F1/valid_F1 = 0.597000/0.519520\n",
      "113.74 epochs, iter 17040: train_loss = 0.877114475, param_norm = 153.194, grad_norm = 0.147, train_F1/valid_F1 = 0.590000/0.520187\n",
      "113.87 epochs, iter 17060: train_loss = 0.873421371, param_norm = 153.212, grad_norm = 0.104, train_F1/valid_F1 = 0.592000/0.508842\n",
      "114.00 epochs, iter 17080: train_loss = 0.870417953, param_norm = 153.221, grad_norm = 0.129, train_F1/valid_F1 = 0.595000/0.520521\n",
      "114.14 epochs, iter 17100: train_loss = 0.867651880, param_norm = 153.236, grad_norm = 0.137, train_F1/valid_F1 = 0.594000/0.515849\n",
      "Saved parameters to ./all_features_10_seq_100hidden_2layer/model.ckpt-17100\n",
      "114.27 epochs, iter 17120: train_loss = 0.863745034, param_norm = 153.260, grad_norm = 0.101, train_F1/valid_F1 = 0.591000/0.515182\n",
      "114.40 epochs, iter 17140: train_loss = 0.884580135, param_norm = 153.277, grad_norm = 0.269, train_F1/valid_F1 = 0.595000/0.521522\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-117-d6887b9d5e87>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlstm_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexperiment_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mexperiment_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;31m# IMPORTANT:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# when you think F1 is not going to improve anymore, wait another 10 epochs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# if you see any better iteration that has not appeared before, keep waiting.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-111-45a0437c499e>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, session, experiment_name)\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0miteration\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m20\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 95\u001b[0;31m                 \u001b[0my_1000_train_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam_norm_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_norm_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparam_norm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgradient_norm\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m                                                                        \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mx_1000_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0my_1000_train\u001b[0m\u001b[0;34m,\u001b[0m                                                                                   \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearning_rate_placeholder\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlr\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     96\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mloss_val\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mold_loss\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m1.2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m                     \u001b[0mlr\u001b[0m \u001b[0;34m/=\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    903\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    904\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 905\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    906\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    907\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1138\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1139\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1140\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1141\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1142\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1319\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1320\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1321\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1322\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1323\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1325\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1326\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1327\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1328\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1329\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1310\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1311\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1312\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1313\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1314\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1418\u001b[0m         return tf_session.TF_Run(\n\u001b[1;32m   1419\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1420\u001b[0;31m             status, run_metadata)\n\u001b[0m\u001b[1;32m   1421\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1422\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "lstm_model.train(session=sess, experiment_name=experiment_name)\n",
    "# IMPORTANT:\n",
    "# when you think F1 is not going to improve anymore, wait another 10 epochs. \n",
    "# if you see any better iteration that has not appeared before, keep waiting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train F1: 0.584\n",
      "dev F1: 0.5235235235235235\n",
      "test F1: 0.3532064128256513\n"
     ]
    }
   ],
   "source": [
    "y_1000_train_pred = sess.run(lstm_model.preds, feed_dict={lstm_model.X: x_1000_train})\n",
    "print(\"train F1:\",F1(y_1000_train, y_1000_train_pred))\n",
    "y_valid_pred = sess.run(lstm_model.preds, feed_dict={lstm_model.X: x_valid})\n",
    "print(\"dev F1:\",F1(y_valid, y_valid_pred))\n",
    "y_test_pred = sess.run(lstm_model.preds, feed_dict={lstm_model.X: x_test})\n",
    "print(\"test F1:\",F1(y_test, y_test_pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
