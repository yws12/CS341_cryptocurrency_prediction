{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>USDT_BTC_high</th>\n",
       "      <th>USDT_BTC_low</th>\n",
       "      <th>USDT_BTC_close</th>\n",
       "      <th>USDT_BTC_open</th>\n",
       "      <th>USDT_BTC_volume</th>\n",
       "      <th>USDT_BTC_quoteVolume</th>\n",
       "      <th>USDT_BTC_weighted_mean</th>\n",
       "      <th>USDT_BTC_pctChange</th>\n",
       "      <th>USDT_ETH_high</th>\n",
       "      <th>USDT_ETH_low</th>\n",
       "      <th>...</th>\n",
       "      <th>BTC_LTC_weighted_mean</th>\n",
       "      <th>BTC_LTC_pctChange</th>\n",
       "      <th>BTC_XRP_high</th>\n",
       "      <th>BTC_XRP_low</th>\n",
       "      <th>BTC_XRP_close</th>\n",
       "      <th>BTC_XRP_open</th>\n",
       "      <th>BTC_XRP_volume</th>\n",
       "      <th>BTC_XRP_quoteVolume</th>\n",
       "      <th>BTC_XRP_weighted_mean</th>\n",
       "      <th>BTC_XRP_pctChange</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>time</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2016-01-02 12:00:00</th>\n",
       "      <td>432.5000</td>\n",
       "      <td>432.50</td>\n",
       "      <td>432.500000</td>\n",
       "      <td>432.50000</td>\n",
       "      <td>40.041239</td>\n",
       "      <td>0.092581</td>\n",
       "      <td>432.500000</td>\n",
       "      <td>2.220446e-16</td>\n",
       "      <td>0.959136</td>\n",
       "      <td>0.959136</td>\n",
       "      <td>...</td>\n",
       "      <td>0.008063</td>\n",
       "      <td>-0.002293</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.033605</td>\n",
       "      <td>2408.822942</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>-0.002859</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-01-02 13:00:00</th>\n",
       "      <td>432.5000</td>\n",
       "      <td>432.50</td>\n",
       "      <td>432.500000</td>\n",
       "      <td>432.50000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>432.986941</td>\n",
       "      <td>1.125876e-03</td>\n",
       "      <td>0.959136</td>\n",
       "      <td>0.959136</td>\n",
       "      <td>...</td>\n",
       "      <td>0.008060</td>\n",
       "      <td>-0.000333</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.004704</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-01-02 14:00:00</th>\n",
       "      <td>437.3635</td>\n",
       "      <td>432.48</td>\n",
       "      <td>433.336667</td>\n",
       "      <td>433.52799</td>\n",
       "      <td>359.269753</td>\n",
       "      <td>0.828819</td>\n",
       "      <td>433.473883</td>\n",
       "      <td>1.124610e-03</td>\n",
       "      <td>0.959136</td>\n",
       "      <td>0.957000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.008073</td>\n",
       "      <td>0.001623</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>1.141981</td>\n",
       "      <td>81071.098773</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.004682</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-01-02 15:00:00</th>\n",
       "      <td>432.4800</td>\n",
       "      <td>432.48</td>\n",
       "      <td>432.480000</td>\n",
       "      <td>432.48000</td>\n",
       "      <td>60.859598</td>\n",
       "      <td>0.140722</td>\n",
       "      <td>432.480000</td>\n",
       "      <td>-2.292832e-03</td>\n",
       "      <td>0.957000</td>\n",
       "      <td>0.957000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.008089</td>\n",
       "      <td>0.002002</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>2.120423</td>\n",
       "      <td>150622.792769</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>-0.000492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-01-02 16:00:00</th>\n",
       "      <td>432.4800</td>\n",
       "      <td>432.48</td>\n",
       "      <td>432.480000</td>\n",
       "      <td>432.48000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>432.350000</td>\n",
       "      <td>-3.005919e-04</td>\n",
       "      <td>0.957000</td>\n",
       "      <td>0.957000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.008079</td>\n",
       "      <td>-0.001224</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.491516</td>\n",
       "      <td>35178.793196</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>-0.007526</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 56 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                     USDT_BTC_high  USDT_BTC_low  USDT_BTC_close  \\\n",
       "time                                                               \n",
       "2016-01-02 12:00:00       432.5000        432.50      432.500000   \n",
       "2016-01-02 13:00:00       432.5000        432.50      432.500000   \n",
       "2016-01-02 14:00:00       437.3635        432.48      433.336667   \n",
       "2016-01-02 15:00:00       432.4800        432.48      432.480000   \n",
       "2016-01-02 16:00:00       432.4800        432.48      432.480000   \n",
       "\n",
       "                     USDT_BTC_open  USDT_BTC_volume  USDT_BTC_quoteVolume  \\\n",
       "time                                                                        \n",
       "2016-01-02 12:00:00      432.50000        40.041239              0.092581   \n",
       "2016-01-02 13:00:00      432.50000         0.000000              0.000000   \n",
       "2016-01-02 14:00:00      433.52799       359.269753              0.828819   \n",
       "2016-01-02 15:00:00      432.48000        60.859598              0.140722   \n",
       "2016-01-02 16:00:00      432.48000         0.000000              0.000000   \n",
       "\n",
       "                     USDT_BTC_weighted_mean  USDT_BTC_pctChange  \\\n",
       "time                                                              \n",
       "2016-01-02 12:00:00              432.500000        2.220446e-16   \n",
       "2016-01-02 13:00:00              432.986941        1.125876e-03   \n",
       "2016-01-02 14:00:00              433.473883        1.124610e-03   \n",
       "2016-01-02 15:00:00              432.480000       -2.292832e-03   \n",
       "2016-01-02 16:00:00              432.350000       -3.005919e-04   \n",
       "\n",
       "                     USDT_ETH_high  USDT_ETH_low        ...          \\\n",
       "time                                                    ...           \n",
       "2016-01-02 12:00:00       0.959136      0.959136        ...           \n",
       "2016-01-02 13:00:00       0.959136      0.959136        ...           \n",
       "2016-01-02 14:00:00       0.959136      0.957000        ...           \n",
       "2016-01-02 15:00:00       0.957000      0.957000        ...           \n",
       "2016-01-02 16:00:00       0.957000      0.957000        ...           \n",
       "\n",
       "                     BTC_LTC_weighted_mean  BTC_LTC_pctChange  BTC_XRP_high  \\\n",
       "time                                                                          \n",
       "2016-01-02 12:00:00               0.008063          -0.002293      0.000014   \n",
       "2016-01-02 13:00:00               0.008060          -0.000333      0.000014   \n",
       "2016-01-02 14:00:00               0.008073           0.001623      0.000014   \n",
       "2016-01-02 15:00:00               0.008089           0.002002      0.000014   \n",
       "2016-01-02 16:00:00               0.008079          -0.001224      0.000014   \n",
       "\n",
       "                     BTC_XRP_low  BTC_XRP_close  BTC_XRP_open  BTC_XRP_volume  \\\n",
       "time                                                                            \n",
       "2016-01-02 12:00:00     0.000014       0.000014      0.000014        0.033605   \n",
       "2016-01-02 13:00:00     0.000014       0.000014      0.000014        0.000000   \n",
       "2016-01-02 14:00:00     0.000014       0.000014      0.000014        1.141981   \n",
       "2016-01-02 15:00:00     0.000014       0.000014      0.000014        2.120423   \n",
       "2016-01-02 16:00:00     0.000014       0.000014      0.000014        0.491516   \n",
       "\n",
       "                     BTC_XRP_quoteVolume  BTC_XRP_weighted_mean  \\\n",
       "time                                                              \n",
       "2016-01-02 12:00:00          2408.822942               0.000014   \n",
       "2016-01-02 13:00:00             0.000000               0.000014   \n",
       "2016-01-02 14:00:00         81071.098773               0.000014   \n",
       "2016-01-02 15:00:00        150622.792769               0.000014   \n",
       "2016-01-02 16:00:00         35178.793196               0.000014   \n",
       "\n",
       "                     BTC_XRP_pctChange  \n",
       "time                                    \n",
       "2016-01-02 12:00:00          -0.002859  \n",
       "2016-01-02 13:00:00           0.004704  \n",
       "2016-01-02 14:00:00           0.004682  \n",
       "2016-01-02 15:00:00          -0.000492  \n",
       "2016-01-02 16:00:00          -0.007526  \n",
       "\n",
       "[5 rows x 56 columns]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "__author__ = \"Yicheng Li\"\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import statsmodels.api as sm\n",
    "import random\n",
    "from sklearn import preprocessing\n",
    "import tensorflow as tf\n",
    "import os\n",
    "\n",
    "dir_path = 'CS341-repo/Data/'\n",
    "df = pd.read_pickle(dir_path+'df_hourly_poloniex.pickle')\n",
    "df = df.dropna()\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler_features = preprocessing.MinMaxScaler(feature_range=(0.1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to create train, validation, test data given sequence length\n",
    "def load_data(df, seq_len, test_size=-1):\n",
    "    # prepare one-hot labels\n",
    "    labels = df['USDT_BTC_pctChange'].as_matrix().reshape([-1,1])\n",
    "    labels = np.concatenate([(labels > 3e-3)*1, ((3e-3 > labels)&(labels > -3e-3))*1, (labels < -3e-3)*1],1)\n",
    "    labels = labels[seq_len-1:] # so labels and data has same length\n",
    "    \n",
    "    feature_set = [x for x in range(56)] #[0,1,2,3,4,6,7]\n",
    "    \n",
    "    data_raw = df.as_matrix() # convert to numpy array\n",
    "    # fit scaler\n",
    "    data_raw = scaler_features.fit_transform(data_raw[:, feature_set])\n",
    "    data = []\n",
    "    \n",
    "    # create all possible sequences of length seq_len\n",
    "    for index in range(len(data_raw) - seq_len + 1): \n",
    "        data.append(data_raw[index: index + seq_len, :])\n",
    "    \n",
    "    data = np.array(data)\n",
    "    \n",
    "    if test_size == -1: # split the old way\n",
    "        n_train_valid_pairs = 3\n",
    "        each_train_set_size_pct = 25\n",
    "        each_valid_set_size_pct = 5\n",
    "\n",
    "        each_train_set_size = round(each_train_set_size_pct/100*data.shape[0])\n",
    "        each_valid_set_size = round(each_valid_set_size_pct/100*data.shape[0])\n",
    "\n",
    "        x_train_sets = []\n",
    "        y_train_sets = []\n",
    "        x_valid_sets = []\n",
    "        y_valid_sets = []\n",
    "        used = 0\n",
    "\n",
    "        for i in range(n_train_valid_pairs):\n",
    "            x_train_sets.append(data[used : used + each_train_set_size,:-1,:]) # cannot see last day, which we aim to predict\n",
    "            y_train_sets.append(labels[used : used + each_train_set_size, :])\n",
    "            used += each_train_set_size\n",
    "\n",
    "            x_valid_sets.append(data[used : used + each_valid_set_size,:-1,:])\n",
    "            y_valid_sets.append(labels[used : used + each_valid_set_size, :])\n",
    "            used += each_valid_set_size\n",
    "\n",
    "        x_test = data[used : , :-1, :]\n",
    "        y_test = labels[used : , :]\n",
    "\n",
    "        x_train = np.concatenate(x_train_sets, axis=0)\n",
    "        y_train = np.concatenate(y_train_sets, axis=0)\n",
    "        x_valid = np.concatenate(x_valid_sets, axis=0)\n",
    "        y_valid = np.concatenate(y_valid_sets, axis=0)\n",
    "    \n",
    "    else:\n",
    "        x_test = data[-test_size : , :-1, :]\n",
    "        y_test = labels[-test_size : , :]\n",
    "        \n",
    "        valid_start = data.shape[0] - test_size - int(test_size/2)\n",
    "        x_valid = data[valid_start:-test_size, :-1, :]\n",
    "        y_valid = labels[valid_start:-test_size, :]\n",
    "        \n",
    "        x_train = data[:valid_start, :-1, :]\n",
    "        y_train = labels[:valid_start, :]\n",
    "    \n",
    "    return [x_train, y_train, x_valid, y_valid, x_test, y_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train.shape =  (16981, 9, 56)\n",
      "y_train.shape =  (16981, 3)\n",
      "x_valid.shape =  (998, 9, 56)\n",
      "y_valid.shape =  (998, 3)\n",
      "x_test.shape =  (1996, 9, 56)\n",
      "y_test.shape =  (1996, 3)\n"
     ]
    }
   ],
   "source": [
    "# create train, test data\n",
    "seq_len = 10 # choose sequence length\n",
    "x_train, y_train, x_valid, y_valid, x_test, y_test = load_data(df, seq_len, test_size=1996)\n",
    "# y_train = y_train.reshape([-1,1])\n",
    "# y_valid = y_valid.reshape([-1,1])\n",
    "# y_test = y_test.reshape([-1,1])\n",
    "print('x_train.shape = ',x_train.shape)\n",
    "print('y_train.shape = ', y_train.shape)\n",
    "print('x_valid.shape = ',x_valid.shape)\n",
    "print('y_valid.shape = ', y_valid.shape)\n",
    "print('x_test.shape = ', x_test.shape)\n",
    "print('y_test.shape = ',y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f1_score_single(y_true, y_pred):\n",
    "    TP = y_true.dot(y_pred) # zero or one\n",
    "    FP = np.sum(y_pred > y_true) # sum over all k classes, zero or one\n",
    "    FN = np.sum(y_pred < y_true) # sum over all k classes, zero or one\n",
    "    \n",
    "    if TP == 0: return 0.\n",
    "    p = 1. * TP / (TP + FP)\n",
    "    r = 1. * TP / (TP + FN)\n",
    "    return 2 * p * r / (p + r)\n",
    "    \n",
    "def F1(y_true, y_pred):\n",
    "    return np.mean([f1_score_single(x, y) for x, y in zip(y_true, y_pred)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "baseline dev_F1= 0.39618856569709127\n",
      "baseline test_F1= 0.3712424849699399\n"
     ]
    }
   ],
   "source": [
    "y_pred = np.roll(y_valid,1, axis=0)\n",
    "print('baseline dev_F1=',F1(y_valid[1:], y_pred[1:]))\n",
    "y_pred = np.roll(y_test,1, axis=0)\n",
    "y_pred[0] = y_valid[-1] # be careful here\n",
    "print('baseline test_F1=',F1(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_in_epoch = 0;\n",
    "perm_array  = np.arange(x_train.shape[0])\n",
    "np.random.shuffle(perm_array)\n",
    "\n",
    "# function to get the next batch\n",
    "def get_next_batch(batch_size):\n",
    "    global index_in_epoch, x_train, perm_array   \n",
    "    start = index_in_epoch\n",
    "    index_in_epoch += batch_size\n",
    "    \n",
    "    if index_in_epoch > x_train.shape[0]:\n",
    "        np.random.shuffle(perm_array) # shuffle permutation array\n",
    "        start = 0 # start next epoch\n",
    "        index_in_epoch = batch_size\n",
    "        \n",
    "    end = index_in_epoch\n",
    "    return x_train[perm_array[start:end]], y_train[perm_array[start:end]]\n",
    "\n",
    "x_1000_train, y_1000_train = get_next_batch(1000) # special batch of 1000 records in training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM_Model(object):\n",
    "    def __init__(self, seq_len):\n",
    "        # parameters\n",
    "        self.n_steps = seq_len-1 \n",
    "        self.n_inputs = x_train.shape[-1]\n",
    "        self.n_neurons = 100  # cell.state_size\n",
    "        self.n_bins = 3 # be careful if you want to change this\n",
    "        self.n_layers = 2\n",
    "        self.batch_size = 100\n",
    "        self.n_epochs = 0 # 0 means to train indefinitely\n",
    "        self.train_set_size = x_train.shape[0]\n",
    "        self.test_set_size = x_test.shape[0]\n",
    "        self.keep_prob = tf.placeholder(tf.float32, [])\n",
    "        self.max_gradient_norm = 5\n",
    "        with tf.variable_scope(\"LSTM_Model\", initializer=tf.contrib.layers.xavier_initializer()):\n",
    "            self.X = tf.placeholder(tf.float32, [None, self.n_steps, self.n_inputs])\n",
    "            self.y = tf.placeholder(tf.float32, [None, self.n_bins])\n",
    "\n",
    "            layers = [tf.contrib.rnn.LSTMCell(num_units=self.n_neurons, \\\n",
    "                                              initializer=tf.contrib.layers.xavier_initializer(), \\\n",
    "                                              activation=tf.nn.elu)\n",
    "                     for layer in range(self.n_layers)]\n",
    "            \n",
    "#             layers = [tf.contrib.rnn.BasicRNNCell(num_units=self.n_neurons, \\\n",
    "#                                               activation=tf.nn.elu)\n",
    "#                      for layer in range(self.n_layers)]\n",
    "            \n",
    "            multi_layer_cell = tf.contrib.rnn.MultiRNNCell(layers)\n",
    "\n",
    "            outputs, states = tf.nn.dynamic_rnn(multi_layer_cell, self.X, dtype=tf.float32)\n",
    "            outputs = tf.nn.dropout(outputs, self.keep_prob) # dropout\n",
    "            # 'outputs' is a tensor of shape [batch_size, n_steps, n_neurons(cell.state_size)]\n",
    "        \n",
    "#             directly output\n",
    "#             logits = outputs[:,self.n_steps-1,:] # keep only last output of sequence\n",
    "            \n",
    "            # ======== attn layer ===================\n",
    "#             sim_mat = tf.matmul(outputs, tf.transpose(outputs, perm=[0,2,1]))\n",
    "#             attn_dist = tf.nn.softmax(sim_mat, 2)\n",
    "#             # (batchsize, n_steps, n_steps)\n",
    "#             attn_outputs = tf.matmul(attn_dist, outputs)\n",
    "#             # (batchsize, n_steps, n_bins)\n",
    "            # ========================================\n",
    "            \n",
    "            stacked_outputs = tf.reshape(outputs, [-1, self.n_neurons]) \n",
    "            stacked_outputs = tf.layers.dense(stacked_outputs, self.n_bins)\n",
    "            final_outputs = tf.reshape(stacked_outputs, [-1, self.n_steps, self.n_bins])\n",
    "            \n",
    "            self.final_logits = final_outputs[:, -1, :] # last timestep\n",
    "            # (batchsize, n_bins), this is logits, not probs!!\n",
    "            self.final_logits = tf.nn.dropout(self.final_logits, self.keep_prob) # dropout\n",
    "            \n",
    "            self.indices = tf.argmax(self.final_logits, axis=-1) # (batchsize, 1)\n",
    "            self.preds = tf.one_hot(self.indices, depth=self.n_bins)\n",
    "            \n",
    "            self.each_loss = tf.nn.softmax_cross_entropy_with_logits(logits=self.final_logits, labels=self.y)\n",
    "            self.loss = tf.reduce_mean(self.each_loss) \n",
    "\n",
    "            params = tf.trainable_variables()\n",
    "            gradients = tf.gradients(self.loss, params)\n",
    "            self.gradient_norm = tf.global_norm(gradients)\n",
    "            clipped_gradients, _ = tf.clip_by_global_norm(gradients, self.max_gradient_norm)\n",
    "            clipped_norm = tf.global_norm(clipped_gradients)\n",
    "            self.param_norm = tf.global_norm(params)\n",
    "            self.learning_rate_placeholder = tf.placeholder(tf.float32, [], name='learning_rate')\n",
    "            optimizer = tf.train.RMSPropOptimizer(learning_rate=self.learning_rate_placeholder) \n",
    "            # training_op = optimizer.minimize(loss)\n",
    "            self.training_op = optimizer.apply_gradients(zip(clipped_gradients, params))\n",
    "\n",
    "            # initialize parameters\n",
    "#             sess = tf.Session()\n",
    "            self.global_step = tf.Variable(0, name='global_step', trainable=False)\n",
    "            self.saver = tf.train.Saver(max_to_keep=2)\n",
    "            self.bestmodel_saver = tf.train.Saver(max_to_keep=2)\n",
    "    \n",
    "    def train(self, session, experiment_name, keep_prob_val):\n",
    "        \n",
    "        bestmodel_dir = experiment_name+'/best_ckpt'\n",
    "        bestmodel_ckpt_path = bestmodel_dir+'/best.ckpt'\n",
    "        best_valid_f1 = None\n",
    "        # Make bestmodel dir if necessary\n",
    "        if not os.path.exists(bestmodel_dir):\n",
    "            os.makedirs(bestmodel_dir)\n",
    "        \n",
    "        ckpt = tf.train.get_checkpoint_state(experiment_name)\n",
    "        v2_path = ckpt.model_checkpoint_path + \".index\" if ckpt else \"\"\n",
    "        if ckpt and (tf.gfile.Exists(ckpt.model_checkpoint_path) or tf.gfile.Exists(v2_path)):\n",
    "            self.saver.restore(sess, ckpt.model_checkpoint_path)\n",
    "            iteration = self.global_step.eval(session=sess) # get last global_step\n",
    "            print(\"Start from iteration:\", iteration)\n",
    "            lr = 1e-3\n",
    "        else:\n",
    "            print('There is not saved parameters. Creating model with fresh parameters.')\n",
    "            sess.run(tf.global_variables_initializer())\n",
    "            iteration = 0\n",
    "            lr = 1e-2 # should fix this...\n",
    "\n",
    "        old_loss = 1000\n",
    "        \n",
    "        while self.n_epochs == 0 or iteration*self.batch_size/self.train_set_size < self.n_epochs:\n",
    "            iteration = iteration + 1\n",
    "            x_batch, y_batch = get_next_batch(self.batch_size) # fetch the next training batch \n",
    "\n",
    "            # train on this batch\n",
    "            sess.run(self.training_op, feed_dict={self.X: x_batch, self.y: y_batch, self.learning_rate_placeholder:lr, \\\n",
    "                                                 self.keep_prob: keep_prob_val})\n",
    "\n",
    "            if iteration % 50 == 0:\n",
    "                y_1000_train_pred, loss_val, param_norm_val, grad_norm_val = \\\n",
    "                    sess.run([self.preds, self.loss, self.param_norm, self.gradient_norm],\\\n",
    "                            feed_dict={self.X: x_1000_train, self.y:y_1000_train, \\\n",
    "                                        self.learning_rate_placeholder:lr, self.keep_prob: keep_prob_val})\n",
    "                    \n",
    "                if loss_val > old_loss * 1.2:\n",
    "                    lr /= 2\n",
    "                old_loss = loss_val\n",
    "\n",
    "                y_valid_pred = sess.run(self.preds, feed_dict={self.X: x_valid, self.keep_prob: keep_prob_val})\n",
    "                \n",
    "                valid_f1 = F1(y_valid, y_valid_pred)\n",
    "                print('%.2f epochs, iter %d: train_loss = %.9f, param_norm = %.3f, grad_norm = %.3f, train_F1/valid_F1 = %.6f/%.6f' \\\n",
    "                      %(iteration*self.batch_size/self.train_set_size, iteration, loss_val, param_norm_val, grad_norm_val, \\\n",
    "                        F1(y_1000_train, y_1000_train_pred), \\\n",
    "                        valid_f1))\n",
    "\n",
    "                if best_valid_f1 is None or valid_f1 > best_valid_f1:\n",
    "                    best_valid_f1 = valid_f1\n",
    "                    print(\"======New best valid F1. Saving to %s...\" % bestmodel_ckpt_path)\n",
    "                    self.bestmodel_saver.save(sess, bestmodel_ckpt_path, global_step=self.global_step)\n",
    "                \n",
    "            if iteration % 100 == 0:\n",
    "                self.global_step.assign(iteration).eval(session=sess) # set and update(eval) global_step with index, i\n",
    "                save_path = self.saver.save(sess, \"./\"+experiment_name+\"/model.ckpt\", global_step=self.global_step)\n",
    "                print('Saved parameters to %s' % save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## run experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_name=\"all_features_10_seq_100hidden_2layer_drop08_longer\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "lstm_model = LSTM_Model(seq_len=seq_len)\n",
    "sess = tf.Session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There is not saved parameters. Creating model with fresh parameters.\n",
      "0.29 epochs, iter 50: train_loss = 1.030339956, param_norm = 22.309, grad_norm = 0.089, train_F1/valid_F1 = 0.520000/0.200401\n",
      "======New best valid F1. Saving to all_features_10_seq_100hidden_2layer_drop08_longer/best_ckpt/best.ckpt...\n",
      "0.59 epochs, iter 100: train_loss = 1.007503271, param_norm = 22.312, grad_norm = 0.044, train_F1/valid_F1 = 0.517000/0.417836\n",
      "======New best valid F1. Saving to all_features_10_seq_100hidden_2layer_drop08_longer/best_ckpt/best.ckpt...\n",
      "Saved parameters to ./all_features_10_seq_100hidden_2layer_drop08_longer/model.ckpt-100\n",
      "0.88 epochs, iter 150: train_loss = 1.053766727, param_norm = 22.860, grad_norm = 0.179, train_F1/valid_F1 = 0.481000/0.279559\n",
      "1.18 epochs, iter 200: train_loss = 1.012766480, param_norm = 24.990, grad_norm = 0.063, train_F1/valid_F1 = 0.520000/0.364729\n",
      "Saved parameters to ./all_features_10_seq_100hidden_2layer_drop08_longer/model.ckpt-200\n",
      "1.47 epochs, iter 250: train_loss = 1.016359210, param_norm = 28.401, grad_norm = 0.108, train_F1/valid_F1 = 0.515000/0.422846\n",
      "======New best valid F1. Saving to all_features_10_seq_100hidden_2layer_drop08_longer/best_ckpt/best.ckpt...\n",
      "1.77 epochs, iter 300: train_loss = 1.004429102, param_norm = 31.366, grad_norm = 0.065, train_F1/valid_F1 = 0.533000/0.420842\n",
      "Saved parameters to ./all_features_10_seq_100hidden_2layer_drop08_longer/model.ckpt-300\n",
      "2.06 epochs, iter 350: train_loss = 1.005203128, param_norm = 33.590, grad_norm = 0.032, train_F1/valid_F1 = 0.524000/0.415832\n",
      "2.36 epochs, iter 400: train_loss = 1.004509568, param_norm = 35.509, grad_norm = 0.109, train_F1/valid_F1 = 0.525000/0.423848\n",
      "======New best valid F1. Saving to all_features_10_seq_100hidden_2layer_drop08_longer/best_ckpt/best.ckpt...\n",
      "Saved parameters to ./all_features_10_seq_100hidden_2layer_drop08_longer/model.ckpt-400\n",
      "2.65 epochs, iter 450: train_loss = 1.007756710, param_norm = 36.533, grad_norm = 0.159, train_F1/valid_F1 = 0.521000/0.423848\n",
      "2.94 epochs, iter 500: train_loss = 1.009344697, param_norm = 37.801, grad_norm = 0.085, train_F1/valid_F1 = 0.513000/0.422846\n",
      "Saved parameters to ./all_features_10_seq_100hidden_2layer_drop08_longer/model.ckpt-500\n",
      "3.24 epochs, iter 550: train_loss = 1.007886767, param_norm = 38.983, grad_norm = 0.045, train_F1/valid_F1 = 0.528000/0.418838\n",
      "3.53 epochs, iter 600: train_loss = 1.004389644, param_norm = 39.971, grad_norm = 0.252, train_F1/valid_F1 = 0.523000/0.420842\n",
      "Saved parameters to ./all_features_10_seq_100hidden_2layer_drop08_longer/model.ckpt-600\n",
      "3.83 epochs, iter 650: train_loss = 1.008848548, param_norm = 40.794, grad_norm = 0.296, train_F1/valid_F1 = 0.513000/0.408818\n",
      "4.12 epochs, iter 700: train_loss = 0.988004506, param_norm = 42.359, grad_norm = 0.062, train_F1/valid_F1 = 0.520000/0.413828\n",
      "Saved parameters to ./all_features_10_seq_100hidden_2layer_drop08_longer/model.ckpt-700\n",
      "4.42 epochs, iter 750: train_loss = 0.985968709, param_norm = 43.154, grad_norm = 0.216, train_F1/valid_F1 = 0.527000/0.424850\n",
      "======New best valid F1. Saving to all_features_10_seq_100hidden_2layer_drop08_longer/best_ckpt/best.ckpt...\n",
      "4.71 epochs, iter 800: train_loss = 1.001268744, param_norm = 44.610, grad_norm = 0.172, train_F1/valid_F1 = 0.528000/0.331663\n",
      "Saved parameters to ./all_features_10_seq_100hidden_2layer_drop08_longer/model.ckpt-800\n",
      "5.01 epochs, iter 850: train_loss = 1.008737683, param_norm = 45.675, grad_norm = 0.345, train_F1/valid_F1 = 0.487000/0.414830\n",
      "5.30 epochs, iter 900: train_loss = 0.983647704, param_norm = 46.767, grad_norm = 0.246, train_F1/valid_F1 = 0.535000/0.423848\n",
      "Saved parameters to ./all_features_10_seq_100hidden_2layer_drop08_longer/model.ckpt-900\n",
      "5.59 epochs, iter 950: train_loss = 0.984235942, param_norm = 48.053, grad_norm = 0.133, train_F1/valid_F1 = 0.536000/0.409820\n",
      "5.89 epochs, iter 1000: train_loss = 0.972049236, param_norm = 48.939, grad_norm = 0.059, train_F1/valid_F1 = 0.520000/0.318637\n",
      "Saved parameters to ./all_features_10_seq_100hidden_2layer_drop08_longer/model.ckpt-1000\n",
      "6.18 epochs, iter 1050: train_loss = 0.992512584, param_norm = 49.700, grad_norm = 0.258, train_F1/valid_F1 = 0.541000/0.410822\n",
      "6.48 epochs, iter 1100: train_loss = 1.010908723, param_norm = 50.944, grad_norm = 0.416, train_F1/valid_F1 = 0.522000/0.413828\n",
      "Saved parameters to ./all_features_10_seq_100hidden_2layer_drop08_longer/model.ckpt-1100\n",
      "6.77 epochs, iter 1150: train_loss = 0.974551022, param_norm = 51.947, grad_norm = 0.039, train_F1/valid_F1 = 0.521000/0.246493\n",
      "7.07 epochs, iter 1200: train_loss = 0.977097392, param_norm = 52.766, grad_norm = 0.045, train_F1/valid_F1 = 0.526000/0.307615\n",
      "Saved parameters to ./all_features_10_seq_100hidden_2layer_drop08_longer/model.ckpt-1200\n",
      "7.36 epochs, iter 1250: train_loss = 0.988936901, param_norm = 53.546, grad_norm = 0.198, train_F1/valid_F1 = 0.530000/0.372745\n",
      "7.66 epochs, iter 1300: train_loss = 0.984390557, param_norm = 54.037, grad_norm = 0.112, train_F1/valid_F1 = 0.534000/0.338677\n",
      "Saved parameters to ./all_features_10_seq_100hidden_2layer_drop08_longer/model.ckpt-1300\n",
      "7.95 epochs, iter 1350: train_loss = 1.008711934, param_norm = 54.842, grad_norm = 0.355, train_F1/valid_F1 = 0.514000/0.423848\n",
      "8.24 epochs, iter 1400: train_loss = 0.979849339, param_norm = 56.072, grad_norm = 0.084, train_F1/valid_F1 = 0.530000/0.311623\n",
      "Saved parameters to ./all_features_10_seq_100hidden_2layer_drop08_longer/model.ckpt-1400\n",
      "8.54 epochs, iter 1450: train_loss = 1.000602007, param_norm = 56.496, grad_norm = 0.250, train_F1/valid_F1 = 0.527000/0.269539\n",
      "8.83 epochs, iter 1500: train_loss = 0.994444311, param_norm = 57.270, grad_norm = 0.175, train_F1/valid_F1 = 0.527000/0.286573\n",
      "Saved parameters to ./all_features_10_seq_100hidden_2layer_drop08_longer/model.ckpt-1500\n",
      "9.13 epochs, iter 1550: train_loss = 0.973990917, param_norm = 57.644, grad_norm = 0.066, train_F1/valid_F1 = 0.540000/0.336673\n",
      "9.42 epochs, iter 1600: train_loss = 0.980158806, param_norm = 58.537, grad_norm = 0.158, train_F1/valid_F1 = 0.529000/0.311623\n",
      "Saved parameters to ./all_features_10_seq_100hidden_2layer_drop08_longer/model.ckpt-1600\n",
      "9.72 epochs, iter 1650: train_loss = 0.972784221, param_norm = 59.265, grad_norm = 0.057, train_F1/valid_F1 = 0.530000/0.335671\n",
      "10.01 epochs, iter 1700: train_loss = 0.972229660, param_norm = 59.904, grad_norm = 0.045, train_F1/valid_F1 = 0.533000/0.406814\n",
      "Saved parameters to ./all_features_10_seq_100hidden_2layer_drop08_longer/model.ckpt-1700\n",
      "10.31 epochs, iter 1750: train_loss = 0.982099950, param_norm = 60.653, grad_norm = 0.215, train_F1/valid_F1 = 0.533000/0.394790\n",
      "10.60 epochs, iter 1800: train_loss = 0.981579781, param_norm = 61.560, grad_norm = 0.048, train_F1/valid_F1 = 0.537000/0.356713\n",
      "Saved parameters to ./all_features_10_seq_100hidden_2layer_drop08_longer/model.ckpt-1800\n",
      "10.89 epochs, iter 1850: train_loss = 0.971668720, param_norm = 62.016, grad_norm = 0.115, train_F1/valid_F1 = 0.538000/0.338677\n",
      "11.19 epochs, iter 1900: train_loss = 0.980569363, param_norm = 62.674, grad_norm = 0.242, train_F1/valid_F1 = 0.531000/0.341683\n",
      "Saved parameters to ./all_features_10_seq_100hidden_2layer_drop08_longer/model.ckpt-1900\n",
      "11.48 epochs, iter 1950: train_loss = 0.971478760, param_norm = 63.296, grad_norm = 0.084, train_F1/valid_F1 = 0.543000/0.299599\n",
      "11.78 epochs, iter 2000: train_loss = 0.990157604, param_norm = 63.878, grad_norm = 0.266, train_F1/valid_F1 = 0.525000/0.389780\n",
      "Saved parameters to ./all_features_10_seq_100hidden_2layer_drop08_longer/model.ckpt-2000\n",
      "12.07 epochs, iter 2050: train_loss = 0.975900888, param_norm = 64.559, grad_norm = 0.081, train_F1/valid_F1 = 0.531000/0.289579\n",
      "12.37 epochs, iter 2100: train_loss = 0.971596956, param_norm = 65.238, grad_norm = 0.093, train_F1/valid_F1 = 0.534000/0.323647\n",
      "Saved parameters to ./all_features_10_seq_100hidden_2layer_drop08_longer/model.ckpt-2100\n",
      "12.66 epochs, iter 2150: train_loss = 0.967151046, param_norm = 65.624, grad_norm = 0.069, train_F1/valid_F1 = 0.526000/0.297595\n",
      "12.96 epochs, iter 2200: train_loss = 0.993492067, param_norm = 66.212, grad_norm = 0.314, train_F1/valid_F1 = 0.531000/0.411824\n",
      "Saved parameters to ./all_features_10_seq_100hidden_2layer_drop08_longer/model.ckpt-2200\n",
      "13.25 epochs, iter 2250: train_loss = 0.986174047, param_norm = 66.866, grad_norm = 0.053, train_F1/valid_F1 = 0.535000/0.267535\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13.54 epochs, iter 2300: train_loss = 0.971173823, param_norm = 67.516, grad_norm = 0.073, train_F1/valid_F1 = 0.522000/0.265531\n",
      "Saved parameters to ./all_features_10_seq_100hidden_2layer_drop08_longer/model.ckpt-2300\n",
      "13.84 epochs, iter 2350: train_loss = 0.960855663, param_norm = 68.029, grad_norm = 0.099, train_F1/valid_F1 = 0.548000/0.371743\n",
      "14.13 epochs, iter 2400: train_loss = 0.969024658, param_norm = 68.511, grad_norm = 0.057, train_F1/valid_F1 = 0.532000/0.245491\n",
      "Saved parameters to ./all_features_10_seq_100hidden_2layer_drop08_longer/model.ckpt-2400\n",
      "14.43 epochs, iter 2450: train_loss = 0.997358859, param_norm = 69.126, grad_norm = 0.253, train_F1/valid_F1 = 0.541000/0.415832\n",
      "14.72 epochs, iter 2500: train_loss = 0.967388093, param_norm = 69.901, grad_norm = 0.081, train_F1/valid_F1 = 0.545000/0.258517\n",
      "Saved parameters to ./all_features_10_seq_100hidden_2layer_drop08_longer/model.ckpt-2500\n",
      "15.02 epochs, iter 2550: train_loss = 0.968555093, param_norm = 70.154, grad_norm = 0.137, train_F1/valid_F1 = 0.532000/0.282565\n",
      "15.31 epochs, iter 2600: train_loss = 0.970384598, param_norm = 70.798, grad_norm = 0.176, train_F1/valid_F1 = 0.530000/0.293587\n",
      "Saved parameters to ./all_features_10_seq_100hidden_2layer_drop08_longer/model.ckpt-2600\n",
      "15.61 epochs, iter 2650: train_loss = 0.976491332, param_norm = 71.271, grad_norm = 0.090, train_F1/valid_F1 = 0.528000/0.322645\n",
      "15.90 epochs, iter 2700: train_loss = 0.967338860, param_norm = 72.255, grad_norm = 0.121, train_F1/valid_F1 = 0.529000/0.290581\n",
      "Saved parameters to ./all_features_10_seq_100hidden_2layer_drop08_longer/model.ckpt-2700\n",
      "16.19 epochs, iter 2750: train_loss = 0.984340608, param_norm = 72.270, grad_norm = 0.174, train_F1/valid_F1 = 0.540000/0.374749\n",
      "16.49 epochs, iter 2800: train_loss = 0.988403320, param_norm = 73.081, grad_norm = 0.211, train_F1/valid_F1 = 0.544000/0.398798\n",
      "Saved parameters to ./all_features_10_seq_100hidden_2layer_drop08_longer/model.ckpt-2800\n",
      "16.78 epochs, iter 2850: train_loss = 0.968761981, param_norm = 73.511, grad_norm = 0.052, train_F1/valid_F1 = 0.539000/0.303607\n",
      "17.08 epochs, iter 2900: train_loss = 0.990329385, param_norm = 74.412, grad_norm = 0.352, train_F1/valid_F1 = 0.521000/0.370741\n",
      "Saved parameters to ./all_features_10_seq_100hidden_2layer_drop08_longer/model.ckpt-2900\n",
      "17.37 epochs, iter 2950: train_loss = 0.964161634, param_norm = 74.709, grad_norm = 0.135, train_F1/valid_F1 = 0.527000/0.284569\n",
      "17.67 epochs, iter 3000: train_loss = 0.972771108, param_norm = 75.510, grad_norm = 0.040, train_F1/valid_F1 = 0.534000/0.276553\n",
      "Saved parameters to ./all_features_10_seq_100hidden_2layer_drop08_longer/model.ckpt-3000\n",
      "17.96 epochs, iter 3050: train_loss = 0.979273558, param_norm = 75.915, grad_norm = 0.254, train_F1/valid_F1 = 0.541000/0.393788\n",
      "18.26 epochs, iter 3100: train_loss = 0.988829970, param_norm = 76.601, grad_norm = 0.268, train_F1/valid_F1 = 0.531000/0.288577\n",
      "Saved parameters to ./all_features_10_seq_100hidden_2layer_drop08_longer/model.ckpt-3100\n",
      "18.55 epochs, iter 3150: train_loss = 0.971072614, param_norm = 77.243, grad_norm = 0.062, train_F1/valid_F1 = 0.525000/0.332665\n",
      "18.84 epochs, iter 3200: train_loss = 0.974930823, param_norm = 77.889, grad_norm = 0.065, train_F1/valid_F1 = 0.532000/0.269539\n",
      "Saved parameters to ./all_features_10_seq_100hidden_2layer_drop08_longer/model.ckpt-3200\n",
      "19.14 epochs, iter 3250: train_loss = 0.957268298, param_norm = 78.237, grad_norm = 0.103, train_F1/valid_F1 = 0.534000/0.390782\n",
      "19.43 epochs, iter 3300: train_loss = 0.975370467, param_norm = 78.811, grad_norm = 0.097, train_F1/valid_F1 = 0.539000/0.311623\n",
      "Saved parameters to ./all_features_10_seq_100hidden_2layer_drop08_longer/model.ckpt-3300\n",
      "19.73 epochs, iter 3350: train_loss = 0.979302347, param_norm = 79.151, grad_norm = 0.205, train_F1/valid_F1 = 0.529000/0.278557\n",
      "20.02 epochs, iter 3400: train_loss = 0.947694838, param_norm = 79.666, grad_norm = 0.048, train_F1/valid_F1 = 0.544000/0.386774\n",
      "Saved parameters to ./all_features_10_seq_100hidden_2layer_drop08_longer/model.ckpt-3400\n",
      "20.32 epochs, iter 3450: train_loss = 0.973403573, param_norm = 80.063, grad_norm = 0.078, train_F1/valid_F1 = 0.537000/0.382766\n",
      "20.61 epochs, iter 3500: train_loss = 0.960411191, param_norm = 80.234, grad_norm = 0.036, train_F1/valid_F1 = 0.543000/0.385772\n",
      "Saved parameters to ./all_features_10_seq_100hidden_2layer_drop08_longer/model.ckpt-3500\n",
      "20.91 epochs, iter 3550: train_loss = 0.960108161, param_norm = 80.962, grad_norm = 0.086, train_F1/valid_F1 = 0.536000/0.373747\n",
      "21.20 epochs, iter 3600: train_loss = 0.990162790, param_norm = 81.322, grad_norm = 0.174, train_F1/valid_F1 = 0.526000/0.348697\n",
      "Saved parameters to ./all_features_10_seq_100hidden_2layer_drop08_longer/model.ckpt-3600\n",
      "21.49 epochs, iter 3650: train_loss = 0.976062119, param_norm = 82.208, grad_norm = 0.130, train_F1/valid_F1 = 0.538000/0.279559\n",
      "21.79 epochs, iter 3700: train_loss = 0.965609372, param_norm = 82.680, grad_norm = 0.053, train_F1/valid_F1 = 0.542000/0.294589\n",
      "Saved parameters to ./all_features_10_seq_100hidden_2layer_drop08_longer/model.ckpt-3700\n",
      "22.08 epochs, iter 3750: train_loss = 0.984632909, param_norm = 83.661, grad_norm = 0.240, train_F1/valid_F1 = 0.533000/0.397796\n",
      "22.38 epochs, iter 3800: train_loss = 0.965237439, param_norm = 83.502, grad_norm = 0.063, train_F1/valid_F1 = 0.550000/0.321643\n",
      "Saved parameters to ./all_features_10_seq_100hidden_2layer_drop08_longer/model.ckpt-3800\n",
      "22.67 epochs, iter 3850: train_loss = 0.965497136, param_norm = 84.219, grad_norm = 0.223, train_F1/valid_F1 = 0.532000/0.347695\n",
      "22.97 epochs, iter 3900: train_loss = 0.959380031, param_norm = 84.772, grad_norm = 0.036, train_F1/valid_F1 = 0.547000/0.286573\n",
      "Saved parameters to ./all_features_10_seq_100hidden_2layer_drop08_longer/model.ckpt-3900\n",
      "23.26 epochs, iter 3950: train_loss = 0.975181282, param_norm = 85.759, grad_norm = 0.175, train_F1/valid_F1 = 0.539000/0.347695\n",
      "23.56 epochs, iter 4000: train_loss = 0.963033855, param_norm = 85.808, grad_norm = 0.030, train_F1/valid_F1 = 0.535000/0.261523\n",
      "Saved parameters to ./all_features_10_seq_100hidden_2layer_drop08_longer/model.ckpt-4000\n",
      "23.85 epochs, iter 4050: train_loss = 0.968212545, param_norm = 86.575, grad_norm = 0.110, train_F1/valid_F1 = 0.539000/0.289579\n",
      "24.14 epochs, iter 4100: train_loss = 0.969658792, param_norm = 86.846, grad_norm = 0.063, train_F1/valid_F1 = 0.531000/0.218437\n",
      "Saved parameters to ./all_features_10_seq_100hidden_2layer_drop08_longer/model.ckpt-4100\n",
      "24.44 epochs, iter 4150: train_loss = 0.947040260, param_norm = 86.978, grad_norm = 0.055, train_F1/valid_F1 = 0.543000/0.294589\n",
      "24.73 epochs, iter 4200: train_loss = 1.428149700, param_norm = 87.636, grad_norm = 7.361, train_F1/valid_F1 = 0.520000/0.301603\n",
      "Saved parameters to ./all_features_10_seq_100hidden_2layer_drop08_longer/model.ckpt-4200\n",
      "25.03 epochs, iter 4250: train_loss = 0.961088955, param_norm = 87.690, grad_norm = 0.107, train_F1/valid_F1 = 0.532000/0.290581\n",
      "25.32 epochs, iter 4300: train_loss = 0.954327762, param_norm = 87.560, grad_norm = 0.030, train_F1/valid_F1 = 0.544000/0.317635\n",
      "Saved parameters to ./all_features_10_seq_100hidden_2layer_drop08_longer/model.ckpt-4300\n",
      "25.62 epochs, iter 4350: train_loss = 0.956045210, param_norm = 88.054, grad_norm = 0.083, train_F1/valid_F1 = 0.535000/0.293587\n",
      "25.91 epochs, iter 4400: train_loss = 0.977953732, param_norm = 87.989, grad_norm = 0.179, train_F1/valid_F1 = 0.538000/0.310621\n",
      "Saved parameters to ./all_features_10_seq_100hidden_2layer_drop08_longer/model.ckpt-4400\n",
      "26.21 epochs, iter 4450: train_loss = 0.964178681, param_norm = 88.469, grad_norm = 0.119, train_F1/valid_F1 = 0.539000/0.263527\n",
      "26.50 epochs, iter 4500: train_loss = 0.953293324, param_norm = 88.538, grad_norm = 0.222, train_F1/valid_F1 = 0.545000/0.415832\n",
      "Saved parameters to ./all_features_10_seq_100hidden_2layer_drop08_longer/model.ckpt-4500\n",
      "26.79 epochs, iter 4550: train_loss = 0.956518710, param_norm = 88.913, grad_norm = 0.130, train_F1/valid_F1 = 0.544000/0.410822\n",
      "27.09 epochs, iter 4600: train_loss = 0.954862952, param_norm = 89.269, grad_norm = 0.036, train_F1/valid_F1 = 0.541000/0.295591\n",
      "Saved parameters to ./all_features_10_seq_100hidden_2layer_drop08_longer/model.ckpt-4600\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27.38 epochs, iter 4650: train_loss = 0.966429949, param_norm = 89.603, grad_norm = 0.340, train_F1/valid_F1 = 0.547000/0.398798\n",
      "27.68 epochs, iter 4700: train_loss = 1.488441944, param_norm = 89.763, grad_norm = 31.863, train_F1/valid_F1 = 0.539000/0.340681\n",
      "Saved parameters to ./all_features_10_seq_100hidden_2layer_drop08_longer/model.ckpt-4700\n",
      "27.97 epochs, iter 4750: train_loss = 0.960404217, param_norm = 89.782, grad_norm = 0.144, train_F1/valid_F1 = 0.549000/0.319639\n",
      "28.27 epochs, iter 4800: train_loss = 0.960650146, param_norm = 89.761, grad_norm = 0.029, train_F1/valid_F1 = 0.533000/0.294589\n",
      "Saved parameters to ./all_features_10_seq_100hidden_2layer_drop08_longer/model.ckpt-4800\n",
      "28.56 epochs, iter 4850: train_loss = 0.947577417, param_norm = 89.907, grad_norm = 0.087, train_F1/valid_F1 = 0.530000/0.291583\n",
      "28.86 epochs, iter 4900: train_loss = 0.957210958, param_norm = 89.900, grad_norm = 0.094, train_F1/valid_F1 = 0.529000/0.304609\n",
      "Saved parameters to ./all_features_10_seq_100hidden_2layer_drop08_longer/model.ckpt-4900\n",
      "29.15 epochs, iter 4950: train_loss = 0.956287026, param_norm = 90.139, grad_norm = 0.138, train_F1/valid_F1 = 0.545000/0.296593\n",
      "29.44 epochs, iter 5000: train_loss = 0.955062866, param_norm = 90.207, grad_norm = 0.212, train_F1/valid_F1 = 0.531000/0.366733\n",
      "Saved parameters to ./all_features_10_seq_100hidden_2layer_drop08_longer/model.ckpt-5000\n",
      "29.74 epochs, iter 5050: train_loss = 0.950841844, param_norm = 90.324, grad_norm = 0.088, train_F1/valid_F1 = 0.543000/0.371743\n",
      "30.03 epochs, iter 5100: train_loss = 0.955648780, param_norm = 90.313, grad_norm = 0.061, train_F1/valid_F1 = 0.537000/0.348697\n",
      "Saved parameters to ./all_features_10_seq_100hidden_2layer_drop08_longer/model.ckpt-5100\n",
      "30.33 epochs, iter 5150: train_loss = 0.957762182, param_norm = 90.371, grad_norm = 0.082, train_F1/valid_F1 = 0.539000/0.280561\n",
      "30.62 epochs, iter 5200: train_loss = 0.958725631, param_norm = 90.452, grad_norm = 0.146, train_F1/valid_F1 = 0.529000/0.333667\n",
      "Saved parameters to ./all_features_10_seq_100hidden_2layer_drop08_longer/model.ckpt-5200\n",
      "30.92 epochs, iter 5250: train_loss = 0.951269984, param_norm = 90.470, grad_norm = 0.051, train_F1/valid_F1 = 0.531000/0.299599\n",
      "31.21 epochs, iter 5300: train_loss = 0.946944952, param_norm = 90.508, grad_norm = 0.207, train_F1/valid_F1 = 0.543000/0.301603\n",
      "Saved parameters to ./all_features_10_seq_100hidden_2layer_drop08_longer/model.ckpt-5300\n",
      "31.51 epochs, iter 5350: train_loss = 0.955824435, param_norm = 90.590, grad_norm = 0.176, train_F1/valid_F1 = 0.536000/0.361723\n",
      "31.80 epochs, iter 5400: train_loss = 0.947996140, param_norm = 90.547, grad_norm = 0.139, train_F1/valid_F1 = 0.543000/0.264529\n",
      "Saved parameters to ./all_features_10_seq_100hidden_2layer_drop08_longer/model.ckpt-5400\n",
      "32.09 epochs, iter 5450: train_loss = 0.954105735, param_norm = 90.603, grad_norm = 2.056, train_F1/valid_F1 = 0.532000/0.288577\n",
      "32.39 epochs, iter 5500: train_loss = 0.947624505, param_norm = 90.647, grad_norm = 0.079, train_F1/valid_F1 = 0.548000/0.390782\n",
      "Saved parameters to ./all_features_10_seq_100hidden_2layer_drop08_longer/model.ckpt-5500\n",
      "32.68 epochs, iter 5550: train_loss = 0.959570825, param_norm = 90.792, grad_norm = 0.110, train_F1/valid_F1 = 0.532000/0.402806\n",
      "32.98 epochs, iter 5600: train_loss = 0.955308199, param_norm = 90.813, grad_norm = 0.052, train_F1/valid_F1 = 0.543000/0.350701\n",
      "Saved parameters to ./all_features_10_seq_100hidden_2layer_drop08_longer/model.ckpt-5600\n",
      "33.27 epochs, iter 5650: train_loss = 0.953491330, param_norm = 90.814, grad_norm = 0.199, train_F1/valid_F1 = 0.545000/0.325651\n",
      "33.57 epochs, iter 5700: train_loss = 0.961974502, param_norm = 90.864, grad_norm = 0.196, train_F1/valid_F1 = 0.528000/0.351703\n",
      "Saved parameters to ./all_features_10_seq_100hidden_2layer_drop08_longer/model.ckpt-5700\n",
      "33.86 epochs, iter 5750: train_loss = 0.956875741, param_norm = 91.017, grad_norm = 0.169, train_F1/valid_F1 = 0.540000/0.279559\n",
      "34.16 epochs, iter 5800: train_loss = 0.954547465, param_norm = 91.051, grad_norm = 0.092, train_F1/valid_F1 = 0.536000/0.287575\n",
      "Saved parameters to ./all_features_10_seq_100hidden_2layer_drop08_longer/model.ckpt-5800\n",
      "34.45 epochs, iter 5850: train_loss = 0.952067018, param_norm = 91.069, grad_norm = 0.172, train_F1/valid_F1 = 0.544000/0.258517\n",
      "34.74 epochs, iter 5900: train_loss = 0.958048046, param_norm = 91.145, grad_norm = 0.198, train_F1/valid_F1 = 0.533000/0.347695\n",
      "Saved parameters to ./all_features_10_seq_100hidden_2layer_drop08_longer/model.ckpt-5900\n",
      "35.04 epochs, iter 5950: train_loss = 0.953659534, param_norm = 91.313, grad_norm = 0.121, train_F1/valid_F1 = 0.533000/0.304609\n",
      "35.33 epochs, iter 6000: train_loss = 0.945318639, param_norm = 91.359, grad_norm = 0.036, train_F1/valid_F1 = 0.554000/0.270541\n",
      "Saved parameters to ./all_features_10_seq_100hidden_2layer_drop08_longer/model.ckpt-6000\n",
      "35.63 epochs, iter 6050: train_loss = 0.957735479, param_norm = 91.587, grad_norm = 0.161, train_F1/valid_F1 = 0.548000/0.277555\n",
      "35.92 epochs, iter 6100: train_loss = 0.949411988, param_norm = 91.640, grad_norm = 0.130, train_F1/valid_F1 = 0.550000/0.341683\n",
      "Saved parameters to ./all_features_10_seq_100hidden_2layer_drop08_longer/model.ckpt-6100\n",
      "36.22 epochs, iter 6150: train_loss = 0.950364351, param_norm = 91.689, grad_norm = 0.080, train_F1/valid_F1 = 0.546000/0.288577\n",
      "36.51 epochs, iter 6200: train_loss = 0.956394851, param_norm = 91.722, grad_norm = 2.818, train_F1/valid_F1 = 0.546000/0.319639\n",
      "Saved parameters to ./all_features_10_seq_100hidden_2layer_drop08_longer/model.ckpt-6200\n",
      "36.81 epochs, iter 6250: train_loss = 0.960285902, param_norm = 91.671, grad_norm = 0.092, train_F1/valid_F1 = 0.541000/0.261523\n",
      "37.10 epochs, iter 6300: train_loss = 0.943552852, param_norm = 91.717, grad_norm = 0.051, train_F1/valid_F1 = 0.563000/0.277555\n",
      "Saved parameters to ./all_features_10_seq_100hidden_2layer_drop08_longer/model.ckpt-6300\n",
      "37.39 epochs, iter 6350: train_loss = 0.950582862, param_norm = 91.791, grad_norm = 0.161, train_F1/valid_F1 = 0.548000/0.326653\n",
      "37.69 epochs, iter 6400: train_loss = 0.960996330, param_norm = 91.917, grad_norm = 0.169, train_F1/valid_F1 = 0.552000/0.291583\n",
      "Saved parameters to ./all_features_10_seq_100hidden_2layer_drop08_longer/model.ckpt-6400\n",
      "37.98 epochs, iter 6450: train_loss = 0.961981356, param_norm = 92.040, grad_norm = 0.179, train_F1/valid_F1 = 0.532000/0.297595\n",
      "38.28 epochs, iter 6500: train_loss = 0.957200110, param_norm = 92.076, grad_norm = 0.146, train_F1/valid_F1 = 0.523000/0.288577\n",
      "Saved parameters to ./all_features_10_seq_100hidden_2layer_drop08_longer/model.ckpt-6500\n",
      "38.57 epochs, iter 6550: train_loss = 0.953739405, param_norm = 92.243, grad_norm = 0.050, train_F1/valid_F1 = 0.545000/0.265531\n",
      "38.87 epochs, iter 6600: train_loss = 0.964504004, param_norm = 92.305, grad_norm = 0.177, train_F1/valid_F1 = 0.538000/0.312625\n",
      "Saved parameters to ./all_features_10_seq_100hidden_2layer_drop08_longer/model.ckpt-6600\n",
      "39.16 epochs, iter 6650: train_loss = 0.954608738, param_norm = 92.343, grad_norm = 0.088, train_F1/valid_F1 = 0.552000/0.290581\n",
      "39.46 epochs, iter 6700: train_loss = 0.950938940, param_norm = 92.403, grad_norm = 0.071, train_F1/valid_F1 = 0.540000/0.350701\n",
      "Saved parameters to ./all_features_10_seq_100hidden_2layer_drop08_longer/model.ckpt-6700\n",
      "39.75 epochs, iter 6750: train_loss = 0.952428401, param_norm = 92.579, grad_norm = 0.122, train_F1/valid_F1 = 0.545000/0.301603\n",
      "40.04 epochs, iter 6800: train_loss = 0.947078049, param_norm = 92.528, grad_norm = 0.037, train_F1/valid_F1 = 0.551000/0.270541\n",
      "Saved parameters to ./all_features_10_seq_100hidden_2layer_drop08_longer/model.ckpt-6800\n",
      "40.34 epochs, iter 6850: train_loss = 0.957465649, param_norm = 92.548, grad_norm = 0.206, train_F1/valid_F1 = 0.547000/0.315631\n",
      "40.63 epochs, iter 6900: train_loss = 0.954157114, param_norm = 92.719, grad_norm = 0.144, train_F1/valid_F1 = 0.544000/0.288577\n",
      "Saved parameters to ./all_features_10_seq_100hidden_2layer_drop08_longer/model.ckpt-6900\n",
      "40.93 epochs, iter 6950: train_loss = 0.953223586, param_norm = 92.735, grad_norm = 0.471, train_F1/valid_F1 = 0.532000/0.266533\n",
      "41.22 epochs, iter 7000: train_loss = 0.949896872, param_norm = 92.798, grad_norm = 0.246, train_F1/valid_F1 = 0.551000/0.286573\n",
      "Saved parameters to ./all_features_10_seq_100hidden_2layer_drop08_longer/model.ckpt-7000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "41.52 epochs, iter 7050: train_loss = 0.951350152, param_norm = 92.911, grad_norm = 0.172, train_F1/valid_F1 = 0.540000/0.338677\n",
      "41.81 epochs, iter 7100: train_loss = 0.952075064, param_norm = 92.960, grad_norm = 0.210, train_F1/valid_F1 = 0.525000/0.276553\n",
      "Saved parameters to ./all_features_10_seq_100hidden_2layer_drop08_longer/model.ckpt-7100\n",
      "42.11 epochs, iter 7150: train_loss = 0.955510139, param_norm = 93.140, grad_norm = 0.068, train_F1/valid_F1 = 0.534000/0.277555\n",
      "42.40 epochs, iter 7200: train_loss = 0.969266832, param_norm = 93.110, grad_norm = 0.415, train_F1/valid_F1 = 0.542000/0.252505\n",
      "Saved parameters to ./all_features_10_seq_100hidden_2layer_drop08_longer/model.ckpt-7200\n",
      "42.69 epochs, iter 7250: train_loss = 0.972505748, param_norm = 93.133, grad_norm = 1.572, train_F1/valid_F1 = 0.550000/0.293587\n",
      "42.99 epochs, iter 7300: train_loss = 0.940819860, param_norm = 93.224, grad_norm = 0.057, train_F1/valid_F1 = 0.536000/0.268537\n",
      "Saved parameters to ./all_features_10_seq_100hidden_2layer_drop08_longer/model.ckpt-7300\n",
      "43.28 epochs, iter 7350: train_loss = 0.945920408, param_norm = 93.230, grad_norm = 0.236, train_F1/valid_F1 = 0.545000/0.306613\n",
      "43.58 epochs, iter 7400: train_loss = 0.969701648, param_norm = 93.300, grad_norm = 0.245, train_F1/valid_F1 = 0.544000/0.252505\n",
      "Saved parameters to ./all_features_10_seq_100hidden_2layer_drop08_longer/model.ckpt-7400\n",
      "43.87 epochs, iter 7450: train_loss = 0.977448583, param_norm = 93.452, grad_norm = 0.583, train_F1/valid_F1 = 0.547000/0.314629\n",
      "44.17 epochs, iter 7500: train_loss = 0.949403048, param_norm = 93.416, grad_norm = 0.255, train_F1/valid_F1 = 0.549000/0.313627\n",
      "Saved parameters to ./all_features_10_seq_100hidden_2layer_drop08_longer/model.ckpt-7500\n",
      "44.46 epochs, iter 7550: train_loss = 0.954866230, param_norm = 93.571, grad_norm = 0.207, train_F1/valid_F1 = 0.557000/0.273547\n",
      "44.76 epochs, iter 7600: train_loss = 0.950451195, param_norm = 93.440, grad_norm = 0.119, train_F1/valid_F1 = 0.543000/0.299599\n",
      "Saved parameters to ./all_features_10_seq_100hidden_2layer_drop08_longer/model.ckpt-7600\n",
      "45.05 epochs, iter 7650: train_loss = 0.952227771, param_norm = 93.459, grad_norm = 0.079, train_F1/valid_F1 = 0.543000/0.309619\n",
      "45.34 epochs, iter 7700: train_loss = 0.953207254, param_norm = 93.594, grad_norm = 0.194, train_F1/valid_F1 = 0.543000/0.257515\n",
      "Saved parameters to ./all_features_10_seq_100hidden_2layer_drop08_longer/model.ckpt-7700\n",
      "45.64 epochs, iter 7750: train_loss = 0.944470525, param_norm = 93.666, grad_norm = 0.116, train_F1/valid_F1 = 0.549000/0.261523\n",
      "45.93 epochs, iter 7800: train_loss = 0.942320287, param_norm = 93.813, grad_norm = 0.151, train_F1/valid_F1 = 0.539000/0.256513\n",
      "Saved parameters to ./all_features_10_seq_100hidden_2layer_drop08_longer/model.ckpt-7800\n",
      "46.23 epochs, iter 7850: train_loss = 0.954084694, param_norm = 93.716, grad_norm = 0.214, train_F1/valid_F1 = 0.539000/0.305611\n",
      "46.52 epochs, iter 7900: train_loss = 0.945198238, param_norm = 93.869, grad_norm = 0.114, train_F1/valid_F1 = 0.550000/0.297595\n",
      "Saved parameters to ./all_features_10_seq_100hidden_2layer_drop08_longer/model.ckpt-7900\n",
      "46.82 epochs, iter 7950: train_loss = 0.954604626, param_norm = 93.950, grad_norm = 0.196, train_F1/valid_F1 = 0.541000/0.279559\n",
      "47.11 epochs, iter 8000: train_loss = 0.951259732, param_norm = 94.018, grad_norm = 0.177, train_F1/valid_F1 = 0.539000/0.320641\n",
      "Saved parameters to ./all_features_10_seq_100hidden_2layer_drop08_longer/model.ckpt-8000\n",
      "47.41 epochs, iter 8050: train_loss = 0.949508548, param_norm = 94.074, grad_norm = 0.195, train_F1/valid_F1 = 0.544000/0.271543\n",
      "47.70 epochs, iter 8100: train_loss = 0.945042551, param_norm = 94.208, grad_norm = 0.092, train_F1/valid_F1 = 0.549000/0.251503\n",
      "Saved parameters to ./all_features_10_seq_100hidden_2layer_drop08_longer/model.ckpt-8100\n",
      "47.99 epochs, iter 8150: train_loss = 0.949129760, param_norm = 94.433, grad_norm = 0.046, train_F1/valid_F1 = 0.547000/0.287575\n",
      "48.29 epochs, iter 8200: train_loss = 0.969993055, param_norm = 94.481, grad_norm = 0.167, train_F1/valid_F1 = 0.528000/0.294589\n",
      "Saved parameters to ./all_features_10_seq_100hidden_2layer_drop08_longer/model.ckpt-8200\n",
      "48.58 epochs, iter 8250: train_loss = 0.978396595, param_norm = 94.529, grad_norm = 2.829, train_F1/valid_F1 = 0.547000/0.324649\n",
      "48.88 epochs, iter 8300: train_loss = 0.936940074, param_norm = 94.590, grad_norm = 0.056, train_F1/valid_F1 = 0.543000/0.279559\n",
      "Saved parameters to ./all_features_10_seq_100hidden_2layer_drop08_longer/model.ckpt-8300\n",
      "49.17 epochs, iter 8350: train_loss = 0.944180667, param_norm = 94.617, grad_norm = 0.037, train_F1/valid_F1 = 0.534000/0.289579\n",
      "49.47 epochs, iter 8400: train_loss = 0.948575020, param_norm = 94.769, grad_norm = 0.242, train_F1/valid_F1 = 0.530000/0.371743\n",
      "Saved parameters to ./all_features_10_seq_100hidden_2layer_drop08_longer/model.ckpt-8400\n",
      "49.76 epochs, iter 8450: train_loss = 0.942152798, param_norm = 95.041, grad_norm = 0.086, train_F1/valid_F1 = 0.545000/0.349699\n",
      "50.06 epochs, iter 8500: train_loss = 0.950453341, param_norm = 94.971, grad_norm = 0.231, train_F1/valid_F1 = 0.554000/0.313627\n",
      "Saved parameters to ./all_features_10_seq_100hidden_2layer_drop08_longer/model.ckpt-8500\n",
      "50.35 epochs, iter 8550: train_loss = 0.958626688, param_norm = 95.026, grad_norm = 0.208, train_F1/valid_F1 = 0.528000/0.359719\n",
      "50.64 epochs, iter 8600: train_loss = 0.944025636, param_norm = 95.095, grad_norm = 0.372, train_F1/valid_F1 = 0.549000/0.268537\n",
      "Saved parameters to ./all_features_10_seq_100hidden_2layer_drop08_longer/model.ckpt-8600\n",
      "50.94 epochs, iter 8650: train_loss = 0.967786610, param_norm = 95.218, grad_norm = 6.480, train_F1/valid_F1 = 0.544000/0.302605\n",
      "51.23 epochs, iter 8700: train_loss = 0.987109065, param_norm = 95.267, grad_norm = 1.208, train_F1/valid_F1 = 0.534000/0.252505\n",
      "Saved parameters to ./all_features_10_seq_100hidden_2layer_drop08_longer/model.ckpt-8700\n",
      "51.53 epochs, iter 8750: train_loss = 0.949721217, param_norm = 95.310, grad_norm = 0.597, train_F1/valid_F1 = 0.544000/0.299599\n",
      "51.82 epochs, iter 8800: train_loss = 0.959770620, param_norm = 95.518, grad_norm = 1.033, train_F1/valid_F1 = 0.555000/0.290581\n",
      "Saved parameters to ./all_features_10_seq_100hidden_2layer_drop08_longer/model.ckpt-8800\n",
      "52.12 epochs, iter 8850: train_loss = 0.948566377, param_norm = 95.700, grad_norm = 0.515, train_F1/valid_F1 = 0.546000/0.265531\n",
      "52.41 epochs, iter 8900: train_loss = 0.954019070, param_norm = 95.831, grad_norm = 2.168, train_F1/valid_F1 = 0.550000/0.256513\n",
      "Saved parameters to ./all_features_10_seq_100hidden_2layer_drop08_longer/model.ckpt-8900\n",
      "52.71 epochs, iter 8950: train_loss = 0.956490517, param_norm = 95.839, grad_norm = 0.084, train_F1/valid_F1 = 0.541000/0.243487\n",
      "53.00 epochs, iter 9000: train_loss = 0.974200606, param_norm = 95.858, grad_norm = 4.215, train_F1/valid_F1 = 0.554000/0.299599\n",
      "Saved parameters to ./all_features_10_seq_100hidden_2layer_drop08_longer/model.ckpt-9000\n",
      "53.29 epochs, iter 9050: train_loss = 0.983745217, param_norm = 96.015, grad_norm = 1.734, train_F1/valid_F1 = 0.542000/0.287575\n",
      "53.59 epochs, iter 9100: train_loss = 0.985644698, param_norm = 96.000, grad_norm = 1.939, train_F1/valid_F1 = 0.547000/0.252505\n",
      "Saved parameters to ./all_features_10_seq_100hidden_2layer_drop08_longer/model.ckpt-9100\n",
      "53.88 epochs, iter 9150: train_loss = 0.956034184, param_norm = 96.040, grad_norm = 0.344, train_F1/valid_F1 = 0.544000/0.280561\n",
      "54.18 epochs, iter 9200: train_loss = 0.948974252, param_norm = 96.103, grad_norm = 0.205, train_F1/valid_F1 = 0.551000/0.299599\n",
      "Saved parameters to ./all_features_10_seq_100hidden_2layer_drop08_longer/model.ckpt-9200\n",
      "54.47 epochs, iter 9250: train_loss = 0.935876966, param_norm = 96.215, grad_norm = 0.085, train_F1/valid_F1 = 0.550000/0.235471\n",
      "54.77 epochs, iter 9300: train_loss = 0.960507333, param_norm = 96.327, grad_norm = 0.181, train_F1/valid_F1 = 0.551000/0.257515\n",
      "Saved parameters to ./all_features_10_seq_100hidden_2layer_drop08_longer/model.ckpt-9300\n",
      "55.06 epochs, iter 9350: train_loss = 0.962361932, param_norm = 96.460, grad_norm = 0.515, train_F1/valid_F1 = 0.547000/0.255511\n",
      "55.36 epochs, iter 9400: train_loss = 0.937735796, param_norm = 96.563, grad_norm = 0.094, train_F1/valid_F1 = 0.544000/0.268537\n",
      "Saved parameters to ./all_features_10_seq_100hidden_2layer_drop08_longer/model.ckpt-9400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "55.65 epochs, iter 9450: train_loss = 0.939069092, param_norm = 96.735, grad_norm = 0.084, train_F1/valid_F1 = 0.546000/0.262525\n",
      "55.94 epochs, iter 9500: train_loss = 0.935321510, param_norm = 96.795, grad_norm = 0.216, train_F1/valid_F1 = 0.559000/0.259519\n",
      "Saved parameters to ./all_features_10_seq_100hidden_2layer_drop08_longer/model.ckpt-9500\n",
      "56.24 epochs, iter 9550: train_loss = 0.943100929, param_norm = 96.941, grad_norm = 0.083, train_F1/valid_F1 = 0.554000/0.256513\n",
      "56.53 epochs, iter 9600: train_loss = 0.953681648, param_norm = 97.003, grad_norm = 0.089, train_F1/valid_F1 = 0.545000/0.249499\n",
      "Saved parameters to ./all_features_10_seq_100hidden_2layer_drop08_longer/model.ckpt-9600\n",
      "56.83 epochs, iter 9650: train_loss = 0.953324437, param_norm = 97.157, grad_norm = 0.237, train_F1/valid_F1 = 0.542000/0.261523\n",
      "57.12 epochs, iter 9700: train_loss = 0.930218399, param_norm = 97.345, grad_norm = 0.060, train_F1/valid_F1 = 0.546000/0.262525\n",
      "Saved parameters to ./all_features_10_seq_100hidden_2layer_drop08_longer/model.ckpt-9700\n",
      "57.42 epochs, iter 9750: train_loss = 0.950059950, param_norm = 97.407, grad_norm = 0.046, train_F1/valid_F1 = 0.543000/0.254509\n",
      "57.71 epochs, iter 9800: train_loss = 0.938578010, param_norm = 97.379, grad_norm = 0.206, train_F1/valid_F1 = 0.551000/0.259519\n",
      "Saved parameters to ./all_features_10_seq_100hidden_2layer_drop08_longer/model.ckpt-9800\n",
      "58.01 epochs, iter 9850: train_loss = 0.943849206, param_norm = 97.322, grad_norm = 0.105, train_F1/valid_F1 = 0.555000/0.237475\n",
      "58.30 epochs, iter 9900: train_loss = 0.946479976, param_norm = 97.518, grad_norm = 0.229, train_F1/valid_F1 = 0.557000/0.275551\n",
      "Saved parameters to ./all_features_10_seq_100hidden_2layer_drop08_longer/model.ckpt-9900\n",
      "58.59 epochs, iter 9950: train_loss = 0.939747810, param_norm = 97.613, grad_norm = 0.069, train_F1/valid_F1 = 0.549000/0.279559\n",
      "58.89 epochs, iter 10000: train_loss = 0.952453852, param_norm = 97.694, grad_norm = 0.306, train_F1/valid_F1 = 0.546000/0.271543\n",
      "Saved parameters to ./all_features_10_seq_100hidden_2layer_drop08_longer/model.ckpt-10000\n",
      "59.18 epochs, iter 10050: train_loss = 0.944962740, param_norm = 97.752, grad_norm = 0.075, train_F1/valid_F1 = 0.526000/0.266533\n",
      "59.48 epochs, iter 10100: train_loss = 0.941450417, param_norm = 98.001, grad_norm = 0.044, train_F1/valid_F1 = 0.555000/0.260521\n",
      "Saved parameters to ./all_features_10_seq_100hidden_2layer_drop08_longer/model.ckpt-10100\n",
      "59.77 epochs, iter 10150: train_loss = 0.945014358, param_norm = 98.029, grad_norm = 0.057, train_F1/valid_F1 = 0.543000/0.261523\n",
      "60.07 epochs, iter 10200: train_loss = 0.939064741, param_norm = 98.218, grad_norm = 0.154, train_F1/valid_F1 = 0.555000/0.252505\n",
      "Saved parameters to ./all_features_10_seq_100hidden_2layer_drop08_longer/model.ckpt-10200\n",
      "60.36 epochs, iter 10250: train_loss = 0.958430052, param_norm = 98.298, grad_norm = 0.236, train_F1/valid_F1 = 0.522000/0.268537\n",
      "60.66 epochs, iter 10300: train_loss = 0.943459868, param_norm = 98.512, grad_norm = 0.188, train_F1/valid_F1 = 0.559000/0.250501\n",
      "Saved parameters to ./all_features_10_seq_100hidden_2layer_drop08_longer/model.ckpt-10300\n",
      "60.95 epochs, iter 10350: train_loss = 0.940796733, param_norm = 98.623, grad_norm = 0.144, train_F1/valid_F1 = 0.550000/0.255511\n",
      "61.24 epochs, iter 10400: train_loss = 0.949657619, param_norm = 98.473, grad_norm = 0.415, train_F1/valid_F1 = 0.546000/0.243487\n",
      "Saved parameters to ./all_features_10_seq_100hidden_2layer_drop08_longer/model.ckpt-10400\n",
      "61.54 epochs, iter 10450: train_loss = 0.943618357, param_norm = 98.759, grad_norm = 0.070, train_F1/valid_F1 = 0.528000/0.323647\n",
      "61.83 epochs, iter 10500: train_loss = 0.939217269, param_norm = 98.901, grad_norm = 0.091, train_F1/valid_F1 = 0.556000/0.263527\n",
      "Saved parameters to ./all_features_10_seq_100hidden_2layer_drop08_longer/model.ckpt-10500\n",
      "62.13 epochs, iter 10550: train_loss = 0.948038578, param_norm = 98.884, grad_norm = 0.257, train_F1/valid_F1 = 0.540000/0.277555\n",
      "62.42 epochs, iter 10600: train_loss = 0.944310725, param_norm = 99.119, grad_norm = 0.129, train_F1/valid_F1 = 0.548000/0.271543\n",
      "Saved parameters to ./all_features_10_seq_100hidden_2layer_drop08_longer/model.ckpt-10600\n",
      "62.72 epochs, iter 10650: train_loss = 0.928966939, param_norm = 99.113, grad_norm = 0.101, train_F1/valid_F1 = 0.570000/0.253507\n",
      "63.01 epochs, iter 10700: train_loss = 0.945376694, param_norm = 99.089, grad_norm = 0.390, train_F1/valid_F1 = 0.554000/0.344689\n",
      "Saved parameters to ./all_features_10_seq_100hidden_2layer_drop08_longer/model.ckpt-10700\n",
      "63.31 epochs, iter 10750: train_loss = 0.944661021, param_norm = 99.272, grad_norm = 0.210, train_F1/valid_F1 = 0.537000/0.267535\n",
      "63.60 epochs, iter 10800: train_loss = 0.938867450, param_norm = 99.270, grad_norm = 0.121, train_F1/valid_F1 = 0.552000/0.275551\n",
      "Saved parameters to ./all_features_10_seq_100hidden_2layer_drop08_longer/model.ckpt-10800\n",
      "63.89 epochs, iter 10850: train_loss = 0.945118666, param_norm = 99.477, grad_norm = 0.102, train_F1/valid_F1 = 0.549000/0.257515\n",
      "64.19 epochs, iter 10900: train_loss = 0.950282693, param_norm = 99.427, grad_norm = 0.110, train_F1/valid_F1 = 0.532000/0.249499\n",
      "Saved parameters to ./all_features_10_seq_100hidden_2layer_drop08_longer/model.ckpt-10900\n",
      "64.48 epochs, iter 10950: train_loss = 0.942673922, param_norm = 99.620, grad_norm = 0.152, train_F1/valid_F1 = 0.547000/0.248497\n",
      "64.78 epochs, iter 11000: train_loss = 0.944306493, param_norm = 99.619, grad_norm = 0.298, train_F1/valid_F1 = 0.555000/0.258517\n",
      "Saved parameters to ./all_features_10_seq_100hidden_2layer_drop08_longer/model.ckpt-11000\n",
      "65.07 epochs, iter 11050: train_loss = 0.954820633, param_norm = 99.738, grad_norm = 0.178, train_F1/valid_F1 = 0.544000/0.262525\n",
      "65.37 epochs, iter 11100: train_loss = 0.946167588, param_norm = 99.859, grad_norm = 0.144, train_F1/valid_F1 = 0.548000/0.247495\n",
      "Saved parameters to ./all_features_10_seq_100hidden_2layer_drop08_longer/model.ckpt-11100\n",
      "65.66 epochs, iter 11150: train_loss = 0.942558110, param_norm = 99.910, grad_norm = 0.086, train_F1/valid_F1 = 0.555000/0.257515\n",
      "65.96 epochs, iter 11200: train_loss = 0.936012447, param_norm = 100.075, grad_norm = 0.212, train_F1/valid_F1 = 0.565000/0.251503\n",
      "Saved parameters to ./all_features_10_seq_100hidden_2layer_drop08_longer/model.ckpt-11200\n",
      "66.25 epochs, iter 11250: train_loss = 0.926191151, param_norm = 100.168, grad_norm = 0.240, train_F1/valid_F1 = 0.551000/0.249499\n",
      "66.54 epochs, iter 11300: train_loss = 0.938971281, param_norm = 100.328, grad_norm = 0.132, train_F1/valid_F1 = 0.541000/0.241483\n",
      "Saved parameters to ./all_features_10_seq_100hidden_2layer_drop08_longer/model.ckpt-11300\n",
      "66.84 epochs, iter 11350: train_loss = 0.949199975, param_norm = 100.494, grad_norm = 0.394, train_F1/valid_F1 = 0.551000/0.251503\n",
      "67.13 epochs, iter 11400: train_loss = 0.930344105, param_norm = 100.556, grad_norm = 0.061, train_F1/valid_F1 = 0.560000/0.267535\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-60-172febd6b675>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlstm_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexperiment_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mexperiment_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeep_prob_val\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.8\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;31m# IMPORTANT:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# when you think F1 is not going to improve anymore, wait another 10 epochs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# if you see any better iteration that has not appeared before, keep waiting.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-57-50afe92500c1>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, session, experiment_name, keep_prob_val)\u001b[0m\n\u001b[1;32m    122\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0miteration\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m100\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglobal_step\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0massign\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miteration\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# set and update(eval) global_step with index, i\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 124\u001b[0;31m                 \u001b[0msave_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msaver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"./\"\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mexperiment_name\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\"/model.ckpt\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mglobal_step\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglobal_step\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    125\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Saved parameters to %s'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0msave_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/training/saver.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(self, sess, save_path, global_step, latest_filename, meta_graph_suffix, write_meta_graph, write_state, strip_default_attrs)\u001b[0m\n\u001b[1;32m   1699\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_default\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1700\u001b[0m           self.export_meta_graph(\n\u001b[0;32m-> 1701\u001b[0;31m               meta_graph_filename, strip_default_attrs=strip_default_attrs)\n\u001b[0m\u001b[1;32m   1702\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1703\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_empty\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/training/saver.py\u001b[0m in \u001b[0;36mexport_meta_graph\u001b[0;34m(self, filename, collection_list, as_text, export_scope, clear_devices, clear_extraneous_savers, strip_default_attrs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     return export_meta_graph(\n\u001b[1;32m   1738\u001b[0m         \u001b[0mfilename\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m         \u001b[0mgraph_def\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_default_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_graph_def\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0madd_shapes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m         \u001b[0msaver_def\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msaver_def\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m         \u001b[0mcollection_list\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcollection_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mas_graph_def\u001b[0;34m(self, from_version, add_shapes)\u001b[0m\n\u001b[1;32m   3122\u001b[0m     \"\"\"\n\u001b[1;32m   3123\u001b[0m     \u001b[0;31m# pylint: enable=line-too-long\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3124\u001b[0;31m     \u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_as_graph_def\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfrom_version\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madd_shapes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3125\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3126\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m_as_graph_def\u001b[0;34m(self, from_version, add_shapes)\u001b[0m\n\u001b[1;32m   3089\u001b[0m               \u001b[0;32massert\u001b[0m \u001b[0;34m\"_output_shapes\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mgraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnode\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3090\u001b[0m               graph.node[-1].attr[\"_output_shapes\"].list.shape.extend(\n\u001b[0;32m-> 3091\u001b[0;31m                   [output.get_shape().as_proto() for output in op.outputs])\n\u001b[0m\u001b[1;32m   3092\u001b[0m             \u001b[0mbytesize\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mByteSize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3093\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mbytesize\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m<<\u001b[0m \u001b[0;36m31\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mbytesize\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m   3089\u001b[0m               \u001b[0;32massert\u001b[0m \u001b[0;34m\"_output_shapes\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mgraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnode\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3090\u001b[0m               graph.node[-1].attr[\"_output_shapes\"].list.shape.extend(\n\u001b[0;32m-> 3091\u001b[0;31m                   [output.get_shape().as_proto() for output in op.outputs])\n\u001b[0m\u001b[1;32m   3092\u001b[0m             \u001b[0mbytesize\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mByteSize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3093\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mbytesize\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m<<\u001b[0m \u001b[0;36m31\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mbytesize\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/tensor_shape.py\u001b[0m in \u001b[0;36mas_proto\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    829\u001b[0m           tensor_shape_pb2.TensorShapeProto.Dim(size=-1\n\u001b[1;32m    830\u001b[0m                                                 if d.value is None else d.value)\n\u001b[0;32m--> 831\u001b[0;31m           \u001b[0;32mfor\u001b[0m \u001b[0md\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dims\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    832\u001b[0m       ])\n\u001b[1;32m    833\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "lstm_model.train(session=sess, experiment_name=experiment_name, keep_prob_val=0.8)\n",
    "# IMPORTANT:\n",
    "# when you think F1 is not going to improve anymore, wait another 10 epochs. \n",
    "# if you see any better iteration that has not appeared before, keep waiting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from all_features_10_seq_100hidden_2layer_drop08_longer/best_ckpt/best.ckpt-700\n",
      "train F1: 0.532\n",
      "dev F1: 0.42084168336673344\n",
      "test F1: 0.3687374749498998\n"
     ]
    }
   ],
   "source": [
    "# load best checkpoint (based on dev f1) and evaluate\n",
    "ckpt = tf.train.get_checkpoint_state(experiment_name+'/best_ckpt')\n",
    "v2_path = ckpt.model_checkpoint_path + \".index\" if ckpt else \"\"\n",
    "if ckpt and (tf.gfile.Exists(ckpt.model_checkpoint_path) or tf.gfile.Exists(v2_path)):\n",
    "    lstm_model.saver.restore(sess, ckpt.model_checkpoint_path)\n",
    "else:\n",
    "    raise ValueError('What? you dont have a best checkpoint?')\n",
    "\n",
    "y_1000_train_pred = sess.run(lstm_model.preds, feed_dict={lstm_model.X: x_1000_train, lstm_model.keep_prob: 0.8})\n",
    "print(\"train F1:\",F1(y_1000_train, y_1000_train_pred))\n",
    "y_valid_pred = sess.run(lstm_model.preds, feed_dict={lstm_model.X: x_valid, lstm_model.keep_prob: 0.8})\n",
    "print(\"dev F1:\",F1(y_valid, y_valid_pred))\n",
    "y_test_pred = sess.run(lstm_model.preds, feed_dict={lstm_model.X: x_test, lstm_model.keep_prob: 0.8})\n",
    "print(\"test F1:\",F1(y_test, y_test_pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
