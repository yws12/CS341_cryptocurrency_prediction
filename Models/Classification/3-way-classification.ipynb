{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shared/anaconda3/lib/python3.6/site-packages/statsmodels/compat/pandas.py:56: FutureWarning: The pandas.core.datetools module is deprecated and will be removed in a future version. Please use the pandas.tseries module instead.\n",
      "  from pandas.core import datetools\n",
      "/home/shared/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>USDT_BTC_high</th>\n",
       "      <th>USDT_BTC_low</th>\n",
       "      <th>USDT_BTC_close</th>\n",
       "      <th>USDT_BTC_open</th>\n",
       "      <th>USDT_BTC_volume</th>\n",
       "      <th>USDT_BTC_quoteVolume</th>\n",
       "      <th>USDT_BTC_weighted_mean</th>\n",
       "      <th>USDT_BTC_pctChange</th>\n",
       "      <th>USDT_ETH_high</th>\n",
       "      <th>USDT_ETH_low</th>\n",
       "      <th>...</th>\n",
       "      <th>BTC_LTC_weighted_mean</th>\n",
       "      <th>BTC_LTC_pctChange</th>\n",
       "      <th>BTC_XRP_high</th>\n",
       "      <th>BTC_XRP_low</th>\n",
       "      <th>BTC_XRP_close</th>\n",
       "      <th>BTC_XRP_open</th>\n",
       "      <th>BTC_XRP_volume</th>\n",
       "      <th>BTC_XRP_quoteVolume</th>\n",
       "      <th>BTC_XRP_weighted_mean</th>\n",
       "      <th>BTC_XRP_pctChange</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>time</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2016-01-02 12:00:00</th>\n",
       "      <td>432.5000</td>\n",
       "      <td>432.50</td>\n",
       "      <td>432.500000</td>\n",
       "      <td>432.50000</td>\n",
       "      <td>40.041239</td>\n",
       "      <td>0.092581</td>\n",
       "      <td>432.500000</td>\n",
       "      <td>2.220446e-16</td>\n",
       "      <td>0.959136</td>\n",
       "      <td>0.959136</td>\n",
       "      <td>...</td>\n",
       "      <td>0.008063</td>\n",
       "      <td>-0.002293</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.033605</td>\n",
       "      <td>2408.822942</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>-0.002859</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-01-02 13:00:00</th>\n",
       "      <td>432.5000</td>\n",
       "      <td>432.50</td>\n",
       "      <td>432.500000</td>\n",
       "      <td>432.50000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>432.986941</td>\n",
       "      <td>1.125876e-03</td>\n",
       "      <td>0.959136</td>\n",
       "      <td>0.959136</td>\n",
       "      <td>...</td>\n",
       "      <td>0.008060</td>\n",
       "      <td>-0.000333</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.004704</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-01-02 14:00:00</th>\n",
       "      <td>437.3635</td>\n",
       "      <td>432.48</td>\n",
       "      <td>433.336667</td>\n",
       "      <td>433.52799</td>\n",
       "      <td>359.269753</td>\n",
       "      <td>0.828819</td>\n",
       "      <td>433.473883</td>\n",
       "      <td>1.124610e-03</td>\n",
       "      <td>0.959136</td>\n",
       "      <td>0.957000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.008073</td>\n",
       "      <td>0.001623</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>1.141981</td>\n",
       "      <td>81071.098773</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.004682</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-01-02 15:00:00</th>\n",
       "      <td>432.4800</td>\n",
       "      <td>432.48</td>\n",
       "      <td>432.480000</td>\n",
       "      <td>432.48000</td>\n",
       "      <td>60.859598</td>\n",
       "      <td>0.140722</td>\n",
       "      <td>432.480000</td>\n",
       "      <td>-2.292832e-03</td>\n",
       "      <td>0.957000</td>\n",
       "      <td>0.957000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.008089</td>\n",
       "      <td>0.002002</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>2.120423</td>\n",
       "      <td>150622.792769</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>-0.000492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-01-02 16:00:00</th>\n",
       "      <td>432.4800</td>\n",
       "      <td>432.48</td>\n",
       "      <td>432.480000</td>\n",
       "      <td>432.48000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>432.350000</td>\n",
       "      <td>-3.005919e-04</td>\n",
       "      <td>0.957000</td>\n",
       "      <td>0.957000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.008079</td>\n",
       "      <td>-0.001224</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.491516</td>\n",
       "      <td>35178.793196</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>-0.007526</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 56 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                     USDT_BTC_high  USDT_BTC_low  USDT_BTC_close  \\\n",
       "time                                                               \n",
       "2016-01-02 12:00:00       432.5000        432.50      432.500000   \n",
       "2016-01-02 13:00:00       432.5000        432.50      432.500000   \n",
       "2016-01-02 14:00:00       437.3635        432.48      433.336667   \n",
       "2016-01-02 15:00:00       432.4800        432.48      432.480000   \n",
       "2016-01-02 16:00:00       432.4800        432.48      432.480000   \n",
       "\n",
       "                     USDT_BTC_open  USDT_BTC_volume  USDT_BTC_quoteVolume  \\\n",
       "time                                                                        \n",
       "2016-01-02 12:00:00      432.50000        40.041239              0.092581   \n",
       "2016-01-02 13:00:00      432.50000         0.000000              0.000000   \n",
       "2016-01-02 14:00:00      433.52799       359.269753              0.828819   \n",
       "2016-01-02 15:00:00      432.48000        60.859598              0.140722   \n",
       "2016-01-02 16:00:00      432.48000         0.000000              0.000000   \n",
       "\n",
       "                     USDT_BTC_weighted_mean  USDT_BTC_pctChange  \\\n",
       "time                                                              \n",
       "2016-01-02 12:00:00              432.500000        2.220446e-16   \n",
       "2016-01-02 13:00:00              432.986941        1.125876e-03   \n",
       "2016-01-02 14:00:00              433.473883        1.124610e-03   \n",
       "2016-01-02 15:00:00              432.480000       -2.292832e-03   \n",
       "2016-01-02 16:00:00              432.350000       -3.005919e-04   \n",
       "\n",
       "                     USDT_ETH_high  USDT_ETH_low        ...          \\\n",
       "time                                                    ...           \n",
       "2016-01-02 12:00:00       0.959136      0.959136        ...           \n",
       "2016-01-02 13:00:00       0.959136      0.959136        ...           \n",
       "2016-01-02 14:00:00       0.959136      0.957000        ...           \n",
       "2016-01-02 15:00:00       0.957000      0.957000        ...           \n",
       "2016-01-02 16:00:00       0.957000      0.957000        ...           \n",
       "\n",
       "                     BTC_LTC_weighted_mean  BTC_LTC_pctChange  BTC_XRP_high  \\\n",
       "time                                                                          \n",
       "2016-01-02 12:00:00               0.008063          -0.002293      0.000014   \n",
       "2016-01-02 13:00:00               0.008060          -0.000333      0.000014   \n",
       "2016-01-02 14:00:00               0.008073           0.001623      0.000014   \n",
       "2016-01-02 15:00:00               0.008089           0.002002      0.000014   \n",
       "2016-01-02 16:00:00               0.008079          -0.001224      0.000014   \n",
       "\n",
       "                     BTC_XRP_low  BTC_XRP_close  BTC_XRP_open  BTC_XRP_volume  \\\n",
       "time                                                                            \n",
       "2016-01-02 12:00:00     0.000014       0.000014      0.000014        0.033605   \n",
       "2016-01-02 13:00:00     0.000014       0.000014      0.000014        0.000000   \n",
       "2016-01-02 14:00:00     0.000014       0.000014      0.000014        1.141981   \n",
       "2016-01-02 15:00:00     0.000014       0.000014      0.000014        2.120423   \n",
       "2016-01-02 16:00:00     0.000014       0.000014      0.000014        0.491516   \n",
       "\n",
       "                     BTC_XRP_quoteVolume  BTC_XRP_weighted_mean  \\\n",
       "time                                                              \n",
       "2016-01-02 12:00:00          2408.822942               0.000014   \n",
       "2016-01-02 13:00:00             0.000000               0.000014   \n",
       "2016-01-02 14:00:00         81071.098773               0.000014   \n",
       "2016-01-02 15:00:00        150622.792769               0.000014   \n",
       "2016-01-02 16:00:00         35178.793196               0.000014   \n",
       "\n",
       "                     BTC_XRP_pctChange  \n",
       "time                                    \n",
       "2016-01-02 12:00:00          -0.002859  \n",
       "2016-01-02 13:00:00           0.004704  \n",
       "2016-01-02 14:00:00           0.004682  \n",
       "2016-01-02 15:00:00          -0.000492  \n",
       "2016-01-02 16:00:00          -0.007526  \n",
       "\n",
       "[5 rows x 56 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "__author__ = \"Yicheng Li\"\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import statsmodels.api as sm\n",
    "import random\n",
    "from sklearn import preprocessing\n",
    "import tensorflow as tf\n",
    "import os\n",
    "\n",
    "dir_path = ''\n",
    "df = pd.read_pickle(dir_path+'df_hourly_poloniex.pickle')\n",
    "df = df.dropna()\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler_features = preprocessing.MinMaxScaler(feature_range=(0.1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to create train, validation, test data given sequence length\n",
    "def load_data(df, seq_len, test_size=-1):\n",
    "    # prepare one-hot labels\n",
    "    labels = df['USDT_BTC_pctChange'].as_matrix().reshape([-1,1])\n",
    "    labels = np.concatenate([(labels > 3e-3)*1, ((3e-3 > labels)&(labels > -3e-3))*1, (labels < -3e-3)*1],1)\n",
    "    labels = labels[seq_len-1:] # so labels and data has same length\n",
    "    \n",
    "    feature_set = [x for x in range(df.shape[-1])] #[0,1,2,3,4,6,7]\n",
    "    \n",
    "    data_raw = df.as_matrix() # convert to numpy array\n",
    "    # fit scaler\n",
    "    data_raw = scaler_features.fit_transform(data_raw[:, feature_set])\n",
    "    data = []\n",
    "    \n",
    "    # create all possible sequences of length seq_len\n",
    "    for index in range(len(data_raw) - seq_len + 1): \n",
    "        data.append(data_raw[index: index + seq_len, :])\n",
    "    \n",
    "    data = np.array(data)\n",
    "    \n",
    "    if test_size == -1: # split the old way\n",
    "        n_train_valid_pairs = 3\n",
    "        each_train_set_size_pct = 25\n",
    "        each_valid_set_size_pct = 5\n",
    "\n",
    "        each_train_set_size = round(each_train_set_size_pct/100*data.shape[0])\n",
    "        each_valid_set_size = round(each_valid_set_size_pct/100*data.shape[0])\n",
    "\n",
    "        x_train_sets = []\n",
    "        y_train_sets = []\n",
    "        x_valid_sets = []\n",
    "        y_valid_sets = []\n",
    "        used = 0\n",
    "\n",
    "        for i in range(n_train_valid_pairs):\n",
    "            x_train_sets.append(data[used : used + each_train_set_size,:-1,:]) # cannot see last day, which we aim to predict\n",
    "            y_train_sets.append(labels[used : used + each_train_set_size, :])\n",
    "            used += each_train_set_size\n",
    "\n",
    "            x_valid_sets.append(data[used : used + each_valid_set_size,:-1,:])\n",
    "            y_valid_sets.append(labels[used : used + each_valid_set_size, :])\n",
    "            used += each_valid_set_size\n",
    "\n",
    "        x_test = data[used : , :-1, :]\n",
    "        y_test = labels[used : , :]\n",
    "\n",
    "        x_train = np.concatenate(x_train_sets, axis=0)\n",
    "        y_train = np.concatenate(y_train_sets, axis=0)\n",
    "        x_valid = np.concatenate(x_valid_sets, axis=0)\n",
    "        y_valid = np.concatenate(y_valid_sets, axis=0)\n",
    "    \n",
    "    else:\n",
    "        x_test = data[-test_size : , :-1, :]\n",
    "        y_test = labels[-test_size : , :]\n",
    "        \n",
    "        valid_start = data.shape[0] - test_size - int(test_size/2)\n",
    "        x_valid = data[valid_start:-test_size, :-1, :]\n",
    "        y_valid = labels[valid_start:-test_size, :]\n",
    "        \n",
    "        x_train = data[:valid_start, :-1, :]\n",
    "        y_train = labels[:valid_start, :]\n",
    "    \n",
    "    return [x_train, y_train, x_valid, y_valid, x_test, y_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train.shape =  (16981, 9, 56)\n",
      "y_train.shape =  (16981, 3)\n",
      "x_valid.shape =  (998, 9, 56)\n",
      "y_valid.shape =  (998, 3)\n",
      "x_test.shape =  (1996, 9, 56)\n",
      "y_test.shape =  (1996, 3)\n"
     ]
    }
   ],
   "source": [
    "# create train, test data\n",
    "seq_len = 10 # choose sequence length\n",
    "x_train, y_train, x_valid, y_valid, x_test, y_test = load_data(df, seq_len, test_size=1996)\n",
    "# y_train = y_train.reshape([-1,1])\n",
    "# y_valid = y_valid.reshape([-1,1])\n",
    "# y_test = y_test.reshape([-1,1])\n",
    "print('x_train.shape = ',x_train.shape)\n",
    "print('y_train.shape = ', y_train.shape)\n",
    "print('x_valid.shape = ',x_valid.shape)\n",
    "print('y_valid.shape = ', y_valid.shape)\n",
    "print('x_test.shape = ', x_test.shape)\n",
    "print('y_test.shape = ',y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def f1_score_single(y_true, y_pred):\n",
    "#     TP = y_true.dot(y_pred) # zero or one\n",
    "#     FP = np.sum(y_pred > y_true) # sum over all k classes, zero or one\n",
    "#     FN = np.sum(y_pred < y_true) # sum over all k classes, zero or one\n",
    "    \n",
    "#     if TP == 0: return 0.\n",
    "#     p = 1. * TP / (TP + FP)\n",
    "#     r = 1. * TP / (TP + FN)\n",
    "#     return 2 * p * r / (p + r)\n",
    "    \n",
    "# def F1(y_true, y_pred):\n",
    "#     return np.mean([f1_score_single(x, y) for x, y in zip(y_true, y_pred)])\n",
    "\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "def F1(y_true, y_pred):\n",
    "    y_true = np.argmax(y_true, axis=1)\n",
    "    y_pred = np.argmax(y_pred, axis=1)\n",
    "    return f1_score(y_true, y_pred, average='macro')  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "baseline dev_F1= 0.35937467499689263\n",
      "baseline test_F1= 0.3656295467079018\n"
     ]
    }
   ],
   "source": [
    "y_pred = np.roll(y_valid,1, axis=0)\n",
    "print('baseline dev_F1=',F1(y_valid[1:], y_pred[1:]))\n",
    "y_pred = np.roll(y_test,1, axis=0)\n",
    "y_pred[0] = y_valid[-1] # be careful here\n",
    "print('baseline test_F1=',F1(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_in_epoch = 0;\n",
    "perm_array  = np.arange(x_train.shape[0])\n",
    "np.random.shuffle(perm_array)\n",
    "\n",
    "# function to get the next batch\n",
    "def get_next_batch(batch_size):\n",
    "    global index_in_epoch, x_train, perm_array   \n",
    "    start = index_in_epoch\n",
    "    index_in_epoch += batch_size\n",
    "    \n",
    "    if index_in_epoch > x_train.shape[0]:\n",
    "        np.random.shuffle(perm_array) # shuffle permutation array\n",
    "        start = 0 # start next epoch\n",
    "        index_in_epoch = batch_size\n",
    "        \n",
    "    end = index_in_epoch\n",
    "    return x_train[perm_array[start:end]], y_train[perm_array[start:end]]\n",
    "\n",
    "x_1000_train, y_1000_train = get_next_batch(1000) # special batch of 1000 records in training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM_Model(object):\n",
    "    def __init__(self, seq_len, use_batch_normalization):\n",
    "        # parameters\n",
    "        self.n_steps = seq_len-1 \n",
    "        self.n_inputs = x_train.shape[-1]\n",
    "        self.n_neurons = 300  # cell.state_size\n",
    "        self.n_bins = 3 # be careful if you want to change this\n",
    "        self.n_layers = 2\n",
    "        self.batch_size = 100\n",
    "        self.n_epochs = 0 # 0 means to train indefinitely\n",
    "        self.train_set_size = x_train.shape[0]\n",
    "        self.test_set_size = x_test.shape[0]\n",
    "        self.keep_prob = tf.placeholder(tf.float32, [])\n",
    "        self.beta = tf.placeholder(tf.float32, []) # to control l2 regularization\n",
    "        self.is_training = tf.placeholder(tf.bool)\n",
    "        self.max_gradient_norm = 5\n",
    "        with tf.variable_scope(\"LSTM_Model\", initializer=tf.contrib.layers.xavier_initializer()):\n",
    "            self.X = tf.placeholder(tf.float32, [None, self.n_steps, self.n_inputs])\n",
    "            self.y = tf.placeholder(tf.float32, [None, self.n_bins])\n",
    "\n",
    "            layers = [tf.contrib.rnn.LSTMCell(num_units=self.n_neurons, \\\n",
    "                                              initializer=tf.contrib.layers.xavier_initializer(), \\\n",
    "                                              activation=tf.nn.elu)\n",
    "                     for layer in range(self.n_layers)]\n",
    "            \n",
    "#             layers = [tf.contrib.rnn.BasicRNNCell(num_units=self.n_neurons, \\\n",
    "#                                               activation=tf.nn.elu)\n",
    "#                      for layer in range(self.n_layers)]\n",
    "            \n",
    "            multi_layer_cell = tf.contrib.rnn.MultiRNNCell(layers)\n",
    "\n",
    "            outputs, states = tf.nn.dynamic_rnn(multi_layer_cell, self.X, dtype=tf.float32)\n",
    "            outputs = tf.nn.dropout(outputs, self.keep_prob) # dropout after LSTM\n",
    "            # 'outputs' is a tensor of shape [batch_size, n_steps, n_neurons(cell.state_size)]\n",
    "        \n",
    "#             directly output\n",
    "#             logits = outputs[:,self.n_steps-1,:] # keep only last output of sequence\n",
    "            \n",
    "            # ======== attn layer ===================\n",
    "#             sim_mat = tf.matmul(outputs, tf.transpose(outputs, perm=[0,2,1]))\n",
    "#             attn_dist = tf.nn.softmax(sim_mat, 2)\n",
    "#             # (batchsize, n_steps, n_steps)\n",
    "#             attn_outputs = tf.matmul(attn_dist, outputs)\n",
    "#             # (batchsize, n_steps, n_bins)\n",
    "            # ========================================\n",
    "            \n",
    "            stacked_outputs = outputs[:, -1, :] # take last timestep\n",
    "#             stacked_outputs = tf.reshape(outputs, [-1, self.n_neurons]) \n",
    "            stacked_outputs = tf.layers.dense(stacked_outputs, 20)  # first FC\n",
    "            # batch normalization\n",
    "            if use_batch_normalization:\n",
    "                stacked_outputs = tf.contrib.layers.batch_norm(stacked_outputs, center=True, \\\n",
    "                                                               scale=False, is_training=self.is_training, scope='bn')\n",
    "            stacked_outputs = tf.nn.dropout(stacked_outputs, self.keep_prob)\n",
    "            \n",
    "            stacked_outputs = tf.layers.dense(stacked_outputs, self.n_bins)  # second FC\n",
    "            self.final_logits = tf.reshape(stacked_outputs, [-1, self.n_bins])\n",
    "            \n",
    "#             self.final_logits = final_outputs[:, -1, :] # last timestep\n",
    "            # (batchsize, n_bins), this is logits, not probs!!\n",
    "            self.final_logits = tf.nn.dropout(self.final_logits, self.keep_prob) # dropout\n",
    "            \n",
    "            self.indices = tf.argmax(self.final_logits, axis=-1) # (batchsize, 1)\n",
    "            self.preds = tf.one_hot(self.indices, depth=self.n_bins)\n",
    "            \n",
    "            self.each_loss = tf.nn.softmax_cross_entropy_with_logits(logits=self.final_logits, labels=self.y)\n",
    "            self.loss = tf.reduce_mean(self.each_loss) \n",
    "           \n",
    "            params = tf.trainable_variables()\n",
    "            \n",
    "            self.loss += self.beta * tf.add_n([ tf.nn.l2_loss(v) for v in params ])\n",
    "            \n",
    "            gradients = tf.gradients(self.loss, params)\n",
    "            self.gradient_norm = tf.global_norm(gradients)\n",
    "            clipped_gradients, _ = tf.clip_by_global_norm(gradients, self.max_gradient_norm)\n",
    "            clipped_norm = tf.global_norm(clipped_gradients)\n",
    "            self.param_norm = tf.global_norm(params)\n",
    "            self.learning_rate_placeholder = tf.placeholder(tf.float32, [], name='learning_rate')\n",
    "            optimizer = tf.train.RMSPropOptimizer(learning_rate=self.learning_rate_placeholder) \n",
    "            # training_op = optimizer.minimize(loss)\n",
    "            self.training_op = optimizer.apply_gradients(zip(clipped_gradients, params))\n",
    "\n",
    "            # initialize parameters\n",
    "#             sess = tf.Session()\n",
    "            self.global_step = tf.Variable(0, name='global_step', trainable=False)\n",
    "            self.saver = tf.train.Saver(max_to_keep=2)\n",
    "            self.bestmodel_saver = tf.train.Saver(max_to_keep=2)\n",
    "    \n",
    "    def train(self, session, experiment_name, keep_prob_val, beta):\n",
    "        \n",
    "        bestmodel_dir = experiment_name+'/best_ckpt'\n",
    "        bestmodel_ckpt_path = bestmodel_dir+'/best.ckpt'\n",
    "        best_valid_f1 = None\n",
    "        # Make bestmodel dir if necessary\n",
    "        if not os.path.exists(bestmodel_dir):\n",
    "            os.makedirs(bestmodel_dir)\n",
    "        \n",
    "        ckpt = tf.train.get_checkpoint_state(experiment_name)\n",
    "        v2_path = ckpt.model_checkpoint_path + \".index\" if ckpt else \"\"\n",
    "        if ckpt and (tf.gfile.Exists(ckpt.model_checkpoint_path) or tf.gfile.Exists(v2_path)):\n",
    "            self.saver.restore(sess, ckpt.model_checkpoint_path)\n",
    "            iteration = self.global_step.eval(session=sess) # get last global_step\n",
    "            print(\"Start from iteration:\", iteration)\n",
    "            lr = 1e-3\n",
    "        else:\n",
    "            print('There is not saved parameters. Creating model with fresh parameters.')\n",
    "            sess.run(tf.global_variables_initializer())\n",
    "            iteration = 0\n",
    "            lr = 1e-2 # should fix this...\n",
    "\n",
    "        old_loss = 1000\n",
    "        \n",
    "        while self.n_epochs == 0 or iteration*self.batch_size/self.train_set_size < self.n_epochs:\n",
    "            iteration = iteration + 1\n",
    "            x_batch, y_batch = get_next_batch(self.batch_size) # fetch the next training batch \n",
    "\n",
    "            # train on this batch\n",
    "            sess.run(self.training_op, feed_dict={self.X: x_batch, self.y: y_batch, self.learning_rate_placeholder:lr, \\\n",
    "                                                 self.keep_prob: keep_prob_val, self.beta: beta, self.is_training: True})\n",
    "\n",
    "            if iteration % 200 == 0:\n",
    "                y_1000_train_pred, loss_val, param_norm_val, grad_norm_val = \\\n",
    "                    sess.run([self.preds, self.loss, self.param_norm, self.gradient_norm],\\\n",
    "                            feed_dict={self.X: x_1000_train, self.y:y_1000_train, \\\n",
    "                                        self.learning_rate_placeholder:lr, self.keep_prob: keep_prob_val, \\\n",
    "                                        self.beta: beta, self.is_training: False})\n",
    "                    \n",
    "                if loss_val > old_loss * 1.2:\n",
    "                    lr /= 2\n",
    "                old_loss = loss_val\n",
    "\n",
    "                y_valid_pred = sess.run(self.preds, feed_dict={self.X: x_valid, self.keep_prob: keep_prob_val,\\\n",
    "                                                              self.is_training: False})\n",
    "                \n",
    "                valid_f1 = F1(y_valid, y_valid_pred)\n",
    "                print('%.2f epochs, iter %d: train_loss = %.9f, param_norm = %.3f, grad_norm = %.3f, train_F1/valid_F1 = %.6f/%.6f' \\\n",
    "                      %(iteration*self.batch_size/self.train_set_size, iteration, loss_val, param_norm_val, grad_norm_val, \\\n",
    "                        F1(y_1000_train, y_1000_train_pred), \\\n",
    "                        valid_f1))\n",
    "\n",
    "                if best_valid_f1 is None or valid_f1 > best_valid_f1:\n",
    "                    best_valid_f1 = valid_f1\n",
    "                    self.global_step.assign(iteration).eval(session=sess)\n",
    "                    print(\"======New best valid F1. Saving to %s...\" % bestmodel_ckpt_path)\n",
    "                    self.bestmodel_saver.save(sess, bestmodel_ckpt_path, global_step=self.global_step)\n",
    "                \n",
    "            if iteration % 1000 == 0:\n",
    "                self.global_step.assign(iteration).eval(session=sess) # set and update(eval) global_step with index, i\n",
    "                save_path = self.saver.save(sess, \"./\"+experiment_name+\"/model.ckpt\", global_step=self.global_step)\n",
    "                print('Saved parameters to %s' % save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## run experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_name=\"04-28-03\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "lstm_model = LSTM_Model(seq_len=seq_len, use_batch_normalization=False)\n",
    "sess = tf.Session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.1730167\n",
      "2.0823033\n",
      "iter 50: train_loss = 1.011497855, train_F1= 0.540000\n",
      "iter 100: train_loss = 0.979385853, train_F1= 0.530000\n",
      "iter 150: train_loss = 0.881866217, train_F1= 0.620000\n",
      "iter 200: train_loss = 0.752146482, train_F1= 0.640000\n",
      "iter 250: train_loss = 0.716808558, train_F1= 0.670000\n",
      "iter 300: train_loss = 0.612567961, train_F1= 0.720000\n",
      "iter 350: train_loss = 0.499484569, train_F1= 0.820000\n",
      "iter 400: train_loss = 0.410808474, train_F1= 0.870000\n",
      "iter 450: train_loss = 0.369383961, train_F1= 0.920000\n",
      "iter 500: train_loss = 0.305401087, train_F1= 0.920000\n",
      "iter 550: train_loss = 0.294736117, train_F1= 0.900000\n",
      "iter 600: train_loss = 0.288513750, train_F1= 0.920000\n",
      "iter 650: train_loss = 0.241855398, train_F1= 0.930000\n",
      "iter 700: train_loss = 0.519630015, train_F1= 0.790000\n",
      "iter 750: train_loss = 0.143681929, train_F1= 0.970000\n",
      "iter 800: train_loss = 0.374593824, train_F1= 0.880000\n",
      "iter 850: train_loss = 0.166960984, train_F1= 0.950000\n",
      "iter 900: train_loss = 0.129060283, train_F1= 0.960000\n",
      "iter 950: train_loss = 0.074580029, train_F1= 0.990000\n",
      "iter 1000: train_loss = 0.138810188, train_F1= 0.970000\n",
      "iter 1050: train_loss = 0.023935737, train_F1= 1.000000\n"
     ]
    }
   ],
   "source": [
    "# sanity check\n",
    "sess.run(tf.global_variables_initializer())\n",
    "# first check random loss\n",
    "sanity_loss = sess.run(lstm_model.loss, feed_dict={lstm_model.X: x_1000_train, lstm_model.y:y_1000_train, \\\n",
    "                                        lstm_model.learning_rate_placeholder:1e-3, lstm_model.keep_prob: 1, \\\n",
    "                                                   lstm_model.beta: 0, lstm_model.is_training: False})\n",
    "print(sanity_loss) # should be around 1.09\n",
    "\n",
    "sanity_loss = sess.run(lstm_model.loss, feed_dict={lstm_model.X: x_1000_train, lstm_model.y:y_1000_train, \\\n",
    "                                        lstm_model.learning_rate_placeholder:1e-3, lstm_model.keep_prob: 1, \\\n",
    "                                                   lstm_model.beta: 0.005, lstm_model.is_training: False})\n",
    "print(sanity_loss) # should be around 1.09\n",
    "\n",
    "x_100_train, y_100_train = get_next_batch(100)\n",
    "iter_ = 0\n",
    "while True:\n",
    "    iter_ += 1\n",
    "    _, y_100_train_pred, loss_val = sess.run([lstm_model.training_op, lstm_model.preds, lstm_model.loss], \\\n",
    "                                             feed_dict={lstm_model.X: x_100_train, lstm_model.y:y_100_train, \\\n",
    "                                        lstm_model.learning_rate_placeholder:1e-3, lstm_model.keep_prob: 1, \\\n",
    "                                                        lstm_model.beta: 0, lstm_model.is_training: True})\n",
    "    if iter_ % 50 == 0:\n",
    "        print('iter %d: train_loss = %.9f, train_F1= %.6f' \\\n",
    "                      %(iter_, loss_val, F1(y_100_train, y_100_train_pred)))\n",
    "    if loss_val < 0.01:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There is not saved parameters. Creating model with fresh parameters.\n",
      "1.18 epochs, iter 200: train_loss = 1.053782940, param_norm = 48.861, grad_norm = 0.212, train_F1/valid_F1 = 0.502000/0.419840\n",
      "======New best valid F1. Saving to 04-28-03/best_ckpt/best.ckpt...\n",
      "2.36 epochs, iter 400: train_loss = 1.021923184, param_norm = 75.250, grad_norm = 0.098, train_F1/valid_F1 = 0.501000/0.419840\n",
      "3.53 epochs, iter 600: train_loss = 1.014582038, param_norm = 85.400, grad_norm = 0.045, train_F1/valid_F1 = 0.505000/0.419840\n",
      "4.71 epochs, iter 800: train_loss = 1.004464388, param_norm = 92.741, grad_norm = 0.089, train_F1/valid_F1 = 0.504000/0.416834\n",
      "5.89 epochs, iter 1000: train_loss = 1.014483333, param_norm = 100.506, grad_norm = 0.400, train_F1/valid_F1 = 0.497000/0.419840\n",
      "Saved parameters to ./04-28-03/model.ckpt-1000\n",
      "7.07 epochs, iter 1200: train_loss = 0.999663711, param_norm = 106.245, grad_norm = 0.116, train_F1/valid_F1 = 0.504000/0.264529\n",
      "8.24 epochs, iter 1400: train_loss = 0.992127061, param_norm = 110.834, grad_norm = 0.054, train_F1/valid_F1 = 0.501000/0.409820\n",
      "9.42 epochs, iter 1600: train_loss = 1.003868699, param_norm = 116.415, grad_norm = 0.131, train_F1/valid_F1 = 0.509000/0.412826\n",
      "10.60 epochs, iter 1800: train_loss = 0.993731797, param_norm = 120.748, grad_norm = 0.157, train_F1/valid_F1 = 0.520000/0.427856\n",
      "======New best valid F1. Saving to 04-28-03/best_ckpt/best.ckpt...\n",
      "11.78 epochs, iter 2000: train_loss = 0.982594132, param_norm = 125.444, grad_norm = 0.081, train_F1/valid_F1 = 0.512000/0.422846\n",
      "Saved parameters to ./04-28-03/model.ckpt-2000\n",
      "12.96 epochs, iter 2200: train_loss = 0.991221666, param_norm = 129.102, grad_norm = 0.108, train_F1/valid_F1 = 0.506000/0.344689\n",
      "14.13 epochs, iter 2400: train_loss = 0.986923397, param_norm = 133.175, grad_norm = 0.147, train_F1/valid_F1 = 0.501000/0.323647\n",
      "15.31 epochs, iter 2600: train_loss = 0.986093163, param_norm = 137.240, grad_norm = 0.146, train_F1/valid_F1 = 0.508000/0.386774\n",
      "16.49 epochs, iter 2800: train_loss = 0.978326797, param_norm = 140.208, grad_norm = 0.133, train_F1/valid_F1 = 0.518000/0.430862\n",
      "======New best valid F1. Saving to 04-28-03/best_ckpt/best.ckpt...\n",
      "17.67 epochs, iter 3000: train_loss = 0.973444283, param_norm = 143.829, grad_norm = 0.084, train_F1/valid_F1 = 0.528000/0.394790\n",
      "Saved parameters to ./04-28-03/model.ckpt-3000\n",
      "18.84 epochs, iter 3200: train_loss = 0.986926973, param_norm = 147.641, grad_norm = 0.252, train_F1/valid_F1 = 0.543000/0.405812\n",
      "20.02 epochs, iter 3400: train_loss = 0.965036392, param_norm = 150.692, grad_norm = 0.079, train_F1/valid_F1 = 0.536000/0.416834\n",
      "21.20 epochs, iter 3600: train_loss = 0.958974898, param_norm = 154.447, grad_norm = 0.101, train_F1/valid_F1 = 0.542000/0.420842\n",
      "22.38 epochs, iter 3800: train_loss = 0.955884218, param_norm = 157.864, grad_norm = 0.123, train_F1/valid_F1 = 0.539000/0.416834\n",
      "23.56 epochs, iter 4000: train_loss = 0.955951452, param_norm = 161.448, grad_norm = 0.138, train_F1/valid_F1 = 0.560000/0.437876\n",
      "======New best valid F1. Saving to 04-28-03/best_ckpt/best.ckpt...\n",
      "Saved parameters to ./04-28-03/model.ckpt-4000\n",
      "24.73 epochs, iter 4200: train_loss = 0.969516158, param_norm = 164.166, grad_norm = 0.332, train_F1/valid_F1 = 0.543000/0.416834\n",
      "25.91 epochs, iter 4400: train_loss = 0.964553833, param_norm = 167.239, grad_norm = 0.188, train_F1/valid_F1 = 0.534000/0.387776\n",
      "27.09 epochs, iter 4600: train_loss = 0.949082136, param_norm = 171.473, grad_norm = 0.094, train_F1/valid_F1 = 0.539000/0.462926\n",
      "======New best valid F1. Saving to 04-28-03/best_ckpt/best.ckpt...\n",
      "28.27 epochs, iter 4800: train_loss = 0.946634650, param_norm = 174.267, grad_norm = 0.101, train_F1/valid_F1 = 0.535000/0.409820\n",
      "29.44 epochs, iter 5000: train_loss = 0.958143771, param_norm = 176.958, grad_norm = 0.186, train_F1/valid_F1 = 0.534000/0.382766\n",
      "Saved parameters to ./04-28-03/model.ckpt-5000\n",
      "30.62 epochs, iter 5200: train_loss = 0.948452473, param_norm = 181.199, grad_norm = 0.126, train_F1/valid_F1 = 0.540000/0.443888\n",
      "31.80 epochs, iter 5400: train_loss = 0.951981723, param_norm = 183.692, grad_norm = 0.150, train_F1/valid_F1 = 0.552000/0.452906\n",
      "32.98 epochs, iter 5600: train_loss = 0.950560093, param_norm = 187.414, grad_norm = 0.180, train_F1/valid_F1 = 0.553000/0.443888\n",
      "34.16 epochs, iter 5800: train_loss = 0.937547982, param_norm = 191.310, grad_norm = 0.114, train_F1/valid_F1 = 0.551000/0.419840\n",
      "35.33 epochs, iter 6000: train_loss = 0.942694724, param_norm = 194.658, grad_norm = 0.137, train_F1/valid_F1 = 0.538000/0.451904\n",
      "Saved parameters to ./04-28-03/model.ckpt-6000\n",
      "36.51 epochs, iter 6200: train_loss = 0.946933985, param_norm = 198.267, grad_norm = 0.178, train_F1/valid_F1 = 0.547000/0.450902\n",
      "37.69 epochs, iter 6400: train_loss = 0.943733037, param_norm = 202.382, grad_norm = 0.171, train_F1/valid_F1 = 0.549000/0.404810\n",
      "38.87 epochs, iter 6600: train_loss = 0.939541399, param_norm = 205.774, grad_norm = 0.177, train_F1/valid_F1 = 0.552000/0.393788\n",
      "40.04 epochs, iter 6800: train_loss = 0.950031877, param_norm = 210.113, grad_norm = 0.301, train_F1/valid_F1 = 0.545000/0.422846\n",
      "41.22 epochs, iter 7000: train_loss = 0.934793711, param_norm = 213.954, grad_norm = 0.144, train_F1/valid_F1 = 0.559000/0.431864\n",
      "Saved parameters to ./04-28-03/model.ckpt-7000\n",
      "42.40 epochs, iter 7200: train_loss = 0.947843313, param_norm = 217.584, grad_norm = 0.169, train_F1/valid_F1 = 0.543000/0.427856\n",
      "43.58 epochs, iter 7400: train_loss = 0.924317241, param_norm = 222.204, grad_norm = 0.080, train_F1/valid_F1 = 0.562000/0.441884\n",
      "44.76 epochs, iter 7600: train_loss = 0.933812320, param_norm = 225.937, grad_norm = 0.146, train_F1/valid_F1 = 0.545000/0.418838\n",
      "45.93 epochs, iter 7800: train_loss = 0.930806398, param_norm = 230.417, grad_norm = 0.191, train_F1/valid_F1 = 0.561000/0.330661\n",
      "47.11 epochs, iter 8000: train_loss = 0.945499897, param_norm = 234.697, grad_norm = 0.112, train_F1/valid_F1 = 0.532000/0.405812\n",
      "Saved parameters to ./04-28-03/model.ckpt-8000\n",
      "48.29 epochs, iter 8200: train_loss = 0.930187106, param_norm = 238.709, grad_norm = 0.194, train_F1/valid_F1 = 0.564000/0.400802\n",
      "49.47 epochs, iter 8400: train_loss = 0.950617075, param_norm = 244.091, grad_norm = 0.224, train_F1/valid_F1 = 0.543000/0.388778\n",
      "50.64 epochs, iter 8600: train_loss = 1.001227498, param_norm = 249.717, grad_norm = 1.016, train_F1/valid_F1 = 0.535000/0.416834\n",
      "51.82 epochs, iter 8800: train_loss = 0.934849501, param_norm = 252.827, grad_norm = 0.128, train_F1/valid_F1 = 0.553000/0.324649\n",
      "53.00 epochs, iter 9000: train_loss = 0.913816154, param_norm = 257.335, grad_norm = 0.118, train_F1/valid_F1 = 0.560000/0.330661\n",
      "Saved parameters to ./04-28-03/model.ckpt-9000\n",
      "54.18 epochs, iter 9200: train_loss = 0.937086165, param_norm = 262.703, grad_norm = 0.294, train_F1/valid_F1 = 0.563000/0.301603\n",
      "55.36 epochs, iter 9400: train_loss = 1.058841348, param_norm = 268.163, grad_norm = 5.477, train_F1/valid_F1 = 0.507000/0.241483\n",
      "56.53 epochs, iter 9600: train_loss = 0.940814912, param_norm = 272.213, grad_norm = 0.116, train_F1/valid_F1 = 0.546000/0.307615\n",
      "57.71 epochs, iter 9800: train_loss = 0.916446447, param_norm = 276.275, grad_norm = 0.137, train_F1/valid_F1 = 0.588000/0.368737\n",
      "58.89 epochs, iter 10000: train_loss = 0.925162673, param_norm = 281.331, grad_norm = 0.135, train_F1/valid_F1 = 0.562000/0.252505\n",
      "Saved parameters to ./04-28-03/model.ckpt-10000\n",
      "60.07 epochs, iter 10200: train_loss = 1.107170582, param_norm = 286.829, grad_norm = 1.294, train_F1/valid_F1 = 0.553000/0.403808\n",
      "61.24 epochs, iter 10400: train_loss = 0.923505068, param_norm = 291.913, grad_norm = 0.213, train_F1/valid_F1 = 0.568000/0.415832\n",
      "62.42 epochs, iter 10600: train_loss = 0.940028965, param_norm = 296.585, grad_norm = 0.223, train_F1/valid_F1 = 0.554000/0.422846\n",
      "63.60 epochs, iter 10800: train_loss = 0.926277101, param_norm = 302.447, grad_norm = 0.184, train_F1/valid_F1 = 0.560000/0.442886\n",
      "64.78 epochs, iter 11000: train_loss = 0.923091412, param_norm = 306.692, grad_norm = 0.122, train_F1/valid_F1 = 0.571000/0.430862\n",
      "Saved parameters to ./04-28-03/model.ckpt-11000\n",
      "65.96 epochs, iter 11200: train_loss = 0.929806590, param_norm = 311.743, grad_norm = 0.347, train_F1/valid_F1 = 0.567000/0.373747\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "67.13 epochs, iter 11400: train_loss = 0.941698968, param_norm = 317.216, grad_norm = 0.203, train_F1/valid_F1 = 0.548000/0.416834\n",
      "68.31 epochs, iter 11600: train_loss = 0.926352680, param_norm = 321.570, grad_norm = 0.114, train_F1/valid_F1 = 0.552000/0.332665\n",
      "69.49 epochs, iter 11800: train_loss = 0.983040392, param_norm = 326.586, grad_norm = 0.344, train_F1/valid_F1 = 0.538000/0.377756\n",
      "70.67 epochs, iter 12000: train_loss = 0.930119991, param_norm = 330.279, grad_norm = 0.169, train_F1/valid_F1 = 0.553000/0.340681\n",
      "Saved parameters to ./04-28-03/model.ckpt-12000\n",
      "71.85 epochs, iter 12200: train_loss = 1.054371595, param_norm = 334.255, grad_norm = 1.994, train_F1/valid_F1 = 0.547000/0.406814\n",
      "73.02 epochs, iter 12400: train_loss = 0.940092206, param_norm = 339.250, grad_norm = 2.673, train_F1/valid_F1 = 0.567000/0.413828\n",
      "74.20 epochs, iter 12600: train_loss = 0.979866564, param_norm = 345.314, grad_norm = 0.309, train_F1/valid_F1 = 0.528000/0.423848\n",
      "75.38 epochs, iter 12800: train_loss = 0.933977902, param_norm = 349.373, grad_norm = 0.237, train_F1/valid_F1 = 0.552000/0.369739\n",
      "76.56 epochs, iter 13000: train_loss = 0.937310278, param_norm = 354.133, grad_norm = 0.206, train_F1/valid_F1 = 0.544000/0.392786\n",
      "Saved parameters to ./04-28-03/model.ckpt-13000\n",
      "77.73 epochs, iter 13200: train_loss = 0.965030730, param_norm = 359.164, grad_norm = 0.488, train_F1/valid_F1 = 0.561000/0.421844\n",
      "78.91 epochs, iter 13400: train_loss = 0.928612947, param_norm = 363.473, grad_norm = 0.181, train_F1/valid_F1 = 0.559000/0.399800\n",
      "80.09 epochs, iter 13600: train_loss = 0.915315747, param_norm = 368.115, grad_norm = 0.206, train_F1/valid_F1 = 0.574000/0.355711\n",
      "81.27 epochs, iter 13800: train_loss = 0.956444144, param_norm = 373.369, grad_norm = 0.298, train_F1/valid_F1 = 0.525000/0.392786\n",
      "82.45 epochs, iter 14000: train_loss = 4.432675362, param_norm = 378.598, grad_norm = 247.851, train_F1/valid_F1 = 0.464000/0.411824\n",
      "Saved parameters to ./04-28-03/model.ckpt-14000\n",
      "83.62 epochs, iter 14200: train_loss = 0.941140652, param_norm = 379.729, grad_norm = 0.383, train_F1/valid_F1 = 0.543000/0.417836\n",
      "84.80 epochs, iter 14400: train_loss = 0.907498360, param_norm = 381.166, grad_norm = 0.129, train_F1/valid_F1 = 0.577000/0.386774\n",
      "85.98 epochs, iter 14600: train_loss = 0.918743908, param_norm = 382.341, grad_norm = 0.278, train_F1/valid_F1 = 0.576000/0.397796\n",
      "87.16 epochs, iter 14800: train_loss = 0.890902877, param_norm = 384.131, grad_norm = 0.171, train_F1/valid_F1 = 0.583000/0.308617\n",
      "88.33 epochs, iter 15000: train_loss = 0.913136005, param_norm = 385.825, grad_norm = 0.388, train_F1/valid_F1 = 0.580000/0.352705\n",
      "Saved parameters to ./04-28-03/model.ckpt-15000\n",
      "89.51 epochs, iter 15200: train_loss = 0.916219950, param_norm = 387.083, grad_norm = 0.261, train_F1/valid_F1 = 0.575000/0.313627\n",
      "90.69 epochs, iter 15400: train_loss = 0.901485741, param_norm = 389.002, grad_norm = 0.267, train_F1/valid_F1 = 0.575000/0.320641\n",
      "91.87 epochs, iter 15600: train_loss = 0.888288081, param_norm = 390.804, grad_norm = 0.168, train_F1/valid_F1 = 0.582000/0.386774\n",
      "93.05 epochs, iter 15800: train_loss = 0.892072797, param_norm = 392.546, grad_norm = 0.170, train_F1/valid_F1 = 0.587000/0.265531\n",
      "94.22 epochs, iter 16000: train_loss = 0.891661227, param_norm = 394.266, grad_norm = 0.192, train_F1/valid_F1 = 0.582000/0.365731\n",
      "Saved parameters to ./04-28-03/model.ckpt-16000\n",
      "95.40 epochs, iter 16200: train_loss = 0.925583839, param_norm = 395.829, grad_norm = 0.346, train_F1/valid_F1 = 0.549000/0.379760\n",
      "96.58 epochs, iter 16400: train_loss = 0.899643898, param_norm = 397.506, grad_norm = 0.374, train_F1/valid_F1 = 0.592000/0.367735\n",
      "97.76 epochs, iter 16600: train_loss = 0.900272310, param_norm = 399.715, grad_norm = 0.274, train_F1/valid_F1 = 0.587000/0.389780\n",
      "98.93 epochs, iter 16800: train_loss = 0.904808104, param_norm = 401.580, grad_norm = 0.161, train_F1/valid_F1 = 0.575000/0.369739\n",
      "100.11 epochs, iter 17000: train_loss = 0.902910888, param_norm = 403.694, grad_norm = 0.291, train_F1/valid_F1 = 0.566000/0.390782\n",
      "Saved parameters to ./04-28-03/model.ckpt-17000\n",
      "101.29 epochs, iter 17200: train_loss = 0.911879301, param_norm = 405.722, grad_norm = 0.380, train_F1/valid_F1 = 0.585000/0.306613\n",
      "102.47 epochs, iter 17400: train_loss = 0.909306526, param_norm = 407.926, grad_norm = 0.482, train_F1/valid_F1 = 0.578000/0.376754\n",
      "103.65 epochs, iter 17600: train_loss = 0.910549462, param_norm = 409.502, grad_norm = 0.462, train_F1/valid_F1 = 0.579000/0.316633\n",
      "104.82 epochs, iter 17800: train_loss = 0.911057174, param_norm = 411.875, grad_norm = 0.361, train_F1/valid_F1 = 0.581000/0.340681\n",
      "106.00 epochs, iter 18000: train_loss = 1.151180148, param_norm = 414.301, grad_norm = 3.821, train_F1/valid_F1 = 0.599000/0.380762\n",
      "Saved parameters to ./04-28-03/model.ckpt-18000\n",
      "107.18 epochs, iter 18200: train_loss = 0.896914661, param_norm = 415.108, grad_norm = 1.468, train_F1/valid_F1 = 0.580000/0.388778\n",
      "108.36 epochs, iter 18400: train_loss = 0.862005591, param_norm = 415.729, grad_norm = 0.448, train_F1/valid_F1 = 0.610000/0.368737\n",
      "109.53 epochs, iter 18600: train_loss = 0.840775371, param_norm = 416.818, grad_norm = 0.309, train_F1/valid_F1 = 0.614000/0.376754\n",
      "110.71 epochs, iter 18800: train_loss = 0.846734047, param_norm = 417.657, grad_norm = 0.312, train_F1/valid_F1 = 0.615000/0.366733\n",
      "111.89 epochs, iter 19000: train_loss = 0.834755778, param_norm = 418.428, grad_norm = 0.236, train_F1/valid_F1 = 0.627000/0.378758\n",
      "Saved parameters to ./04-28-03/model.ckpt-19000\n",
      "113.07 epochs, iter 19200: train_loss = 0.837678552, param_norm = 419.426, grad_norm = 0.625, train_F1/valid_F1 = 0.622000/0.386774\n",
      "114.25 epochs, iter 19400: train_loss = 0.822803617, param_norm = 420.381, grad_norm = 0.262, train_F1/valid_F1 = 0.624000/0.401804\n",
      "115.42 epochs, iter 19600: train_loss = 0.833952665, param_norm = 421.339, grad_norm = 0.286, train_F1/valid_F1 = 0.626000/0.345691\n",
      "116.60 epochs, iter 19800: train_loss = 0.831623793, param_norm = 422.030, grad_norm = 0.267, train_F1/valid_F1 = 0.624000/0.356713\n",
      "117.78 epochs, iter 20000: train_loss = 0.812985480, param_norm = 423.033, grad_norm = 0.543, train_F1/valid_F1 = 0.638000/0.373747\n",
      "Saved parameters to ./04-28-03/model.ckpt-20000\n",
      "118.96 epochs, iter 20200: train_loss = 0.830657601, param_norm = 424.050, grad_norm = 0.683, train_F1/valid_F1 = 0.615000/0.398798\n",
      "120.13 epochs, iter 20400: train_loss = 0.847686052, param_norm = 425.018, grad_norm = 1.018, train_F1/valid_F1 = 0.619000/0.379760\n",
      "121.31 epochs, iter 20600: train_loss = 0.788893759, param_norm = 425.867, grad_norm = 0.344, train_F1/valid_F1 = 0.642000/0.329659\n",
      "122.49 epochs, iter 20800: train_loss = 0.806178987, param_norm = 426.846, grad_norm = 0.768, train_F1/valid_F1 = 0.625000/0.327655\n",
      "123.67 epochs, iter 21000: train_loss = 0.785728335, param_norm = 427.799, grad_norm = 0.351, train_F1/valid_F1 = 0.652000/0.353707\n",
      "Saved parameters to ./04-28-03/model.ckpt-21000\n",
      "124.85 epochs, iter 21200: train_loss = 0.831406116, param_norm = 428.686, grad_norm = 0.689, train_F1/valid_F1 = 0.618000/0.362725\n",
      "126.02 epochs, iter 21400: train_loss = 0.839265287, param_norm = 429.537, grad_norm = 1.566, train_F1/valid_F1 = 0.637000/0.339679\n",
      "127.20 epochs, iter 21600: train_loss = 0.773678482, param_norm = 430.500, grad_norm = 0.268, train_F1/valid_F1 = 0.653000/0.367735\n",
      "128.38 epochs, iter 21800: train_loss = 0.800001502, param_norm = 431.466, grad_norm = 0.443, train_F1/valid_F1 = 0.636000/0.368737\n",
      "129.56 epochs, iter 22000: train_loss = 0.872093022, param_norm = 432.361, grad_norm = 1.316, train_F1/valid_F1 = 0.652000/0.350701\n",
      "Saved parameters to ./04-28-03/model.ckpt-22000\n",
      "130.73 epochs, iter 22200: train_loss = 0.795635581, param_norm = 433.313, grad_norm = 0.517, train_F1/valid_F1 = 0.630000/0.337675\n",
      "131.91 epochs, iter 22400: train_loss = 0.778588712, param_norm = 434.089, grad_norm = 0.373, train_F1/valid_F1 = 0.649000/0.342685\n",
      "133.09 epochs, iter 22600: train_loss = 0.802234948, param_norm = 435.168, grad_norm = 0.591, train_F1/valid_F1 = 0.644000/0.363727\n",
      "134.27 epochs, iter 22800: train_loss = 0.790817261, param_norm = 436.411, grad_norm = 0.441, train_F1/valid_F1 = 0.654000/0.365731\n",
      "135.45 epochs, iter 23000: train_loss = 0.778987408, param_norm = 437.278, grad_norm = 0.442, train_F1/valid_F1 = 0.644000/0.359719\n",
      "Saved parameters to ./04-28-03/model.ckpt-23000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "136.62 epochs, iter 23200: train_loss = 0.834573746, param_norm = 438.423, grad_norm = 1.832, train_F1/valid_F1 = 0.652000/0.324649\n",
      "137.80 epochs, iter 23400: train_loss = 0.997677684, param_norm = 439.590, grad_norm = 7.265, train_F1/valid_F1 = 0.664000/0.357715\n",
      "138.98 epochs, iter 23600: train_loss = 0.736655712, param_norm = 440.519, grad_norm = 1.522, train_F1/valid_F1 = 0.668000/0.342685\n",
      "140.16 epochs, iter 23800: train_loss = 0.776344836, param_norm = 441.646, grad_norm = 1.908, train_F1/valid_F1 = 0.662000/0.352705\n",
      "141.33 epochs, iter 24000: train_loss = 0.738935173, param_norm = 442.592, grad_norm = 0.359, train_F1/valid_F1 = 0.672000/0.391784\n",
      "Saved parameters to ./04-28-03/model.ckpt-24000\n",
      "142.51 epochs, iter 24200: train_loss = 0.741675019, param_norm = 443.565, grad_norm = 0.515, train_F1/valid_F1 = 0.657000/0.336673\n",
      "143.69 epochs, iter 24400: train_loss = 0.736011147, param_norm = 444.672, grad_norm = 0.422, train_F1/valid_F1 = 0.673000/0.377756\n",
      "144.87 epochs, iter 24600: train_loss = 0.747319758, param_norm = 445.714, grad_norm = 1.005, train_F1/valid_F1 = 0.670000/0.330661\n",
      "146.05 epochs, iter 24800: train_loss = 0.741498649, param_norm = 446.932, grad_norm = 0.512, train_F1/valid_F1 = 0.667000/0.273547\n",
      "147.22 epochs, iter 25000: train_loss = 0.744874418, param_norm = 448.040, grad_norm = 0.364, train_F1/valid_F1 = 0.671000/0.285571\n",
      "Saved parameters to ./04-28-03/model.ckpt-25000\n",
      "148.40 epochs, iter 25200: train_loss = 0.720904946, param_norm = 449.194, grad_norm = 0.497, train_F1/valid_F1 = 0.674000/0.261523\n",
      "149.58 epochs, iter 25400: train_loss = 0.775388718, param_norm = 450.176, grad_norm = 0.943, train_F1/valid_F1 = 0.654000/0.265531\n",
      "150.76 epochs, iter 25600: train_loss = 0.887095869, param_norm = 451.269, grad_norm = 2.081, train_F1/valid_F1 = 0.648000/0.263527\n",
      "151.93 epochs, iter 25800: train_loss = 0.732983232, param_norm = 452.561, grad_norm = 0.468, train_F1/valid_F1 = 0.675000/0.249499\n",
      "153.11 epochs, iter 26000: train_loss = 0.746551394, param_norm = 453.558, grad_norm = 0.543, train_F1/valid_F1 = 0.676000/0.344689\n",
      "Saved parameters to ./04-28-03/model.ckpt-26000\n",
      "154.29 epochs, iter 26200: train_loss = 0.798907340, param_norm = 454.559, grad_norm = 2.931, train_F1/valid_F1 = 0.689000/0.276553\n",
      "155.47 epochs, iter 26400: train_loss = 0.745838702, param_norm = 455.815, grad_norm = 0.525, train_F1/valid_F1 = 0.673000/0.274549\n",
      "156.65 epochs, iter 26600: train_loss = 0.694806933, param_norm = 456.909, grad_norm = 1.174, train_F1/valid_F1 = 0.690000/0.325651\n",
      "157.82 epochs, iter 26800: train_loss = 2.274406195, param_norm = 458.117, grad_norm = 3172.487, train_F1/valid_F1 = 0.680000/0.362725\n",
      "159.00 epochs, iter 27000: train_loss = 0.666432858, param_norm = 458.603, grad_norm = 1.631, train_F1/valid_F1 = 0.704000/0.270541\n",
      "Saved parameters to ./04-28-03/model.ckpt-27000\n",
      "160.18 epochs, iter 27200: train_loss = 0.620227218, param_norm = 458.994, grad_norm = 0.367, train_F1/valid_F1 = 0.730000/0.261523\n",
      "161.36 epochs, iter 27400: train_loss = 0.612742722, param_norm = 459.496, grad_norm = 0.450, train_F1/valid_F1 = 0.733000/0.297595\n",
      "162.53 epochs, iter 27600: train_loss = 0.605396688, param_norm = 459.908, grad_norm = 2.783, train_F1/valid_F1 = 0.749000/0.313627\n",
      "163.71 epochs, iter 27800: train_loss = 0.587854505, param_norm = 460.408, grad_norm = 0.552, train_F1/valid_F1 = 0.744000/0.302605\n",
      "164.89 epochs, iter 28000: train_loss = 0.588029921, param_norm = 460.827, grad_norm = 0.834, train_F1/valid_F1 = 0.752000/0.318637\n",
      "Saved parameters to ./04-28-03/model.ckpt-28000\n",
      "166.07 epochs, iter 28200: train_loss = 0.606167018, param_norm = 461.232, grad_norm = 5.814, train_F1/valid_F1 = 0.741000/0.309619\n",
      "167.25 epochs, iter 28400: train_loss = 0.605025291, param_norm = 461.616, grad_norm = 3.192, train_F1/valid_F1 = 0.754000/0.314629\n",
      "168.42 epochs, iter 28600: train_loss = 0.570859313, param_norm = 462.063, grad_norm = 0.533, train_F1/valid_F1 = 0.749000/0.326653\n",
      "169.60 epochs, iter 28800: train_loss = 0.572614908, param_norm = 462.563, grad_norm = 0.541, train_F1/valid_F1 = 0.748000/0.308617\n",
      "170.78 epochs, iter 29000: train_loss = 0.556012332, param_norm = 463.044, grad_norm = 0.577, train_F1/valid_F1 = 0.751000/0.304609\n",
      "Saved parameters to ./04-28-03/model.ckpt-29000\n",
      "171.96 epochs, iter 29200: train_loss = 0.655693114, param_norm = 463.514, grad_norm = 0.732, train_F1/valid_F1 = 0.707000/0.296593\n",
      "173.13 epochs, iter 29400: train_loss = 0.557930291, param_norm = 463.931, grad_norm = 0.418, train_F1/valid_F1 = 0.763000/0.308617\n",
      "174.31 epochs, iter 29600: train_loss = 0.547790945, param_norm = 464.377, grad_norm = 0.774, train_F1/valid_F1 = 0.767000/0.334669\n",
      "175.49 epochs, iter 29800: train_loss = 0.533617616, param_norm = 464.845, grad_norm = 0.511, train_F1/valid_F1 = 0.771000/0.319639\n",
      "176.67 epochs, iter 30000: train_loss = 0.541353524, param_norm = 465.317, grad_norm = 0.989, train_F1/valid_F1 = 0.768000/0.332665\n",
      "Saved parameters to ./04-28-03/model.ckpt-30000\n",
      "177.85 epochs, iter 30200: train_loss = 0.602418303, param_norm = 465.821, grad_norm = 1.134, train_F1/valid_F1 = 0.737000/0.328657\n",
      "179.02 epochs, iter 30400: train_loss = 0.518386304, param_norm = 466.243, grad_norm = 0.522, train_F1/valid_F1 = 0.776000/0.310621\n",
      "180.20 epochs, iter 30600: train_loss = 0.554237306, param_norm = 466.701, grad_norm = 0.904, train_F1/valid_F1 = 0.758000/0.280561\n",
      "181.38 epochs, iter 30800: train_loss = 0.503925920, param_norm = 467.215, grad_norm = 0.907, train_F1/valid_F1 = 0.779000/0.343687\n",
      "182.56 epochs, iter 31000: train_loss = 0.530261457, param_norm = 467.628, grad_norm = 0.845, train_F1/valid_F1 = 0.786000/0.291583\n",
      "Saved parameters to ./04-28-03/model.ckpt-31000\n",
      "183.73 epochs, iter 31200: train_loss = 0.497575670, param_norm = 468.078, grad_norm = 0.938, train_F1/valid_F1 = 0.792000/0.317635\n",
      "184.91 epochs, iter 31400: train_loss = 0.515755653, param_norm = 468.464, grad_norm = 0.742, train_F1/valid_F1 = 0.772000/0.290581\n",
      "186.09 epochs, iter 31600: train_loss = 0.496193647, param_norm = 468.971, grad_norm = 0.733, train_F1/valid_F1 = 0.786000/0.328657\n",
      "187.27 epochs, iter 31800: train_loss = 0.520898044, param_norm = 469.431, grad_norm = 0.686, train_F1/valid_F1 = 0.777000/0.293587\n",
      "188.45 epochs, iter 32000: train_loss = 0.543279946, param_norm = 469.950, grad_norm = 1.187, train_F1/valid_F1 = 0.771000/0.329659\n",
      "Saved parameters to ./04-28-03/model.ckpt-32000\n",
      "189.62 epochs, iter 32200: train_loss = 0.491601199, param_norm = 470.436, grad_norm = 1.677, train_F1/valid_F1 = 0.789000/0.316633\n",
      "190.80 epochs, iter 32400: train_loss = 0.510919273, param_norm = 470.899, grad_norm = 0.903, train_F1/valid_F1 = 0.786000/0.267535\n",
      "191.98 epochs, iter 32600: train_loss = 0.446553737, param_norm = 471.404, grad_norm = 0.593, train_F1/valid_F1 = 0.806000/0.308617\n",
      "193.16 epochs, iter 32800: train_loss = 0.459844679, param_norm = 471.891, grad_norm = 0.863, train_F1/valid_F1 = 0.796000/0.309619\n",
      "194.33 epochs, iter 33000: train_loss = 0.438358635, param_norm = 472.391, grad_norm = 0.775, train_F1/valid_F1 = 0.813000/0.311623\n",
      "Saved parameters to ./04-28-03/model.ckpt-33000\n",
      "195.51 epochs, iter 33200: train_loss = 0.685071290, param_norm = 472.891, grad_norm = 7.977, train_F1/valid_F1 = 0.805000/0.287575\n",
      "196.69 epochs, iter 33400: train_loss = 0.629275203, param_norm = 473.068, grad_norm = 40.293, train_F1/valid_F1 = 0.820000/0.324649\n",
      "197.87 epochs, iter 33600: train_loss = 0.410110027, param_norm = 473.255, grad_norm = 1.543, train_F1/valid_F1 = 0.826000/0.300601\n",
      "199.05 epochs, iter 33800: train_loss = 0.394309998, param_norm = 473.459, grad_norm = 0.482, train_F1/valid_F1 = 0.828000/0.291583\n",
      "200.22 epochs, iter 34000: train_loss = 0.395523012, param_norm = 473.661, grad_norm = 1.170, train_F1/valid_F1 = 0.827000/0.286573\n",
      "Saved parameters to ./04-28-03/model.ckpt-34000\n",
      "201.40 epochs, iter 34200: train_loss = 0.396295637, param_norm = 473.869, grad_norm = 0.659, train_F1/valid_F1 = 0.826000/0.292585\n",
      "202.58 epochs, iter 34400: train_loss = 0.380671710, param_norm = 474.071, grad_norm = 0.698, train_F1/valid_F1 = 0.842000/0.304609\n",
      "203.76 epochs, iter 34600: train_loss = 0.377575010, param_norm = 474.287, grad_norm = 0.437, train_F1/valid_F1 = 0.844000/0.289579\n",
      "204.93 epochs, iter 34800: train_loss = 0.365212172, param_norm = 474.473, grad_norm = 1.142, train_F1/valid_F1 = 0.838000/0.315631\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "206.11 epochs, iter 35000: train_loss = 0.354198337, param_norm = 474.676, grad_norm = 0.450, train_F1/valid_F1 = 0.849000/0.300601\n",
      "Saved parameters to ./04-28-03/model.ckpt-35000\n",
      "207.29 epochs, iter 35200: train_loss = 0.389337361, param_norm = 474.838, grad_norm = 1.053, train_F1/valid_F1 = 0.825000/0.324649\n",
      "208.47 epochs, iter 35400: train_loss = 0.357461870, param_norm = 475.032, grad_norm = 0.740, train_F1/valid_F1 = 0.841000/0.318637\n",
      "209.65 epochs, iter 35600: train_loss = 0.364189535, param_norm = 475.218, grad_norm = 0.676, train_F1/valid_F1 = 0.841000/0.326653\n",
      "210.82 epochs, iter 35800: train_loss = 0.354498625, param_norm = 475.415, grad_norm = 2.894, train_F1/valid_F1 = 0.853000/0.305611\n",
      "212.00 epochs, iter 36000: train_loss = 0.347531438, param_norm = 475.603, grad_norm = 1.448, train_F1/valid_F1 = 0.847000/0.334669\n",
      "Saved parameters to ./04-28-03/model.ckpt-36000\n",
      "213.18 epochs, iter 36200: train_loss = 0.332261145, param_norm = 475.751, grad_norm = 0.449, train_F1/valid_F1 = 0.856000/0.312625\n",
      "214.36 epochs, iter 36400: train_loss = 0.326012403, param_norm = 475.972, grad_norm = 0.546, train_F1/valid_F1 = 0.858000/0.306613\n",
      "215.54 epochs, iter 36600: train_loss = 0.369388789, param_norm = 476.131, grad_norm = 1.241, train_F1/valid_F1 = 0.845000/0.313627\n",
      "216.71 epochs, iter 36800: train_loss = 0.322371602, param_norm = 476.312, grad_norm = 0.643, train_F1/valid_F1 = 0.858000/0.310621\n",
      "217.89 epochs, iter 37000: train_loss = 0.328060240, param_norm = 476.496, grad_norm = 0.498, train_F1/valid_F1 = 0.869000/0.322645\n",
      "Saved parameters to ./04-28-03/model.ckpt-37000\n",
      "219.07 epochs, iter 37200: train_loss = 0.314688981, param_norm = 476.695, grad_norm = 0.546, train_F1/valid_F1 = 0.871000/0.325651\n",
      "220.25 epochs, iter 37400: train_loss = 0.323451847, param_norm = 476.858, grad_norm = 1.300, train_F1/valid_F1 = 0.860000/0.323647\n",
      "221.42 epochs, iter 37600: train_loss = 0.301234156, param_norm = 477.008, grad_norm = 0.682, train_F1/valid_F1 = 0.863000/0.325651\n",
      "222.60 epochs, iter 37800: train_loss = 0.324214101, param_norm = 477.217, grad_norm = 1.842, train_F1/valid_F1 = 0.866000/0.317635\n",
      "223.78 epochs, iter 38000: train_loss = 0.308409661, param_norm = 477.340, grad_norm = 2.289, train_F1/valid_F1 = 0.868000/0.337675\n",
      "Saved parameters to ./04-28-03/model.ckpt-38000\n",
      "224.96 epochs, iter 38200: train_loss = 0.424492419, param_norm = 477.518, grad_norm = 4.437, train_F1/valid_F1 = 0.851000/0.298597\n",
      "226.14 epochs, iter 38400: train_loss = 0.273337394, param_norm = 477.596, grad_norm = 0.809, train_F1/valid_F1 = 0.879000/0.350701\n",
      "227.31 epochs, iter 38600: train_loss = 0.273517132, param_norm = 477.685, grad_norm = 0.473, train_F1/valid_F1 = 0.888000/0.331663\n",
      "228.49 epochs, iter 38800: train_loss = 0.291921556, param_norm = 477.756, grad_norm = 8.452, train_F1/valid_F1 = 0.895000/0.354709\n",
      "229.67 epochs, iter 39000: train_loss = 0.269501656, param_norm = 477.830, grad_norm = 0.624, train_F1/valid_F1 = 0.884000/0.363727\n",
      "Saved parameters to ./04-28-03/model.ckpt-39000\n",
      "230.85 epochs, iter 39200: train_loss = 0.279055536, param_norm = 477.920, grad_norm = 1.146, train_F1/valid_F1 = 0.890000/0.359719\n",
      "232.02 epochs, iter 39400: train_loss = 0.268872619, param_norm = 477.996, grad_norm = 0.646, train_F1/valid_F1 = 0.885000/0.352705\n",
      "233.20 epochs, iter 39600: train_loss = 0.256179154, param_norm = 478.083, grad_norm = 0.684, train_F1/valid_F1 = 0.896000/0.343687\n",
      "234.38 epochs, iter 39800: train_loss = 0.260153145, param_norm = 478.158, grad_norm = 0.977, train_F1/valid_F1 = 0.898000/0.338677\n",
      "235.56 epochs, iter 40000: train_loss = 0.246742755, param_norm = 478.240, grad_norm = 0.498, train_F1/valid_F1 = 0.898000/0.333667\n",
      "Saved parameters to ./04-28-03/model.ckpt-40000\n",
      "236.74 epochs, iter 40200: train_loss = 0.305473506, param_norm = 478.319, grad_norm = 3.595, train_F1/valid_F1 = 0.895000/0.317635\n",
      "237.91 epochs, iter 40400: train_loss = 0.238970369, param_norm = 478.350, grad_norm = 0.737, train_F1/valid_F1 = 0.901000/0.352705\n",
      "239.09 epochs, iter 40600: train_loss = 0.235120893, param_norm = 478.389, grad_norm = 0.449, train_F1/valid_F1 = 0.911000/0.336673\n",
      "240.27 epochs, iter 40800: train_loss = 0.236119688, param_norm = 478.427, grad_norm = 0.463, train_F1/valid_F1 = 0.909000/0.338677\n",
      "241.45 epochs, iter 41000: train_loss = 0.240346462, param_norm = 478.457, grad_norm = 0.601, train_F1/valid_F1 = 0.902000/0.333667\n",
      "Saved parameters to ./04-28-03/model.ckpt-41000\n",
      "242.62 epochs, iter 41200: train_loss = 0.230315134, param_norm = 478.484, grad_norm = 0.417, train_F1/valid_F1 = 0.907000/0.346693\n",
      "243.80 epochs, iter 41400: train_loss = 0.228212506, param_norm = 478.518, grad_norm = 0.663, train_F1/valid_F1 = 0.915000/0.350701\n",
      "244.98 epochs, iter 41600: train_loss = 0.227262437, param_norm = 478.555, grad_norm = 1.325, train_F1/valid_F1 = 0.911000/0.336673\n",
      "246.16 epochs, iter 41800: train_loss = 0.229948282, param_norm = 478.587, grad_norm = 0.533, train_F1/valid_F1 = 0.911000/0.337675\n",
      "247.34 epochs, iter 42000: train_loss = 0.237130567, param_norm = 478.607, grad_norm = 1.758, train_F1/valid_F1 = 0.909000/0.353707\n",
      "Saved parameters to ./04-28-03/model.ckpt-42000\n",
      "248.51 epochs, iter 42200: train_loss = 0.230752572, param_norm = 478.648, grad_norm = 2.003, train_F1/valid_F1 = 0.912000/0.354709\n",
      "249.69 epochs, iter 42400: train_loss = 0.232795775, param_norm = 478.676, grad_norm = 1.824, train_F1/valid_F1 = 0.914000/0.344689\n",
      "250.87 epochs, iter 42600: train_loss = 0.222695619, param_norm = 478.708, grad_norm = 0.517, train_F1/valid_F1 = 0.913000/0.339679\n",
      "252.05 epochs, iter 42800: train_loss = 0.220539004, param_norm = 478.736, grad_norm = 1.848, train_F1/valid_F1 = 0.920000/0.350701\n",
      "253.22 epochs, iter 43000: train_loss = 0.214259624, param_norm = 478.759, grad_norm = 0.413, train_F1/valid_F1 = 0.920000/0.354709\n",
      "Saved parameters to ./04-28-03/model.ckpt-43000\n",
      "254.40 epochs, iter 43200: train_loss = 0.259270519, param_norm = 478.797, grad_norm = 11.528, train_F1/valid_F1 = 0.910000/0.346693\n",
      "255.58 epochs, iter 43400: train_loss = 0.211240724, param_norm = 478.813, grad_norm = 0.537, train_F1/valid_F1 = 0.915000/0.351703\n",
      "256.76 epochs, iter 43600: train_loss = 0.211608782, param_norm = 478.829, grad_norm = 1.432, train_F1/valid_F1 = 0.920000/0.349699\n",
      "257.94 epochs, iter 43800: train_loss = 0.208092406, param_norm = 478.847, grad_norm = 0.661, train_F1/valid_F1 = 0.921000/0.355711\n",
      "259.11 epochs, iter 44000: train_loss = 0.207608491, param_norm = 478.859, grad_norm = 0.960, train_F1/valid_F1 = 0.922000/0.353707\n",
      "Saved parameters to ./04-28-03/model.ckpt-44000\n",
      "260.29 epochs, iter 44200: train_loss = 0.207078338, param_norm = 478.876, grad_norm = 0.403, train_F1/valid_F1 = 0.924000/0.348697\n",
      "261.47 epochs, iter 44400: train_loss = 0.203904748, param_norm = 478.889, grad_norm = 0.490, train_F1/valid_F1 = 0.921000/0.350701\n",
      "262.65 epochs, iter 44600: train_loss = 0.206877917, param_norm = 478.901, grad_norm = 0.486, train_F1/valid_F1 = 0.924000/0.353707\n",
      "263.82 epochs, iter 44800: train_loss = 0.205896616, param_norm = 478.922, grad_norm = 0.496, train_F1/valid_F1 = 0.922000/0.346693\n",
      "265.00 epochs, iter 45000: train_loss = 0.204751551, param_norm = 478.936, grad_norm = 0.863, train_F1/valid_F1 = 0.923000/0.352705\n",
      "Saved parameters to ./04-28-03/model.ckpt-45000\n",
      "266.18 epochs, iter 45200: train_loss = 0.204952195, param_norm = 478.944, grad_norm = 0.625, train_F1/valid_F1 = 0.920000/0.356713\n",
      "267.36 epochs, iter 45400: train_loss = 0.204703763, param_norm = 478.961, grad_norm = 2.021, train_F1/valid_F1 = 0.922000/0.370741\n",
      "268.54 epochs, iter 45600: train_loss = 0.202862471, param_norm = 478.974, grad_norm = 0.482, train_F1/valid_F1 = 0.925000/0.360721\n",
      "269.71 epochs, iter 45800: train_loss = 0.197022185, param_norm = 478.989, grad_norm = 0.436, train_F1/valid_F1 = 0.926000/0.354709\n",
      "270.89 epochs, iter 46000: train_loss = 0.199552298, param_norm = 479.005, grad_norm = 0.463, train_F1/valid_F1 = 0.924000/0.364729\n",
      "Saved parameters to ./04-28-03/model.ckpt-46000\n",
      "272.07 epochs, iter 46200: train_loss = 0.195415914, param_norm = 479.020, grad_norm = 0.410, train_F1/valid_F1 = 0.929000/0.363727\n",
      "273.25 epochs, iter 46400: train_loss = 0.202714369, param_norm = 479.034, grad_norm = 0.586, train_F1/valid_F1 = 0.921000/0.356713\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "274.42 epochs, iter 46600: train_loss = 0.195762277, param_norm = 479.047, grad_norm = 0.457, train_F1/valid_F1 = 0.927000/0.358717\n",
      "275.60 epochs, iter 46800: train_loss = 0.195859909, param_norm = 479.063, grad_norm = 0.801, train_F1/valid_F1 = 0.929000/0.358717\n",
      "276.78 epochs, iter 47000: train_loss = 0.194992363, param_norm = 479.077, grad_norm = 0.452, train_F1/valid_F1 = 0.929000/0.363727\n",
      "Saved parameters to ./04-28-03/model.ckpt-47000\n",
      "277.96 epochs, iter 47200: train_loss = 0.195090175, param_norm = 479.090, grad_norm = 0.506, train_F1/valid_F1 = 0.925000/0.364729\n",
      "279.14 epochs, iter 47400: train_loss = 0.194875628, param_norm = 479.102, grad_norm = 0.576, train_F1/valid_F1 = 0.929000/0.356713\n",
      "280.31 epochs, iter 47600: train_loss = 0.192548320, param_norm = 479.120, grad_norm = 4.592, train_F1/valid_F1 = 0.932000/0.356713\n",
      "281.49 epochs, iter 47800: train_loss = 0.235659540, param_norm = 479.132, grad_norm = 10.270, train_F1/valid_F1 = 0.930000/0.365731\n",
      "282.67 epochs, iter 48000: train_loss = 0.187452912, param_norm = 479.138, grad_norm = 0.394, train_F1/valid_F1 = 0.932000/0.350701\n",
      "Saved parameters to ./04-28-03/model.ckpt-48000\n",
      "283.85 epochs, iter 48200: train_loss = 0.188730374, param_norm = 479.145, grad_norm = 0.487, train_F1/valid_F1 = 0.931000/0.357715\n",
      "285.02 epochs, iter 48400: train_loss = 0.187952012, param_norm = 479.154, grad_norm = 0.991, train_F1/valid_F1 = 0.935000/0.361723\n",
      "286.20 epochs, iter 48600: train_loss = 0.184682399, param_norm = 479.159, grad_norm = 0.402, train_F1/valid_F1 = 0.933000/0.360721\n",
      "287.38 epochs, iter 48800: train_loss = 0.185303733, param_norm = 479.167, grad_norm = 0.367, train_F1/valid_F1 = 0.935000/0.366733\n",
      "288.56 epochs, iter 49000: train_loss = 0.184555724, param_norm = 479.174, grad_norm = 0.496, train_F1/valid_F1 = 0.930000/0.363727\n",
      "Saved parameters to ./04-28-03/model.ckpt-49000\n",
      "289.74 epochs, iter 49200: train_loss = 0.184838310, param_norm = 479.181, grad_norm = 0.418, train_F1/valid_F1 = 0.936000/0.365731\n",
      "290.91 epochs, iter 49400: train_loss = 0.184426695, param_norm = 479.189, grad_norm = 0.438, train_F1/valid_F1 = 0.935000/0.361723\n",
      "292.09 epochs, iter 49600: train_loss = 0.182435423, param_norm = 479.196, grad_norm = 0.387, train_F1/valid_F1 = 0.937000/0.357715\n",
      "293.27 epochs, iter 49800: train_loss = 0.182369888, param_norm = 479.204, grad_norm = 0.376, train_F1/valid_F1 = 0.937000/0.357715\n",
      "294.45 epochs, iter 50000: train_loss = 0.182946220, param_norm = 479.210, grad_norm = 0.403, train_F1/valid_F1 = 0.939000/0.362725\n",
      "Saved parameters to ./04-28-03/model.ckpt-50000\n",
      "295.62 epochs, iter 50200: train_loss = 0.181253910, param_norm = 479.216, grad_norm = 0.407, train_F1/valid_F1 = 0.936000/0.363727\n",
      "296.80 epochs, iter 50400: train_loss = 0.179582506, param_norm = 479.223, grad_norm = 0.417, train_F1/valid_F1 = 0.936000/0.358717\n",
      "297.98 epochs, iter 50600: train_loss = 0.181824759, param_norm = 479.230, grad_norm = 0.501, train_F1/valid_F1 = 0.936000/0.357715\n",
      "299.16 epochs, iter 50800: train_loss = 0.182158485, param_norm = 479.235, grad_norm = 0.504, train_F1/valid_F1 = 0.935000/0.358717\n",
      "300.34 epochs, iter 51000: train_loss = 0.181335419, param_norm = 479.243, grad_norm = 0.454, train_F1/valid_F1 = 0.938000/0.353707\n",
      "Saved parameters to ./04-28-03/model.ckpt-51000\n",
      "301.51 epochs, iter 51200: train_loss = 0.179177880, param_norm = 479.249, grad_norm = 0.486, train_F1/valid_F1 = 0.936000/0.354709\n",
      "302.69 epochs, iter 51400: train_loss = 0.179266110, param_norm = 479.256, grad_norm = 0.558, train_F1/valid_F1 = 0.937000/0.350701\n",
      "303.87 epochs, iter 51600: train_loss = 0.178068951, param_norm = 479.262, grad_norm = 0.442, train_F1/valid_F1 = 0.942000/0.366733\n",
      "305.05 epochs, iter 51800: train_loss = 0.179630816, param_norm = 479.269, grad_norm = 0.536, train_F1/valid_F1 = 0.940000/0.357715\n",
      "306.22 epochs, iter 52000: train_loss = 0.177278981, param_norm = 479.274, grad_norm = 0.461, train_F1/valid_F1 = 0.935000/0.360721\n",
      "Saved parameters to ./04-28-03/model.ckpt-52000\n",
      "307.40 epochs, iter 52200: train_loss = 0.177298978, param_norm = 479.282, grad_norm = 0.425, train_F1/valid_F1 = 0.941000/0.356713\n",
      "308.58 epochs, iter 52400: train_loss = 0.176395252, param_norm = 479.286, grad_norm = 0.556, train_F1/valid_F1 = 0.936000/0.353707\n",
      "309.76 epochs, iter 52600: train_loss = 0.176780611, param_norm = 479.294, grad_norm = 0.504, train_F1/valid_F1 = 0.941000/0.350701\n",
      "310.94 epochs, iter 52800: train_loss = 0.174040318, param_norm = 479.301, grad_norm = 0.579, train_F1/valid_F1 = 0.939000/0.359719\n",
      "312.11 epochs, iter 53000: train_loss = 0.174956441, param_norm = 479.305, grad_norm = 1.061, train_F1/valid_F1 = 0.939000/0.361723\n",
      "Saved parameters to ./04-28-03/model.ckpt-53000\n",
      "313.29 epochs, iter 53200: train_loss = 0.175194293, param_norm = 479.315, grad_norm = 0.912, train_F1/valid_F1 = 0.940000/0.358717\n",
      "314.47 epochs, iter 53400: train_loss = 0.174955308, param_norm = 479.321, grad_norm = 0.493, train_F1/valid_F1 = 0.937000/0.345691\n",
      "315.65 epochs, iter 53600: train_loss = 0.173418835, param_norm = 479.329, grad_norm = 0.491, train_F1/valid_F1 = 0.939000/0.354709\n",
      "316.82 epochs, iter 53800: train_loss = 0.172295257, param_norm = 479.334, grad_norm = 0.527, train_F1/valid_F1 = 0.944000/0.352705\n",
      "318.00 epochs, iter 54000: train_loss = 0.172623798, param_norm = 479.340, grad_norm = 0.517, train_F1/valid_F1 = 0.939000/0.345691\n",
      "Saved parameters to ./04-28-03/model.ckpt-54000\n",
      "319.18 epochs, iter 54200: train_loss = 0.169824064, param_norm = 479.346, grad_norm = 0.358, train_F1/valid_F1 = 0.941000/0.347695\n",
      "320.36 epochs, iter 54400: train_loss = 0.171730161, param_norm = 479.354, grad_norm = 0.613, train_F1/valid_F1 = 0.938000/0.345691\n",
      "321.54 epochs, iter 54600: train_loss = 0.168522671, param_norm = 479.361, grad_norm = 0.359, train_F1/valid_F1 = 0.939000/0.350701\n",
      "322.71 epochs, iter 54800: train_loss = 0.171640411, param_norm = 479.366, grad_norm = 0.756, train_F1/valid_F1 = 0.947000/0.353707\n",
      "323.89 epochs, iter 55000: train_loss = 0.168045804, param_norm = 479.371, grad_norm = 0.492, train_F1/valid_F1 = 0.942000/0.350701\n",
      "Saved parameters to ./04-28-03/model.ckpt-55000\n",
      "325.07 epochs, iter 55200: train_loss = 0.167795405, param_norm = 479.379, grad_norm = 0.425, train_F1/valid_F1 = 0.942000/0.350701\n",
      "326.25 epochs, iter 55400: train_loss = 0.168612242, param_norm = 479.387, grad_norm = 0.440, train_F1/valid_F1 = 0.941000/0.349699\n",
      "327.42 epochs, iter 55600: train_loss = 0.167586893, param_norm = 479.393, grad_norm = 0.395, train_F1/valid_F1 = 0.945000/0.357715\n",
      "328.60 epochs, iter 55800: train_loss = 0.169421673, param_norm = 479.399, grad_norm = 0.656, train_F1/valid_F1 = 0.940000/0.346693\n",
      "329.78 epochs, iter 56000: train_loss = 0.168038666, param_norm = 479.407, grad_norm = 0.564, train_F1/valid_F1 = 0.944000/0.354709\n",
      "Saved parameters to ./04-28-03/model.ckpt-56000\n",
      "330.96 epochs, iter 56200: train_loss = 0.168123379, param_norm = 479.415, grad_norm = 0.666, train_F1/valid_F1 = 0.941000/0.338677\n",
      "332.14 epochs, iter 56400: train_loss = 0.167854518, param_norm = 479.418, grad_norm = 0.512, train_F1/valid_F1 = 0.943000/0.348697\n",
      "333.31 epochs, iter 56600: train_loss = 0.165186778, param_norm = 479.426, grad_norm = 0.477, train_F1/valid_F1 = 0.943000/0.359719\n",
      "334.49 epochs, iter 56800: train_loss = 0.164037496, param_norm = 479.432, grad_norm = 0.415, train_F1/valid_F1 = 0.948000/0.348697\n",
      "335.67 epochs, iter 57000: train_loss = 0.164719507, param_norm = 479.437, grad_norm = 1.138, train_F1/valid_F1 = 0.944000/0.355711\n",
      "Saved parameters to ./04-28-03/model.ckpt-57000\n",
      "336.85 epochs, iter 57200: train_loss = 0.161635265, param_norm = 479.444, grad_norm = 0.416, train_F1/valid_F1 = 0.948000/0.348697\n",
      "338.02 epochs, iter 57400: train_loss = 0.160698816, param_norm = 479.450, grad_norm = 0.386, train_F1/valid_F1 = 0.948000/0.351703\n",
      "339.20 epochs, iter 57600: train_loss = 0.163176924, param_norm = 479.458, grad_norm = 0.600, train_F1/valid_F1 = 0.949000/0.353707\n",
      "340.38 epochs, iter 57800: train_loss = 0.162709743, param_norm = 479.462, grad_norm = 0.541, train_F1/valid_F1 = 0.947000/0.350701\n",
      "341.56 epochs, iter 58000: train_loss = 0.160277799, param_norm = 479.470, grad_norm = 0.400, train_F1/valid_F1 = 0.949000/0.352705\n",
      "Saved parameters to ./04-28-03/model.ckpt-58000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "342.74 epochs, iter 58200: train_loss = 0.161214188, param_norm = 479.477, grad_norm = 0.554, train_F1/valid_F1 = 0.949000/0.356713\n",
      "343.91 epochs, iter 58400: train_loss = 0.162113458, param_norm = 479.484, grad_norm = 0.540, train_F1/valid_F1 = 0.950000/0.353707\n",
      "345.09 epochs, iter 58600: train_loss = 0.162804186, param_norm = 479.491, grad_norm = 5.585, train_F1/valid_F1 = 0.950000/0.354709\n",
      "346.27 epochs, iter 58800: train_loss = 0.185805574, param_norm = 479.497, grad_norm = 12.449, train_F1/valid_F1 = 0.947000/0.354709\n",
      "347.45 epochs, iter 59000: train_loss = 0.158980012, param_norm = 479.503, grad_norm = 0.518, train_F1/valid_F1 = 0.948000/0.363727\n",
      "Saved parameters to ./04-28-03/model.ckpt-59000\n",
      "348.62 epochs, iter 59200: train_loss = 0.159244075, param_norm = 479.508, grad_norm = 0.529, train_F1/valid_F1 = 0.951000/0.357715\n",
      "349.80 epochs, iter 59400: train_loss = 0.156986982, param_norm = 479.514, grad_norm = 0.505, train_F1/valid_F1 = 0.952000/0.351703\n",
      "350.98 epochs, iter 59600: train_loss = 0.158582926, param_norm = 479.521, grad_norm = 1.842, train_F1/valid_F1 = 0.949000/0.352705\n",
      "352.16 epochs, iter 59800: train_loss = 0.158218265, param_norm = 479.526, grad_norm = 0.546, train_F1/valid_F1 = 0.949000/0.346693\n",
      "353.34 epochs, iter 60000: train_loss = 0.154047742, param_norm = 479.532, grad_norm = 0.479, train_F1/valid_F1 = 0.952000/0.357715\n",
      "Saved parameters to ./04-28-03/model.ckpt-60000\n",
      "354.51 epochs, iter 60200: train_loss = 0.154799417, param_norm = 479.539, grad_norm = 0.401, train_F1/valid_F1 = 0.952000/0.354709\n",
      "355.69 epochs, iter 60400: train_loss = 0.155584484, param_norm = 479.546, grad_norm = 0.511, train_F1/valid_F1 = 0.949000/0.368737\n",
      "356.87 epochs, iter 60600: train_loss = 0.155444115, param_norm = 479.553, grad_norm = 0.390, train_F1/valid_F1 = 0.950000/0.362725\n",
      "358.05 epochs, iter 60800: train_loss = 0.157069609, param_norm = 479.561, grad_norm = 0.950, train_F1/valid_F1 = 0.949000/0.365731\n",
      "359.23 epochs, iter 61000: train_loss = 0.154071197, param_norm = 479.566, grad_norm = 0.609, train_F1/valid_F1 = 0.950000/0.368737\n",
      "Saved parameters to ./04-28-03/model.ckpt-61000\n",
      "360.40 epochs, iter 61200: train_loss = 0.153065264, param_norm = 479.574, grad_norm = 0.455, train_F1/valid_F1 = 0.952000/0.368737\n",
      "361.58 epochs, iter 61400: train_loss = 0.154100686, param_norm = 479.578, grad_norm = 0.563, train_F1/valid_F1 = 0.953000/0.360721\n",
      "362.76 epochs, iter 61600: train_loss = 0.151119024, param_norm = 479.583, grad_norm = 0.472, train_F1/valid_F1 = 0.954000/0.363727\n",
      "363.94 epochs, iter 61800: train_loss = 0.160729051, param_norm = 479.591, grad_norm = 12.480, train_F1/valid_F1 = 0.948000/0.364729\n",
      "365.11 epochs, iter 62000: train_loss = 0.164020002, param_norm = 479.597, grad_norm = 12.438, train_F1/valid_F1 = 0.950000/0.357715\n",
      "Saved parameters to ./04-28-03/model.ckpt-62000\n",
      "366.29 epochs, iter 62200: train_loss = 0.152000576, param_norm = 479.603, grad_norm = 0.524, train_F1/valid_F1 = 0.951000/0.356713\n",
      "367.47 epochs, iter 62400: train_loss = 0.150798768, param_norm = 479.611, grad_norm = 0.574, train_F1/valid_F1 = 0.951000/0.355711\n",
      "368.65 epochs, iter 62600: train_loss = 0.149045199, param_norm = 479.617, grad_norm = 0.679, train_F1/valid_F1 = 0.953000/0.355711\n",
      "369.83 epochs, iter 62800: train_loss = 0.150081754, param_norm = 479.621, grad_norm = 0.509, train_F1/valid_F1 = 0.952000/0.361723\n",
      "371.00 epochs, iter 63000: train_loss = 0.150662526, param_norm = 479.629, grad_norm = 0.636, train_F1/valid_F1 = 0.953000/0.369739\n",
      "Saved parameters to ./04-28-03/model.ckpt-63000\n",
      "372.18 epochs, iter 63200: train_loss = 0.169160277, param_norm = 479.635, grad_norm = 12.597, train_F1/valid_F1 = 0.955000/0.366733\n",
      "373.36 epochs, iter 63400: train_loss = 0.148701444, param_norm = 479.640, grad_norm = 0.625, train_F1/valid_F1 = 0.952000/0.363727\n",
      "374.54 epochs, iter 63600: train_loss = 0.149260640, param_norm = 479.645, grad_norm = 0.593, train_F1/valid_F1 = 0.952000/0.360721\n",
      "375.71 epochs, iter 63800: train_loss = 0.146892756, param_norm = 479.651, grad_norm = 0.772, train_F1/valid_F1 = 0.953000/0.355711\n",
      "376.89 epochs, iter 64000: train_loss = 0.146534562, param_norm = 479.657, grad_norm = 0.622, train_F1/valid_F1 = 0.953000/0.364729\n",
      "Saved parameters to ./04-28-03/model.ckpt-64000\n",
      "378.07 epochs, iter 64200: train_loss = 0.154860869, param_norm = 479.663, grad_norm = 5.857, train_F1/valid_F1 = 0.953000/0.361723\n",
      "379.25 epochs, iter 64400: train_loss = 0.145213097, param_norm = 479.672, grad_norm = 0.758, train_F1/valid_F1 = 0.953000/0.354709\n",
      "380.43 epochs, iter 64600: train_loss = 0.145600036, param_norm = 479.675, grad_norm = 0.469, train_F1/valid_F1 = 0.952000/0.363727\n",
      "381.60 epochs, iter 64800: train_loss = 0.147271335, param_norm = 479.682, grad_norm = 0.619, train_F1/valid_F1 = 0.951000/0.354709\n",
      "382.78 epochs, iter 65000: train_loss = 0.145857766, param_norm = 479.689, grad_norm = 0.694, train_F1/valid_F1 = 0.953000/0.363727\n",
      "Saved parameters to ./04-28-03/model.ckpt-65000\n",
      "383.96 epochs, iter 65200: train_loss = 0.144007504, param_norm = 479.694, grad_norm = 0.480, train_F1/valid_F1 = 0.955000/0.360721\n",
      "385.14 epochs, iter 65400: train_loss = 0.142754138, param_norm = 479.701, grad_norm = 0.421, train_F1/valid_F1 = 0.955000/0.362725\n",
      "386.31 epochs, iter 65600: train_loss = 0.141894534, param_norm = 479.707, grad_norm = 0.506, train_F1/valid_F1 = 0.954000/0.353707\n",
      "387.49 epochs, iter 65800: train_loss = 0.142753705, param_norm = 479.714, grad_norm = 0.548, train_F1/valid_F1 = 0.956000/0.358717\n",
      "388.67 epochs, iter 66000: train_loss = 0.142606646, param_norm = 479.721, grad_norm = 0.557, train_F1/valid_F1 = 0.955000/0.360721\n",
      "Saved parameters to ./04-28-03/model.ckpt-66000\n",
      "389.85 epochs, iter 66200: train_loss = 0.142048404, param_norm = 479.729, grad_norm = 0.535, train_F1/valid_F1 = 0.953000/0.350701\n",
      "391.03 epochs, iter 66400: train_loss = 0.141384885, param_norm = 479.732, grad_norm = 0.510, train_F1/valid_F1 = 0.956000/0.357715\n",
      "392.20 epochs, iter 66600: train_loss = 0.140296549, param_norm = 479.738, grad_norm = 0.487, train_F1/valid_F1 = 0.956000/0.362725\n",
      "393.38 epochs, iter 66800: train_loss = 0.142119482, param_norm = 479.744, grad_norm = 0.777, train_F1/valid_F1 = 0.954000/0.351703\n",
      "394.56 epochs, iter 67000: train_loss = 0.140784353, param_norm = 479.749, grad_norm = 0.711, train_F1/valid_F1 = 0.954000/0.358717\n",
      "Saved parameters to ./04-28-03/model.ckpt-67000\n",
      "395.74 epochs, iter 67200: train_loss = 0.140378028, param_norm = 479.756, grad_norm = 0.580, train_F1/valid_F1 = 0.958000/0.348697\n",
      "396.91 epochs, iter 67400: train_loss = 0.137164280, param_norm = 479.761, grad_norm = 0.492, train_F1/valid_F1 = 0.955000/0.357715\n",
      "398.09 epochs, iter 67600: train_loss = 0.149922609, param_norm = 479.769, grad_norm = 12.753, train_F1/valid_F1 = 0.956000/0.369739\n",
      "399.27 epochs, iter 67800: train_loss = 0.137432322, param_norm = 479.775, grad_norm = 0.569, train_F1/valid_F1 = 0.956000/0.359719\n",
      "400.45 epochs, iter 68000: train_loss = 0.138643473, param_norm = 479.780, grad_norm = 0.523, train_F1/valid_F1 = 0.959000/0.364729\n",
      "Saved parameters to ./04-28-03/model.ckpt-68000\n",
      "401.63 epochs, iter 68200: train_loss = 0.138341725, param_norm = 479.786, grad_norm = 0.627, train_F1/valid_F1 = 0.956000/0.359719\n",
      "402.80 epochs, iter 68400: train_loss = 0.138068914, param_norm = 479.797, grad_norm = 0.660, train_F1/valid_F1 = 0.958000/0.357715\n",
      "403.98 epochs, iter 68600: train_loss = 0.137710243, param_norm = 479.800, grad_norm = 0.520, train_F1/valid_F1 = 0.956000/0.361723\n",
      "405.16 epochs, iter 68800: train_loss = 0.137353763, param_norm = 479.805, grad_norm = 0.639, train_F1/valid_F1 = 0.958000/0.354709\n",
      "406.34 epochs, iter 69000: train_loss = 0.135470361, param_norm = 479.812, grad_norm = 0.759, train_F1/valid_F1 = 0.958000/0.357715\n",
      "Saved parameters to ./04-28-03/model.ckpt-69000\n",
      "407.51 epochs, iter 69200: train_loss = 0.134985074, param_norm = 479.820, grad_norm = 0.455, train_F1/valid_F1 = 0.958000/0.353707\n",
      "408.69 epochs, iter 69400: train_loss = 0.143043697, param_norm = 479.825, grad_norm = 16.890, train_F1/valid_F1 = 0.955000/0.360721\n",
      "409.87 epochs, iter 69600: train_loss = 0.134182811, param_norm = 479.830, grad_norm = 3.988, train_F1/valid_F1 = 0.960000/0.352705\n",
      "411.05 epochs, iter 69800: train_loss = 0.135717362, param_norm = 479.837, grad_norm = 0.657, train_F1/valid_F1 = 0.958000/0.353707\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "412.23 epochs, iter 70000: train_loss = 0.132895410, param_norm = 479.842, grad_norm = 0.495, train_F1/valid_F1 = 0.959000/0.362725\n",
      "Saved parameters to ./04-28-03/model.ckpt-70000\n",
      "413.40 epochs, iter 70200: train_loss = 0.133469552, param_norm = 479.846, grad_norm = 11.929, train_F1/valid_F1 = 0.957000/0.360721\n",
      "414.58 epochs, iter 70400: train_loss = 0.132174209, param_norm = 479.852, grad_norm = 0.654, train_F1/valid_F1 = 0.959000/0.366733\n",
      "415.76 epochs, iter 70600: train_loss = 0.133499995, param_norm = 479.858, grad_norm = 0.619, train_F1/valid_F1 = 0.960000/0.353707\n",
      "416.94 epochs, iter 70800: train_loss = 0.131632462, param_norm = 479.865, grad_norm = 0.681, train_F1/valid_F1 = 0.960000/0.360721\n",
      "418.11 epochs, iter 71000: train_loss = 0.132353738, param_norm = 479.870, grad_norm = 0.713, train_F1/valid_F1 = 0.959000/0.351703\n",
      "Saved parameters to ./04-28-03/model.ckpt-71000\n",
      "419.29 epochs, iter 71200: train_loss = 0.131123632, param_norm = 479.876, grad_norm = 0.921, train_F1/valid_F1 = 0.961000/0.351703\n",
      "420.47 epochs, iter 71400: train_loss = 0.130110115, param_norm = 479.882, grad_norm = 1.433, train_F1/valid_F1 = 0.959000/0.358717\n",
      "421.65 epochs, iter 71600: train_loss = 0.132472888, param_norm = 479.889, grad_norm = 2.933, train_F1/valid_F1 = 0.961000/0.345691\n",
      "422.83 epochs, iter 71800: train_loss = 0.130740792, param_norm = 479.893, grad_norm = 0.731, train_F1/valid_F1 = 0.961000/0.358717\n",
      "424.00 epochs, iter 72000: train_loss = 0.128286332, param_norm = 479.900, grad_norm = 1.123, train_F1/valid_F1 = 0.962000/0.354709\n",
      "Saved parameters to ./04-28-03/model.ckpt-72000\n",
      "425.18 epochs, iter 72200: train_loss = 0.127817765, param_norm = 479.907, grad_norm = 0.552, train_F1/valid_F1 = 0.961000/0.356713\n",
      "426.36 epochs, iter 72400: train_loss = 0.128034949, param_norm = 479.912, grad_norm = 0.519, train_F1/valid_F1 = 0.966000/0.346693\n",
      "427.54 epochs, iter 72600: train_loss = 0.128690928, param_norm = 479.918, grad_norm = 0.527, train_F1/valid_F1 = 0.963000/0.362725\n",
      "428.71 epochs, iter 72800: train_loss = 0.126970813, param_norm = 479.924, grad_norm = 0.622, train_F1/valid_F1 = 0.964000/0.357715\n",
      "429.89 epochs, iter 73000: train_loss = 0.125141412, param_norm = 479.929, grad_norm = 0.500, train_F1/valid_F1 = 0.963000/0.355711\n",
      "Saved parameters to ./04-28-03/model.ckpt-73000\n",
      "431.07 epochs, iter 73200: train_loss = 0.127776042, param_norm = 479.936, grad_norm = 1.137, train_F1/valid_F1 = 0.962000/0.349699\n",
      "432.25 epochs, iter 73400: train_loss = 0.126902863, param_norm = 479.941, grad_norm = 0.624, train_F1/valid_F1 = 0.964000/0.355711\n",
      "433.43 epochs, iter 73600: train_loss = 0.123757929, param_norm = 479.946, grad_norm = 1.320, train_F1/valid_F1 = 0.965000/0.349699\n",
      "434.60 epochs, iter 73800: train_loss = 0.125798061, param_norm = 479.953, grad_norm = 0.733, train_F1/valid_F1 = 0.966000/0.352705\n",
      "435.78 epochs, iter 74000: train_loss = 0.124870941, param_norm = 479.961, grad_norm = 0.680, train_F1/valid_F1 = 0.965000/0.360721\n",
      "Saved parameters to ./04-28-03/model.ckpt-74000\n",
      "436.96 epochs, iter 74200: train_loss = 0.125013307, param_norm = 479.964, grad_norm = 1.158, train_F1/valid_F1 = 0.964000/0.362725\n",
      "438.14 epochs, iter 74400: train_loss = 0.123619251, param_norm = 479.969, grad_norm = 2.315, train_F1/valid_F1 = 0.965000/0.354709\n",
      "439.31 epochs, iter 74600: train_loss = 0.123136200, param_norm = 479.975, grad_norm = 0.587, train_F1/valid_F1 = 0.966000/0.357715\n",
      "440.49 epochs, iter 74800: train_loss = 0.122007422, param_norm = 479.983, grad_norm = 0.567, train_F1/valid_F1 = 0.966000/0.355711\n",
      "441.67 epochs, iter 75000: train_loss = 0.123408444, param_norm = 479.990, grad_norm = 0.949, train_F1/valid_F1 = 0.963000/0.358717\n",
      "Saved parameters to ./04-28-03/model.ckpt-75000\n",
      "442.85 epochs, iter 75200: train_loss = 0.124292038, param_norm = 479.997, grad_norm = 5.935, train_F1/valid_F1 = 0.964000/0.357715\n",
      "444.03 epochs, iter 75400: train_loss = 0.123050906, param_norm = 480.003, grad_norm = 0.798, train_F1/valid_F1 = 0.964000/0.344689\n",
      "445.20 epochs, iter 75600: train_loss = 0.122719206, param_norm = 480.008, grad_norm = 0.771, train_F1/valid_F1 = 0.966000/0.350701\n",
      "446.38 epochs, iter 75800: train_loss = 0.120422713, param_norm = 480.014, grad_norm = 0.499, train_F1/valid_F1 = 0.966000/0.351703\n",
      "447.56 epochs, iter 76000: train_loss = 0.121269584, param_norm = 480.020, grad_norm = 0.679, train_F1/valid_F1 = 0.965000/0.341683\n",
      "Saved parameters to ./04-28-03/model.ckpt-76000\n",
      "448.74 epochs, iter 76200: train_loss = 0.119770981, param_norm = 480.025, grad_norm = 0.804, train_F1/valid_F1 = 0.967000/0.347695\n",
      "449.91 epochs, iter 76400: train_loss = 0.117741995, param_norm = 480.032, grad_norm = 0.530, train_F1/valid_F1 = 0.966000/0.354709\n",
      "451.09 epochs, iter 76600: train_loss = 0.116425917, param_norm = 480.036, grad_norm = 0.583, train_F1/valid_F1 = 0.966000/0.349699\n",
      "452.27 epochs, iter 76800: train_loss = 0.117674313, param_norm = 480.044, grad_norm = 0.618, train_F1/valid_F1 = 0.966000/0.342685\n",
      "453.45 epochs, iter 77000: train_loss = 0.116646744, param_norm = 480.049, grad_norm = 0.514, train_F1/valid_F1 = 0.966000/0.345691\n",
      "Saved parameters to ./04-28-03/model.ckpt-77000\n",
      "454.63 epochs, iter 77200: train_loss = 0.117616355, param_norm = 480.056, grad_norm = 0.591, train_F1/valid_F1 = 0.965000/0.344689\n",
      "455.80 epochs, iter 77400: train_loss = 0.119729087, param_norm = 480.064, grad_norm = 0.861, train_F1/valid_F1 = 0.961000/0.348697\n",
      "456.98 epochs, iter 77600: train_loss = 0.116644181, param_norm = 480.069, grad_norm = 0.502, train_F1/valid_F1 = 0.965000/0.349699\n",
      "458.16 epochs, iter 77800: train_loss = 0.145870045, param_norm = 480.073, grad_norm = 17.596, train_F1/valid_F1 = 0.963000/0.348697\n",
      "459.34 epochs, iter 78000: train_loss = 0.120336890, param_norm = 480.076, grad_norm = 2.557, train_F1/valid_F1 = 0.963000/0.347695\n",
      "Saved parameters to ./04-28-03/model.ckpt-78000\n",
      "460.51 epochs, iter 78200: train_loss = 0.115522780, param_norm = 480.080, grad_norm = 0.597, train_F1/valid_F1 = 0.966000/0.347695\n",
      "461.69 epochs, iter 78400: train_loss = 0.115965523, param_norm = 480.082, grad_norm = 0.651, train_F1/valid_F1 = 0.965000/0.350701\n",
      "462.87 epochs, iter 78600: train_loss = 0.124712072, param_norm = 480.086, grad_norm = 18.782, train_F1/valid_F1 = 0.964000/0.347695\n",
      "464.05 epochs, iter 78800: train_loss = 0.114760958, param_norm = 480.088, grad_norm = 1.189, train_F1/valid_F1 = 0.965000/0.344689\n",
      "465.23 epochs, iter 79000: train_loss = 0.113517046, param_norm = 480.090, grad_norm = 0.375, train_F1/valid_F1 = 0.965000/0.350701\n",
      "Saved parameters to ./04-28-03/model.ckpt-79000\n",
      "466.40 epochs, iter 79200: train_loss = 0.114341199, param_norm = 480.093, grad_norm = 0.546, train_F1/valid_F1 = 0.967000/0.342685\n",
      "467.58 epochs, iter 79400: train_loss = 0.113548450, param_norm = 480.096, grad_norm = 0.575, train_F1/valid_F1 = 0.968000/0.348697\n",
      "468.76 epochs, iter 79600: train_loss = 0.113814414, param_norm = 480.099, grad_norm = 0.601, train_F1/valid_F1 = 0.967000/0.348697\n",
      "469.94 epochs, iter 79800: train_loss = 0.130846694, param_norm = 480.101, grad_norm = 19.686, train_F1/valid_F1 = 0.965000/0.346693\n",
      "471.11 epochs, iter 80000: train_loss = 0.111728236, param_norm = 480.104, grad_norm = 0.435, train_F1/valid_F1 = 0.967000/0.340681\n",
      "Saved parameters to ./04-28-03/model.ckpt-80000\n",
      "472.29 epochs, iter 80200: train_loss = 0.111971930, param_norm = 480.107, grad_norm = 10.143, train_F1/valid_F1 = 0.967000/0.338677\n",
      "473.47 epochs, iter 80400: train_loss = 0.120919637, param_norm = 480.110, grad_norm = 14.930, train_F1/valid_F1 = 0.966000/0.346693\n",
      "474.65 epochs, iter 80600: train_loss = 0.111676082, param_norm = 480.112, grad_norm = 0.463, train_F1/valid_F1 = 0.966000/0.345691\n",
      "475.83 epochs, iter 80800: train_loss = 0.120827705, param_norm = 480.115, grad_norm = 19.919, train_F1/valid_F1 = 0.965000/0.349699\n",
      "477.00 epochs, iter 81000: train_loss = 0.112118028, param_norm = 480.118, grad_norm = 0.624, train_F1/valid_F1 = 0.965000/0.356713\n",
      "Saved parameters to ./04-28-03/model.ckpt-81000\n",
      "478.18 epochs, iter 81200: train_loss = 0.110754393, param_norm = 480.121, grad_norm = 0.413, train_F1/valid_F1 = 0.966000/0.352705\n",
      "479.36 epochs, iter 81400: train_loss = 0.111182503, param_norm = 480.123, grad_norm = 0.624, train_F1/valid_F1 = 0.967000/0.347695\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "480.54 epochs, iter 81600: train_loss = 0.111369342, param_norm = 480.127, grad_norm = 0.585, train_F1/valid_F1 = 0.966000/0.348697\n",
      "481.71 epochs, iter 81800: train_loss = 0.111041635, param_norm = 480.130, grad_norm = 0.592, train_F1/valid_F1 = 0.966000/0.349699\n",
      "482.89 epochs, iter 82000: train_loss = 0.111297175, param_norm = 480.132, grad_norm = 0.526, train_F1/valid_F1 = 0.965000/0.347695\n",
      "Saved parameters to ./04-28-03/model.ckpt-82000\n",
      "484.07 epochs, iter 82200: train_loss = 0.109403223, param_norm = 480.135, grad_norm = 0.460, train_F1/valid_F1 = 0.967000/0.345691\n",
      "485.25 epochs, iter 82400: train_loss = 0.109668761, param_norm = 480.138, grad_norm = 0.510, train_F1/valid_F1 = 0.966000/0.348697\n",
      "486.43 epochs, iter 82600: train_loss = 0.110341132, param_norm = 480.141, grad_norm = 0.636, train_F1/valid_F1 = 0.965000/0.352705\n",
      "487.60 epochs, iter 82800: train_loss = 0.108701527, param_norm = 480.144, grad_norm = 0.502, train_F1/valid_F1 = 0.967000/0.350701\n",
      "488.78 epochs, iter 83000: train_loss = 0.109163910, param_norm = 480.147, grad_norm = 0.492, train_F1/valid_F1 = 0.966000/0.345691\n",
      "Saved parameters to ./04-28-03/model.ckpt-83000\n",
      "489.96 epochs, iter 83200: train_loss = 0.109594688, param_norm = 480.150, grad_norm = 0.661, train_F1/valid_F1 = 0.967000/0.346693\n",
      "491.14 epochs, iter 83400: train_loss = 0.109147385, param_norm = 480.154, grad_norm = 0.478, train_F1/valid_F1 = 0.968000/0.350701\n",
      "492.31 epochs, iter 83600: train_loss = 0.108115017, param_norm = 480.156, grad_norm = 0.532, train_F1/valid_F1 = 0.967000/0.347695\n",
      "493.49 epochs, iter 83800: train_loss = 0.117689423, param_norm = 480.158, grad_norm = 13.614, train_F1/valid_F1 = 0.963000/0.350701\n",
      "494.67 epochs, iter 84000: train_loss = 0.108390868, param_norm = 480.161, grad_norm = 0.662, train_F1/valid_F1 = 0.967000/0.352705\n",
      "Saved parameters to ./04-28-03/model.ckpt-84000\n",
      "495.85 epochs, iter 84200: train_loss = 0.108654596, param_norm = 480.164, grad_norm = 0.590, train_F1/valid_F1 = 0.967000/0.352705\n",
      "497.03 epochs, iter 84400: train_loss = 0.108885169, param_norm = 480.166, grad_norm = 0.669, train_F1/valid_F1 = 0.967000/0.349699\n",
      "498.20 epochs, iter 84600: train_loss = 0.107161894, param_norm = 480.169, grad_norm = 0.565, train_F1/valid_F1 = 0.968000/0.341683\n",
      "499.38 epochs, iter 84800: train_loss = 0.107154265, param_norm = 480.173, grad_norm = 0.591, train_F1/valid_F1 = 0.967000/0.342685\n",
      "500.56 epochs, iter 85000: train_loss = 0.107986741, param_norm = 480.174, grad_norm = 0.876, train_F1/valid_F1 = 0.968000/0.343687\n",
      "Saved parameters to ./04-28-03/model.ckpt-85000\n",
      "501.74 epochs, iter 85200: train_loss = 0.106465399, param_norm = 480.176, grad_norm = 0.655, train_F1/valid_F1 = 0.968000/0.344689\n",
      "502.92 epochs, iter 85400: train_loss = 0.107503302, param_norm = 480.181, grad_norm = 0.898, train_F1/valid_F1 = 0.967000/0.342685\n",
      "504.09 epochs, iter 85600: train_loss = 0.109665453, param_norm = 480.184, grad_norm = 4.172, train_F1/valid_F1 = 0.966000/0.344689\n",
      "505.27 epochs, iter 85800: train_loss = 0.106806241, param_norm = 480.187, grad_norm = 0.470, train_F1/valid_F1 = 0.967000/0.344689\n",
      "506.45 epochs, iter 86000: train_loss = 0.105952181, param_norm = 480.188, grad_norm = 0.586, train_F1/valid_F1 = 0.968000/0.342685\n",
      "Saved parameters to ./04-28-03/model.ckpt-86000\n",
      "507.63 epochs, iter 86200: train_loss = 0.106072471, param_norm = 480.191, grad_norm = 0.507, train_F1/valid_F1 = 0.967000/0.342685\n",
      "508.80 epochs, iter 86400: train_loss = 0.104965582, param_norm = 480.194, grad_norm = 0.485, train_F1/valid_F1 = 0.968000/0.332665\n",
      "509.98 epochs, iter 86600: train_loss = 0.103603527, param_norm = 480.196, grad_norm = 0.501, train_F1/valid_F1 = 0.970000/0.333667\n",
      "511.16 epochs, iter 86800: train_loss = 0.104888074, param_norm = 480.200, grad_norm = 0.515, train_F1/valid_F1 = 0.967000/0.336673\n",
      "512.34 epochs, iter 87000: train_loss = 0.106053360, param_norm = 480.202, grad_norm = 0.611, train_F1/valid_F1 = 0.967000/0.342685\n",
      "Saved parameters to ./04-28-03/model.ckpt-87000\n",
      "513.52 epochs, iter 87200: train_loss = 0.103702039, param_norm = 480.205, grad_norm = 0.456, train_F1/valid_F1 = 0.970000/0.339679\n",
      "514.69 epochs, iter 87400: train_loss = 0.105249777, param_norm = 480.209, grad_norm = 1.368, train_F1/valid_F1 = 0.968000/0.336673\n",
      "515.87 epochs, iter 87600: train_loss = 0.104181863, param_norm = 480.211, grad_norm = 0.490, train_F1/valid_F1 = 0.969000/0.339679\n",
      "517.05 epochs, iter 87800: train_loss = 0.105390459, param_norm = 480.214, grad_norm = 1.616, train_F1/valid_F1 = 0.967000/0.337675\n",
      "518.23 epochs, iter 88000: train_loss = 0.104117185, param_norm = 480.217, grad_norm = 0.584, train_F1/valid_F1 = 0.968000/0.326653\n",
      "Saved parameters to ./04-28-03/model.ckpt-88000\n",
      "519.40 epochs, iter 88200: train_loss = 0.105120704, param_norm = 480.219, grad_norm = 1.066, train_F1/valid_F1 = 0.968000/0.331663\n",
      "520.58 epochs, iter 88400: train_loss = 0.104227833, param_norm = 480.223, grad_norm = 0.532, train_F1/valid_F1 = 0.966000/0.338677\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-53-f838b6b091a6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlstm_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexperiment_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mexperiment_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeep_prob_val\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbeta\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;31m# IMPORTANT:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# when you think F1 is not going to improve anymore, wait another 50-100 epochs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# if you see any better iteration that has not appeared before, keep waiting.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-50-c611f9b68b3e>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, session, experiment_name, keep_prob_val, beta)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m             \u001b[0;31m# train on this batch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 115\u001b[0;31m             \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mx_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0my_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearning_rate_placeholder\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlr\u001b[0m\u001b[0;34m,\u001b[0m                                                  \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeep_prob\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mkeep_prob_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbeta\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbeta\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_training\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    116\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0miteration\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m200\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/shared/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    903\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    904\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 905\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    906\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    907\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/shared/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1138\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1139\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1140\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1141\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1142\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/shared/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1319\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1320\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1321\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1322\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1323\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/shared/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1325\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1326\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1327\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1328\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1329\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/shared/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1310\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1311\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1312\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1313\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1314\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/shared/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1418\u001b[0m         return tf_session.TF_Run(\n\u001b[1;32m   1419\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1420\u001b[0;31m             status, run_metadata)\n\u001b[0m\u001b[1;32m   1421\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1422\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "lstm_model.train(session=sess, experiment_name=experiment_name, keep_prob_val=1, beta=0)\n",
    "# IMPORTANT:\n",
    "# when you think F1 is not going to improve anymore, wait another 50-100 epochs. \n",
    "# if you see any better iteration that has not appeared before, keep waiting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from 04-28-03/best_ckpt/best.ckpt-4600\n",
      "train F1: 0.539\n",
      "dev F1: 0.46292585170340683\n",
      "test F1: 0.4248496993987976\n"
     ]
    }
   ],
   "source": [
    "# load best checkpoint (based on dev f1) and evaluate\n",
    "ckpt = tf.train.get_checkpoint_state(experiment_name+'/best_ckpt')\n",
    "v2_path = ckpt.model_checkpoint_path + \".index\" if ckpt else \"\"\n",
    "if ckpt and (tf.gfile.Exists(ckpt.model_checkpoint_path) or tf.gfile.Exists(v2_path)):\n",
    "    lstm_model.saver.restore(sess, ckpt.model_checkpoint_path)\n",
    "else:\n",
    "    raise ValueError('What? you dont have a best checkpoint?')\n",
    "\n",
    "y_1000_train_pred = sess.run(lstm_model.preds, feed_dict={lstm_model.X: x_1000_train, lstm_model.keep_prob: 1, \\\n",
    "                                                          lstm_model.beta:0, lstm_model.is_training: False})\n",
    "print(\"train F1:\",F1(y_1000_train, y_1000_train_pred))\n",
    "y_valid_pred = sess.run(lstm_model.preds, feed_dict={lstm_model.X: x_valid, lstm_model.keep_prob: 1, \\\n",
    "                                                     lstm_model.beta:0, lstm_model.is_training: False})\n",
    "print(\"dev F1:\",F1(y_valid, y_valid_pred))\n",
    "y_test_pred = sess.run(lstm_model.preds, feed_dict={lstm_model.X: x_test, lstm_model.keep_prob: 1, \\\n",
    "                                                    lstm_model.beta:0, lstm_model.is_training: False})\n",
    "print(\"test F1:\",F1(y_test, y_test_pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
