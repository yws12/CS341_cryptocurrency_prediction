{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shared/anaconda3/lib/python3.6/site-packages/statsmodels/compat/pandas.py:56: FutureWarning: The pandas.core.datetools module is deprecated and will be removed in a future version. Please use the pandas.tseries module instead.\n",
      "  from pandas.core import datetools\n",
      "/home/shared/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>USDT_BTC_close</th>\n",
       "      <th>USDT_BTC_high</th>\n",
       "      <th>USDT_BTC_low</th>\n",
       "      <th>USDT_BTC_open</th>\n",
       "      <th>USDT_BTC_quoteVolume</th>\n",
       "      <th>USDT_BTC_volume</th>\n",
       "      <th>USDT_BTC_weightedAverage</th>\n",
       "      <th>BTC_ETH_close</th>\n",
       "      <th>BTC_ETH_high</th>\n",
       "      <th>BTC_ETH_low</th>\n",
       "      <th>...</th>\n",
       "      <th>BTC_LTC_quoteVolume</th>\n",
       "      <th>BTC_LTC_volume</th>\n",
       "      <th>BTC_LTC_weightedAverage</th>\n",
       "      <th>BTC_XRP_close</th>\n",
       "      <th>BTC_XRP_high</th>\n",
       "      <th>BTC_XRP_low</th>\n",
       "      <th>BTC_XRP_open</th>\n",
       "      <th>BTC_XRP_quoteVolume</th>\n",
       "      <th>BTC_XRP_volume</th>\n",
       "      <th>BTC_XRP_weightedAverage</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>time</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2016-01-01 08:00:00</th>\n",
       "      <td>433.010137</td>\n",
       "      <td>433.010137</td>\n",
       "      <td>433.010137</td>\n",
       "      <td>433.010137</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>433.010137</td>\n",
       "      <td>0.002174</td>\n",
       "      <td>0.002177</td>\n",
       "      <td>0.002174</td>\n",
       "      <td>...</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.011309</td>\n",
       "      <td>0.008078</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-01-01 08:05:00</th>\n",
       "      <td>433.010137</td>\n",
       "      <td>433.010137</td>\n",
       "      <td>433.010137</td>\n",
       "      <td>433.010137</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>433.010137</td>\n",
       "      <td>0.002177</td>\n",
       "      <td>0.002177</td>\n",
       "      <td>0.002175</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.008079</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>980.096150</td>\n",
       "      <td>0.013429</td>\n",
       "      <td>0.000014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-01-01 08:10:00</th>\n",
       "      <td>433.010137</td>\n",
       "      <td>433.010137</td>\n",
       "      <td>433.010137</td>\n",
       "      <td>433.010137</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>433.010137</td>\n",
       "      <td>0.002175</td>\n",
       "      <td>0.002175</td>\n",
       "      <td>0.002175</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.008079</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>657.512017</td>\n",
       "      <td>0.009028</td>\n",
       "      <td>0.000014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-01-01 08:15:00</th>\n",
       "      <td>433.010137</td>\n",
       "      <td>433.010137</td>\n",
       "      <td>433.010137</td>\n",
       "      <td>433.010137</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>433.010137</td>\n",
       "      <td>0.002174</td>\n",
       "      <td>0.002175</td>\n",
       "      <td>0.002174</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.008079</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>862.028200</td>\n",
       "      <td>0.011801</td>\n",
       "      <td>0.000014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-01-01 08:20:00</th>\n",
       "      <td>433.010137</td>\n",
       "      <td>433.010137</td>\n",
       "      <td>433.010137</td>\n",
       "      <td>433.010137</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>433.010137</td>\n",
       "      <td>0.002174</td>\n",
       "      <td>0.002174</td>\n",
       "      <td>0.002174</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.008079</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>5331.418950</td>\n",
       "      <td>0.072934</td>\n",
       "      <td>0.000014</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 28 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                     USDT_BTC_close  USDT_BTC_high  USDT_BTC_low  \\\n",
       "time                                                               \n",
       "2016-01-01 08:00:00      433.010137     433.010137    433.010137   \n",
       "2016-01-01 08:05:00      433.010137     433.010137    433.010137   \n",
       "2016-01-01 08:10:00      433.010137     433.010137    433.010137   \n",
       "2016-01-01 08:15:00      433.010137     433.010137    433.010137   \n",
       "2016-01-01 08:20:00      433.010137     433.010137    433.010137   \n",
       "\n",
       "                     USDT_BTC_open  USDT_BTC_quoteVolume  USDT_BTC_volume  \\\n",
       "time                                                                        \n",
       "2016-01-01 08:00:00     433.010137                   0.0              0.0   \n",
       "2016-01-01 08:05:00     433.010137                   0.0              0.0   \n",
       "2016-01-01 08:10:00     433.010137                   0.0              0.0   \n",
       "2016-01-01 08:15:00     433.010137                   0.0              0.0   \n",
       "2016-01-01 08:20:00     433.010137                   0.0              0.0   \n",
       "\n",
       "                     USDT_BTC_weightedAverage  BTC_ETH_close  BTC_ETH_high  \\\n",
       "time                                                                         \n",
       "2016-01-01 08:00:00                433.010137       0.002174      0.002177   \n",
       "2016-01-01 08:05:00                433.010137       0.002177      0.002177   \n",
       "2016-01-01 08:10:00                433.010137       0.002175      0.002175   \n",
       "2016-01-01 08:15:00                433.010137       0.002174      0.002175   \n",
       "2016-01-01 08:20:00                433.010137       0.002174      0.002174   \n",
       "\n",
       "                     BTC_ETH_low           ...             \\\n",
       "time                                       ...              \n",
       "2016-01-01 08:00:00     0.002174           ...              \n",
       "2016-01-01 08:05:00     0.002175           ...              \n",
       "2016-01-01 08:10:00     0.002175           ...              \n",
       "2016-01-01 08:15:00     0.002174           ...              \n",
       "2016-01-01 08:20:00     0.002174           ...              \n",
       "\n",
       "                     BTC_LTC_quoteVolume  BTC_LTC_volume  \\\n",
       "time                                                       \n",
       "2016-01-01 08:00:00                  1.4        0.011309   \n",
       "2016-01-01 08:05:00                  0.0        0.000000   \n",
       "2016-01-01 08:10:00                  0.0        0.000000   \n",
       "2016-01-01 08:15:00                  0.0        0.000000   \n",
       "2016-01-01 08:20:00                  0.0        0.000000   \n",
       "\n",
       "                     BTC_LTC_weightedAverage  BTC_XRP_close  BTC_XRP_high  \\\n",
       "time                                                                        \n",
       "2016-01-01 08:00:00                 0.008078       0.000014      0.000014   \n",
       "2016-01-01 08:05:00                 0.008079       0.000014      0.000014   \n",
       "2016-01-01 08:10:00                 0.008079       0.000014      0.000014   \n",
       "2016-01-01 08:15:00                 0.008079       0.000014      0.000014   \n",
       "2016-01-01 08:20:00                 0.008079       0.000014      0.000014   \n",
       "\n",
       "                     BTC_XRP_low  BTC_XRP_open  BTC_XRP_quoteVolume  \\\n",
       "time                                                                  \n",
       "2016-01-01 08:00:00     0.000014      0.000014             0.000000   \n",
       "2016-01-01 08:05:00     0.000014      0.000014           980.096150   \n",
       "2016-01-01 08:10:00     0.000014      0.000014           657.512017   \n",
       "2016-01-01 08:15:00     0.000014      0.000014           862.028200   \n",
       "2016-01-01 08:20:00     0.000014      0.000014          5331.418950   \n",
       "\n",
       "                     BTC_XRP_volume  BTC_XRP_weightedAverage  \n",
       "time                                                          \n",
       "2016-01-01 08:00:00        0.000000                 0.000014  \n",
       "2016-01-01 08:05:00        0.013429                 0.000014  \n",
       "2016-01-01 08:10:00        0.009028                 0.000014  \n",
       "2016-01-01 08:15:00        0.011801                 0.000014  \n",
       "2016-01-01 08:20:00        0.072934                 0.000014  \n",
       "\n",
       "[5 rows x 28 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "__author__ = \"Yicheng Li\"\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import statsmodels.api as sm\n",
    "import random\n",
    "from sklearn import preprocessing\n",
    "import tensorflow as tf\n",
    "\n",
    "dir_path = ''\n",
    "df = pd.read_pickle(dir_path+'df_5min.pkl')\n",
    "df = df.dropna()\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler_price = preprocessing.MinMaxScaler(feature_range=(0.1, 1))\n",
    "scaler_features = preprocessing.MinMaxScaler(feature_range=(0.1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to create train, validation, test data given sequence length\n",
    "def load_data(df, seq_len):\n",
    "    labels = df['USDT_BTC_weightedAverage'].as_matrix()\n",
    "    feature_set = [6]\n",
    "    \n",
    "    data_raw = df.as_matrix() # convert to numpy array\n",
    "#     data_raw = preprocessing.minmax_scale(data_raw[:, feature_set]) # normalizing features\n",
    "    scaler_price.fit(data_raw[:, [6]])\n",
    "    data_raw = scaler_features.fit_transform(data_raw[:, feature_set])\n",
    "    data = []\n",
    "    \n",
    "    # create all possible sequences of length seq_len\n",
    "    for index in range(len(data_raw) - seq_len + 1): \n",
    "        data.append(data_raw[index: index + seq_len, :])\n",
    "    \n",
    "    data = np.array(data)\n",
    "    \n",
    "    n_train_valid_pairs = 3\n",
    "    each_train_set_size_pct = 25\n",
    "    each_valid_set_size_pct = 5\n",
    "    \n",
    "    each_train_set_size = round(each_train_set_size_pct/100*data.shape[0])\n",
    "    each_valid_set_size = round(each_valid_set_size_pct/100*data.shape[0])\n",
    "    \n",
    "    x_train_sets = []\n",
    "    y_train_sets = []\n",
    "    x_valid_sets = []\n",
    "    y_valid_sets = []\n",
    "    used = 0\n",
    "    \n",
    "    for i in range(n_train_valid_pairs):\n",
    "        x_train_sets.append(data[used : used + each_train_set_size,:-1,:]) # cannot see last day, which we aim to predict\n",
    "        y_train_sets.append(labels[used + seq_len-1 : used + each_train_set_size + seq_len-1])\n",
    "        used += each_train_set_size\n",
    "    \n",
    "        x_valid_sets.append(data[used : used + each_valid_set_size,:-1,:])\n",
    "        y_valid_sets.append(labels[used + seq_len-1 : used + each_valid_set_size + seq_len-1])\n",
    "        used += each_valid_set_size\n",
    "    \n",
    "    x_test = data[used : , :-1, :]\n",
    "    y_test = labels[seq_len-1 + used : ]\n",
    "    \n",
    "    x_train = np.concatenate(x_train_sets, axis=0)\n",
    "    y_train = np.concatenate(y_train_sets, axis=0)\n",
    "    x_valid = np.concatenate(x_valid_sets, axis=0)\n",
    "    y_valid = np.concatenate(y_valid_sets, axis=0)\n",
    "    \n",
    "    return [x_train, y_train, x_valid, y_valid, x_test, y_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train.shape =  (180096, 4, 1)\n",
      "y_train.shape =  (180096, 1)\n",
      "x_valid.shape =  (36018, 4, 1)\n",
      "y_valid.shape =  (36018, 1)\n",
      "x_test.shape =  (24016, 4, 1)\n",
      "y_test.shape =  (24016, 1)\n"
     ]
    }
   ],
   "source": [
    "# create train, test data\n",
    "seq_len = 5 # choose sequence length\n",
    "x_train, y_train, x_valid, y_valid, x_test, y_test = load_data(df, seq_len)\n",
    "y_train = y_train.reshape([-1,1])\n",
    "y_valid = y_valid.reshape([-1,1])\n",
    "y_test = y_test.reshape([-1,1])\n",
    "print('x_train.shape = ',x_train.shape)\n",
    "print('y_train.shape = ', y_train.shape)\n",
    "print('x_valid.shape = ',x_valid.shape)\n",
    "print('y_valid.shape = ', y_valid.shape)\n",
    "print('x_test.shape = ', x_test.shape)\n",
    "print('y_test.shape = ',y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shuffle training data\n",
    "s = np.arange(x_train.shape[0])\n",
    "np.random.shuffle(s)\n",
    "x_train = x_train[s]\n",
    "y_train = y_train[s]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SMAPE(y, y_pred):\n",
    "    if len(y) != len(y_pred):\n",
    "        raise ValueError('Length of prediction array is not equal to length of y array.')\n",
    "    return np.mean(np.abs(y-y_pred)*2/(np.abs(y)+np.abs(y_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## baseline performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "baseline dev_SMAPE= 0.0023080371393128284\n",
      "baseline test_SMAPE= 0.002474303030669981\n"
     ]
    }
   ],
   "source": [
    "y_pred = np.roll(y_valid,1, axis=0)\n",
    "print('baseline dev_SMAPE=',SMAPE(y_valid[1:], y_pred[1:]))\n",
    "y_pred = np.roll(y_test,1, axis=0)\n",
    "y_pred[0] = y_valid[-1]\n",
    "print('baseline test_SMAPE=',SMAPE(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM model graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_in_epoch = 0;\n",
    "perm_array  = np.arange(x_train.shape[0])\n",
    "np.random.shuffle(perm_array)\n",
    "\n",
    "# function to get the next batch\n",
    "def get_next_batch(batch_size):\n",
    "    global index_in_epoch, x_train, perm_array   \n",
    "    start = index_in_epoch\n",
    "    index_in_epoch += batch_size\n",
    "    \n",
    "    if index_in_epoch > x_train.shape[0]:\n",
    "        np.random.shuffle(perm_array) # shuffle permutation array\n",
    "        start = 0 # start next epoch\n",
    "        index_in_epoch = batch_size\n",
    "        \n",
    "    end = index_in_epoch\n",
    "    return x_train[perm_array[start:end]], scaler_price.transform(y_train[perm_array[start:end]])\n",
    "\n",
    "x_1000_train, y_1000_train = get_next_batch(1000) # special batch of 1000 records in training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/shared/anaconda3/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/base.py:198: retry (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use the retry module or similar alternatives.\n"
     ]
    }
   ],
   "source": [
    "# parameters\n",
    "n_steps = seq_len-1 \n",
    "n_inputs = x_train.shape[-1]\n",
    "n_neurons = 300  # cell.state_size\n",
    "n_outputs = 1\n",
    "n_layers = 2\n",
    "learning_rate = 1e-4\n",
    "batch_size = 100\n",
    "n_epochs = 200 \n",
    "train_set_size = x_train.shape[0]\n",
    "test_set_size = x_test.shape[0]\n",
    "keep_prob = 1\n",
    "max_gradient_norm = 5\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "X = tf.placeholder(tf.float32, [None, n_steps, n_inputs])\n",
    "y = tf.placeholder(tf.float32, [None, n_outputs])\n",
    "\n",
    "# use LSTM Cell\n",
    "layers = [tf.contrib.rnn.LSTMCell(num_units=n_neurons, \\\n",
    "                                  initializer=tf.contrib.layers.xavier_initializer(), \\\n",
    "                                  activation=tf.nn.elu)\n",
    "         for layer in range(n_layers)]\n",
    "\n",
    "# use Basic LSTM Cell \n",
    "# layers = [tf.contrib.rnn.BasicLSTMCell(num_units=n_neurons, activation=tf.nn.elu)\n",
    "#           for layer in range(n_layers)]\n",
    "\n",
    "# use LSTM Cell with peephole connections\n",
    "#layers = [tf.contrib.rnn.LSTMCell(num_units=n_neurons, \n",
    "#                                  activation=tf.nn.leaky_relu, use_peepholes = True)\n",
    "#          for layer in range(n_layers)]\n",
    "\n",
    "# use GRU cell\n",
    "#layers = [tf.contrib.rnn.GRUCell(num_units=n_neurons, activation=tf.nn.leaky_relu)\n",
    "#          for layer in range(n_layers)]\n",
    "\n",
    "multi_layer_cell = tf.contrib.rnn.MultiRNNCell(layers)\n",
    "\n",
    "outputs, states = tf.nn.dynamic_rnn(multi_layer_cell, X, dtype=tf.float32)\n",
    "outputs = tf.nn.dropout(outputs, keep_prob)\n",
    "\n",
    "# 'outputs' is a tensor of shape [batch_size, n_steps, n_neurons(cell.state_size)]\n",
    "stacked_outputs = tf.reshape(outputs, [-1, n_neurons]) \n",
    "stacked_outputs = tf.layers.dense(stacked_outputs, n_outputs)\n",
    "outputs = tf.reshape(stacked_outputs, [-1, n_steps, n_outputs])\n",
    "outputs = outputs[:,n_steps-1,:] # keep only last output of sequence\n",
    "\n",
    "loss = tf.reduce_mean(tf.square(tf.div(outputs,y)-1)) # loss function = mean squared error \n",
    "# loss = tf.reduce_mean(tf.square(outputs - y)) # loss function = mean squared error \n",
    "#loss = tf.reduce_mean(tf.abs(y-outputs)*2/(tf.abs(y)+tf.abs(outputs))) # SMAPE\n",
    "params = tf.trainable_variables()\n",
    "gradients = tf.gradients(loss, params)\n",
    "gradient_norm = tf.global_norm(gradients)\n",
    "clipped_gradients, _ = tf.clip_by_global_norm(gradients, max_gradient_norm)\n",
    "clipped_norm = tf.global_norm(clipped_gradients)\n",
    "param_norm = tf.global_norm(params)\n",
    "optimizer = tf.train.RMSPropOptimizer(learning_rate=learning_rate) \n",
    "# training_op = optimizer.minimize(loss)\n",
    "training_op = optimizer.apply_gradients(zip(clipped_gradients, params))\n",
    "\n",
    "# initialize parameters\n",
    "sess = tf.Session()\n",
    "global_step = tf.Variable(0, name='global_step', trainable=False)\n",
    "saver = tf.train.Saver(max_to_keep=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_name = 'overfit_5min_LSTM_1_features_5seq_300neuron_2layer' # type your experiment name here before running the code below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There is not saved parameters. Creating model with fresh parameters.\n",
      "0.28 epochs, iter 500: train_loss = 0.000809108, param_norm = 35.884, train_SMAPE/valid_SMAPE = 0.091670/0.069795\n",
      "0.56 epochs, iter 1000: train_loss = 0.000459015, param_norm = 35.888, train_SMAPE/valid_SMAPE = 0.068833/0.052310\n",
      "Saved parameters to ./overfit_5min_LSTM_1_features_5seq_300neuron_2layer/model.ckpt-1000\n",
      "0.83 epochs, iter 1500: train_loss = 0.000331958, param_norm = 35.891, train_SMAPE/valid_SMAPE = 0.058269/0.043841\n",
      "1.11 epochs, iter 2000: train_loss = 0.000519974, param_norm = 35.895, train_SMAPE/valid_SMAPE = 0.073975/0.053701\n",
      "Saved parameters to ./overfit_5min_LSTM_1_features_5seq_300neuron_2layer/model.ckpt-2000\n",
      "1.39 epochs, iter 2500: train_loss = 0.000377285, param_norm = 35.899, train_SMAPE/valid_SMAPE = 0.063473/0.046101\n",
      "1.67 epochs, iter 3000: train_loss = 0.000389563, param_norm = 35.903, train_SMAPE/valid_SMAPE = 0.063737/0.046047\n",
      "Saved parameters to ./overfit_5min_LSTM_1_features_5seq_300neuron_2layer/model.ckpt-3000\n",
      "1.94 epochs, iter 3500: train_loss = 0.000269377, param_norm = 35.906, train_SMAPE/valid_SMAPE = 0.053626/0.038880\n",
      "2.22 epochs, iter 4000: train_loss = 0.000355276, param_norm = 35.910, train_SMAPE/valid_SMAPE = 0.060687/0.044806\n",
      "Saved parameters to ./overfit_5min_LSTM_1_features_5seq_300neuron_2layer/model.ckpt-4000\n",
      "2.50 epochs, iter 4500: train_loss = 0.000494402, param_norm = 35.914, train_SMAPE/valid_SMAPE = 0.071995/0.053026\n",
      "2.78 epochs, iter 5000: train_loss = 0.000375006, param_norm = 35.917, train_SMAPE/valid_SMAPE = 0.062178/0.046958\n",
      "Saved parameters to ./overfit_5min_LSTM_1_features_5seq_300neuron_2layer/model.ckpt-5000\n",
      "3.05 epochs, iter 5500: train_loss = 0.000448704, param_norm = 35.920, train_SMAPE/valid_SMAPE = 0.069757/0.050871\n",
      "3.33 epochs, iter 6000: train_loss = 0.000211481, param_norm = 35.924, train_SMAPE/valid_SMAPE = 0.047249/0.036056\n",
      "Saved parameters to ./overfit_5min_LSTM_1_features_5seq_300neuron_2layer/model.ckpt-6000\n",
      "3.61 epochs, iter 6500: train_loss = 0.000297580, param_norm = 35.927, train_SMAPE/valid_SMAPE = 0.056650/0.042587\n",
      "3.89 epochs, iter 7000: train_loss = 0.000272178, param_norm = 35.930, train_SMAPE/valid_SMAPE = 0.053098/0.041195\n",
      "Saved parameters to ./overfit_5min_LSTM_1_features_5seq_300neuron_2layer/model.ckpt-7000\n",
      "4.16 epochs, iter 7500: train_loss = 0.000288407, param_norm = 35.932, train_SMAPE/valid_SMAPE = 0.055904/0.042175\n",
      "4.44 epochs, iter 8000: train_loss = 0.000258354, param_norm = 35.935, train_SMAPE/valid_SMAPE = 0.052042/0.040388\n",
      "Saved parameters to ./overfit_5min_LSTM_1_features_5seq_300neuron_2layer/model.ckpt-8000\n",
      "4.72 epochs, iter 8500: train_loss = 0.000310382, param_norm = 35.938, train_SMAPE/valid_SMAPE = 0.058668/0.043405\n",
      "5.00 epochs, iter 9000: train_loss = 0.000248212, param_norm = 35.940, train_SMAPE/valid_SMAPE = 0.052307/0.039301\n",
      "Saved parameters to ./overfit_5min_LSTM_1_features_5seq_300neuron_2layer/model.ckpt-9000\n",
      "5.27 epochs, iter 9500: train_loss = 0.000377815, param_norm = 35.943, train_SMAPE/valid_SMAPE = 0.062969/0.048551\n",
      "5.55 epochs, iter 10000: train_loss = 0.000340961, param_norm = 35.945, train_SMAPE/valid_SMAPE = 0.061094/0.045773\n",
      "Saved parameters to ./overfit_5min_LSTM_1_features_5seq_300neuron_2layer/model.ckpt-10000\n",
      "5.83 epochs, iter 10500: train_loss = 0.000305504, param_norm = 35.948, train_SMAPE/valid_SMAPE = 0.057030/0.043823\n",
      "6.11 epochs, iter 11000: train_loss = 0.000141280, param_norm = 35.950, train_SMAPE/valid_SMAPE = 0.039733/0.030095\n",
      "Saved parameters to ./overfit_5min_LSTM_1_features_5seq_300neuron_2layer/model.ckpt-11000\n",
      "6.39 epochs, iter 11500: train_loss = 0.000288610, param_norm = 35.953, train_SMAPE/valid_SMAPE = 0.056138/0.042422\n",
      "6.66 epochs, iter 12000: train_loss = 0.000165744, param_norm = 35.955, train_SMAPE/valid_SMAPE = 0.042760/0.032618\n",
      "Saved parameters to ./overfit_5min_LSTM_1_features_5seq_300neuron_2layer/model.ckpt-12000\n",
      "6.94 epochs, iter 12500: train_loss = 0.000262865, param_norm = 35.958, train_SMAPE/valid_SMAPE = 0.053431/0.040670\n",
      "7.22 epochs, iter 13000: train_loss = 0.000172465, param_norm = 35.961, train_SMAPE/valid_SMAPE = 0.042685/0.033439\n",
      "Saved parameters to ./overfit_5min_LSTM_1_features_5seq_300neuron_2layer/model.ckpt-13000\n"
     ]
    }
   ],
   "source": [
    "# run graph\n",
    "ckpt = tf.train.get_checkpoint_state(experiment_name)\n",
    "v2_path = ckpt.model_checkpoint_path + \".index\" if ckpt else \"\"\n",
    "if ckpt and (tf.gfile.Exists(ckpt.model_checkpoint_path) or tf.gfile.Exists(v2_path)):\n",
    "#     iteration = tf.get_variable('iteration',[1])\n",
    "    saver.restore(sess, ckpt.model_checkpoint_path)\n",
    "    iteration = global_step.eval(session=sess) # get last global_step\n",
    "    print(\"Start from iteration:\", iteration)\n",
    "else:\n",
    "    print('There is not saved parameters. Creating model with fresh parameters.')\n",
    "#     iteration = tf.get_variable('iteration',[1], initializer = tf.zeros_initializer)\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    iteration = 0\n",
    "#     sess.run(iteration)\n",
    "    \n",
    "while True:\n",
    "    iteration = iteration + 1\n",
    "    x_batch, y_batch = get_next_batch(batch_size) # fetch the next training batch \n",
    "     \n",
    "    # train on this batch\n",
    "    sess.run(training_op, feed_dict={X: x_batch, y: y_batch})\n",
    "    \n",
    "    if iteration % 500 == 0:\n",
    "#             loss_train = loss.eval(feed_dict={X: x_train, y: y_train}) \n",
    "#             loss_valid = loss.eval(feed_dict={X: x_valid, y: y_valid}) \n",
    "        y_1000_train_pred, loss_val, param_norm_val = sess.run([outputs, loss, param_norm],\\\n",
    "                                                                feed_dict={X: x_1000_train, y:y_1000_train})\n",
    "        y_valid_pred = scaler_price.inverse_transform(sess.run(outputs, feed_dict={X: x_valid}))\n",
    "        print('%.2f epochs, iter %d: train_loss = %.9f, param_norm = %.3f, train_SMAPE/valid_SMAPE = %.6f/%.6f' \\\n",
    "              %(iteration*batch_size/train_set_size, iteration, loss_val, param_norm_val, \\\n",
    "                SMAPE(scaler_price.inverse_transform(y_1000_train), scaler_price.inverse_transform(y_1000_train_pred)), \\\n",
    "                SMAPE(y_valid, y_valid_pred)))\n",
    "#             print('%.2f epochs: loss train/valid = %.6f/%.6f'%(\n",
    "#                 iteration*batch_size/train_set_size, loss_train, loss_valid))\n",
    "    if iteration % 1000 == 0:\n",
    "        global_step.assign(iteration).eval(session=sess) # set and update(eval) global_step with index, i\n",
    "        save_path = saver.save(sess, \"./\"+experiment_name+\"/model.ckpt\", global_step=global_step)\n",
    "        print('Saved parameters to %s' % save_path)\n",
    "\n",
    "y_train_pred = sess.run(outputs, feed_dict={X: x_train})\n",
    "y_valid_pred = sess.run(outputs, feed_dict={X: x_valid})\n",
    "y_test_pred = sess.run(outputs, feed_dict={X: x_test})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0045600566389654\n",
      "0.00566148485662456\n"
     ]
    }
   ],
   "source": [
    "# y_train_pred = scaler_price.inverse_transform(sess.run(outputs, feed_dict={X: x_train}))\n",
    "# print(SMAPE(y_train, y_train_pred))\n",
    "\n",
    "y_valid_pred = scaler_price.inverse_transform(sess.run(outputs, feed_dict={X: x_valid}))\n",
    "print(SMAPE(y_valid, y_valid_pred))\n",
    "\n",
    "y_test_pred = scaler_price.inverse_transform(sess.run(outputs, feed_dict={X: x_test}))\n",
    "print(SMAPE(y_test, y_test_pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
